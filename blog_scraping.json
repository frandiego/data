{
  "blog": [
    {
      "title": "Lo más relevante de la ciencia de datos en 2018",
      "url": "https://www.bbvadata.com/es/the-most-important-developments-in-data-science-of-2018/",
      "date": "26 diciembre, 2018",
      "content": "\nEl año que estamos a punto de dar por finalizado ha sido uno de los más importantes en cuanto al desarrollo de tecnologías de aprendizaje autónomo (machine learning). Durante el 2018, gran parte del debate se ha centrado en cómo seguir perfeccionando estas tecnologías y pasar de la simple optimización al afianzamiento de una disciplina avanzada en Ciencia de Datos, al mismo tiempo que se persigue una aplicación real de la Inteligencia Artificial.\nDesde los desafíos que plantea la industrialización en sistemas de Inteligencia Artificial hasta los defectos que pueden aparecer en la implementación de modelos de Deep Learning, en los últimos 12 meses hemos asistido a una gran proliferación de noticias y debates significativos sobre cómo hacer sostenible e inclusiva la revolución de la Inteligencia Artificial. En este artículo, que publicamos como reconocimiento a la vida y obra de Stephen Hawking, ya recopilamos algunas frases del genio que anticipó la necesidad de un desarrollo responsable de la Inteligencia Artificial. “El éxito en la creación de una Inteligencia Artificial eficaz podría ser el acontecimiento más importante en la historia de nuestra civilización”, decía el Dr. Hawking, que con “éxito” se refería a detectar y neutralizar los riesgos potenciales que esta revolución tecnológica podría traer consigo si no se tenía en cuenta el enfoque ético y de equidad. \nInspirados en el éxito del artículo “qué vimos y nos gustó del 2017”, publicado ahora hace un año, hemos decidido lanzar un nuevo artículo echando un vistazo, en primer lugar, al progreso alcanzado en cuanto al proceso de industrialización de la Inteligencia Artificial. En los párrafos que siguen también abordaremos los hechos más relevantes de este año relacionados con Procesamiento de Lenguaje Natural (en adelante NLP), Deep Learning para grafos, Causal Inference y el abordaje de la ética en la implementación de tecnologías de Machine Learning.\nIndustrialización de la IA\npor Rafael Hernández, Jose A Rodríguez, Roberto Maestre, César de Pablo.\nEl aprendizaje autónomo y la Inteligencia Artificial ya no son sólo un tema de investigación, sino que avanzan por el camino de convertirse en herramientas productivas, cuyo desarrollo encierra sus propios desafíos. Este año hemos visto esfuerzos en el camino hacia herramientas más profesionalizadas que permitan poner en práctica el desarrollo del aprendizaje autónomo.\nUn ejemplo es la aparición de marcos de trabajo que buscan profesionalizar y automatizar plataformas de aprendizaje autónomo, como MLFlow o Tensorflow TFX. En el ámbito de Efficient Neural Architecture Search (ENAS), hemos sido testigos de la integración de AutoML en marcos de trabajo como Tensorflow, Keras y Pytorch. Además, Featuretools, un framework creado por el MIT para descubrir combinaciones de atributos, se aplicó para la detección de fraudes con tarjetas de crédito en colaboración con el BBVA.\nPero también hemos leído artículos sobre compañías que comparten sus aprendizajes al desplegar modelos de aprendizaje autónomo o Inteligencia Artificial a gran escala. En este sentido, Uber ha revisado su plataforma Michelangelo, Linkedin y su AI Academy han trabajado en llevar la formación en IA a todos los empleados, Amazon incluso ha abierto a todo el mundo la formación en aprendizaje automático que antes era solo accesible a sus empleados; Apple publicó un artículo describiendo su plataforma para proporcionar Deep Learning a escala interna, mientras, en otro artículo, Booking.com describía cómo democratizaban experimentos controlados en línea.\nDurante este año también se han consolidado talleres en aprendizaje autónomo y software: se celebró una nueva edición de NIPS MLSystems Workshop al tiempo que se inauguraba una nueva conferencia (SysML).\nFinalmente, también hemos visto durante este año algunos ejemplos relevantes en los que el aprendizaje autónomo superaba los límites de las “áreas” de una empresa. Por ejemplo, Amazon ofrece a los clientes de SageMaker un modelo de previsión de series temporales que comenzaron a desarrollar internamente para su propia predicción de demanda, mientras Uber desplegó aprendizaje autónomo para su propio pronóstico financiero.\nProcesamiento de Lenguaje Natural-NLP\npor César de Pablo.\nEn el último año hemos sido testigos de la aparición de una nueva generación de métodos de embbeding, embeddings universales o, más adecuadamente, modelos de lenguaje, que han demostrado ser útiles en diferentes tareas de Procesamiento de Lenguaje Natural, ayudando en problemas como la clasificación o traducción de texto mediante la utilización de enormes cantidades de datos de texto sin etiquetar. ELMO (Deep Contextualized Word representations), ULMFit e importantes mejoras en la arquitectura Transformer (liderada finalmente por BERT) han mostrado grandes avances en cuanto a clasificación de texto, NER o lectura automática. ELMO proporciona vectores fijos, tal y como hace Word2Vec, pero con el añadido de que éstos están contextualizados. Por el contrario, ULMF es un procedimiento que permite ajustar un modelo de lenguaje a una nueva tarea con unos pocos ejemplos supervisados. BERT aborda ambas vertientes, ya que se trata de un modelo de lenguaje pre-entrenado que tiene en cuenta el contexto, pero que también puede extraer embeddings. Su inconveniente para escalar fuera de Google es que, incluso para el reajuste, conlleva un alto coste de computación y requiere una gran cantidad de memoria.\nSiguiendo con NLP, otra buena noticia es que estos progresos no se orientan únicamente al inglés, sino que pueden ser aplicados en otros idiomas con algunos cambios.\nAprendizaje profundo en grafos\npor César de Pablo.\nLas tecnologías relacionadas con Deep Learning han cosechado resultados muy prometedores en lenguaje e imágenes, en ambos casos debido a arquitecturas especializadas que tratan con secuencias (LSTM) o grids (CNN). Sin embargo, una gran cantidad de problemas pueden beneficiarse de una representación estructurada, aunque sin exhibir una estructura regular, sino una gráfica genérica con aplicaciones en sistemas de recomendación, Procesamiento de Lenguaje Natural o modelado de usuarios. Esta investigación (con casi 30 autores) ha servido para proporcionar un marco que abarca diferentes enfoques de modelado desde GCN (Graph Convolutional Networks) hasta una arquitectura de transformador generalizada aplicada a los gráficos. Gracias a DeepMind incluso tenemos una biblioteca de referencia basada en Tensorflow. También hemos visto el uso comercial de algoritmos relacionados como GraphSAGE en Pinterest.\nInferencia Causal\npor Juan Arévalo.\nEste año hemos sido testigos de la irrupción de la Inferencia Causal en el campo de la Ciencia de Datos con contribuciones importantes como las del libro de Jude Pearl, “The book of Why”, o el artículo de Miguel Hernán et al. sobre cómo incorporar la Causalidad en la Ciencia de Datos, entre otras cosas. El advenimiento de la Revolución Causal es concomitante a la aplicación exitosa del Análisis Contrafactual en la Comunidad de Sistema de Recomendación (ver SIGIR’16 para una introducción progresiva). No en vano, han habido dos premios “Best Paper”,  en las conferencias WSDM’18 y RecSys’18, por desarrollar estimadores contrafactuales que podrían prevenir la discriminación en selecciones (el artículo de Airbnb sobre la discriminación en selección en online A/B Testing es buen ejemplo de ello). Esto se suma a los esfuerzos en otras conferencias de Machine Learning bien conocidas, donde la inferencia causal está ganando fuerza (ver los workshops de NeurIPS y ICML )\nPese a todo, aunque la aplicación de la inferencia causal en el Machine Learning sigue siendo limitada, vemos una mayor interacción futura entre estos dos campos (del mismo modo que sucedió con la Inferencia Bayesiana y el Deep Learning).\nYoshua Bengio, uno de los padres del Deep Learning, explica en una entrevista en el MIT Technology Review  que “necesitamos llevar al Deep Learning a ser capaz de razonar, aprender de la causalidad”, porque “si se tiene un buen modelo causal del mundo, puedes generalizar incluso situaciones no familiares. Eso es crucial. Los humanos somos capaces de proyectarnos a nosotros mismos en situaciones muy diferentes a las de nuestro día a día. Las máquinas no pueden, porque no tienen esos modelos causales”.\nÉtica en IA\npor Juan Murillo and Roberto Maestre.\nSe están haciendo grandes esfuerzos para definir, desarrollar e integrar la ética en la Inteligencia Artificial (IA). Las más altas instituciones, especialmente en la UE (pero también en los EE. UU.), trabajan en la elaboración de principios que definan las reglas dirigidas a permitir la implementación de soluciones de IA que eviten riesgos de trato discriminatorio hacia los ciudadanos. Una vez que tomamos consciencia de que la IA puede amplificar el sesgo que existe en los datos de entrada de un modelo, debemos identificar y mitigar estos efectos en el desarrollo de sistemas que guían decisiones o desencadenan acciones automáticamente. Una tendencia interesante en la industria es desarrollar nuevas métricas (tanto de clasificación como de regresión) para monitorear dichos sesgos. A lo largo de 2018 hemos detectado un gran impulso orientado a considerar estos aspectos en el desarrollo de nuevos modelos, integrando todos estos conceptos.\nPero, a la hora de bajar de los grandes principios éticos a los casos de uso específicos, a menudo las empresas temen perder la necesaria capacidad discriminante que garantiza la sostenibilidad de sus negocios si aplican medidas antidiscriminación en los modelos que determinan a quién prestar servicio y a quién no. Por ejemplo este tipo de modelos en el sector financiero rigen decisiones de aceptación de solicitudes de crédito (admisión) y de asignación de precio a la financiación (cálculo de la tasa de interés en función del perfil crediticio del solicitante). El prescindir del empleo de una variable por considerarla discriminatoria en admisión, o la decisión de disminuir la horquilla de variación en el cálculo de precios para ser más ecuánime puede impactar en los resultados, y éste impacto ha de ser medido, como muestra este ejemplo. Por otro lado en este otro artículo investigadores del MIT Media Lab han demostrado que pequeñas variaciones de bajo impacto en negocio comportan incrementos de alto impacto positivo en favor de la equidad, medida mediante el aumento del número de solicitantes aceptados por los modelos de admisión, que en ningún caso trabajan con datos sensibles. En definitiva, la Inteligencia Artificial nos puede ayudar a encontrar el punto de equilibrio que maximice los beneficios empresariales y sociales.\n\n"
    },
    {
      "title": "Los Mejores Cursos Online en Ciencia de Datos",
      "url": "https://www.bbvadata.com/es/the-best-online-courses-for-data-scientists/",
      "date": "23 noviembre, 2018",
      "content": "\nEl perfil profesional del científico de datos está en auge, y, como consecuencia, cada vez tenemos una mayor oferta de formación a nuestra disposición, tanto para aquellos que buscan especializarse en campos específicos con un nivel avanzado como para los que desean iniciarse en el mundo de la ciencia de datos.\nAdemás de los cursos y másteres presenciales, que ya abordamos anteriormente en este artículo, existen una gran variedad de cursos online para aquellos que requieran de mayor flexibilidad en su formación. Desde los conceptos fundamentales del aprendizaje autónomo (Machine Learning) hasta especializaciones en modelos probabilísticos, la oferta de formación en línea respecto a ciencia de datos se adapta a todos los niveles y necesidades. En las siguientes líneas presentamos una selección de cursos recomendados por científicos de datos de BBVA Data & Analytics, ordenados según el nivel de especialización. Se trata de un listado basado en la opinión personal de varios compañeros y compañeras, por lo que no se debe tomar como una guía exhaustiva.\nNuestra selección centra el aprendizaje en los conocimientos y técnicas fundamentales en ciencia de datos, abordando conceptos como Deep Learning, Machine Learning, o Natural Processing Language (NLP), entre otros. Algunos de estos cursos en línea son específicos para la materia tratada, mientras que otros forman parte de un programa de formación más amplio que incluye más cursos.\nNivel básico / principiante\nEn este primer nivel incluimos aquellos cursos en línea que aportan una base de conocimientos para iniciarse en la ciencia de datos. Ofrecen una primera aproximación a los lenguajes y herramientas utilizados, así como una explicación de conceptos como Deep Learning o Machine Learning.\n\nEspecialización en Data Science.  Creado por la Universidad Johns Hopkins. Este programa de formación está compuesto por 10 cursos de diferente duración destinados a principiantes que se quieran iniciar en el mundo de la ciencia de datos. Su enfoque persigue aglutinar todos los conceptos básicos que guían la ciencia de datos. Disponible en Coursera.\nDeep Learning Course. Creado por Lazy Programmer. Como en el caso anterior, se trata de un conjunto de cursos en línea enmarcados en el concepto de Deep Learning, pero que repasa prácticamente todos los conceptos de ciencia de datos desde un nivel básico, pasando por Machine Learning o NLP. La sencillez de las explicaciones son uno de los puntos fuertes de estos cursos.\nMachine Learning Foundations: a case study approach. Creado por la Universidad de Washington. Con un tiempo de dedicación aproximado de entre 30 y 48 horas, este curso enseña casos prácticos para la mayoría de los problemas típicos de Machine Learning, como regresiones, predicciones o clasificaciones. Bien explicado. Es el primer curso del programa de formación Machine Learning Specialization, formado por cuatro cursos y disponible en Coursera.\n\nNivel intermedio\nAunque se siguen abordando conceptos básicos de Machine Learning y Deep Learning, el grado de especialización es mayor en este segundo nivel. Hay cursos con más peso teórico y otros centrados fundamentalmente en la parte práctica, pero todos requieren algunos conocimientos previos.\n\nMachine Learning. Creado por la Universidad de Stanford. Con un temario estructurado en 11 semanas, se trata de un curso que contempla todos los conceptos básicos de Machine Learning desde un enfoque teórico, pero sin perder el punto de vista aplicado. El nivel es alto y resulta muy recomendable para quien quiera empezar a saber qué es Machine Learning y sus matemáticas, así como para quien quiera refrescar algunos conceptos vistos en el pasado. Eso sí, requiere esfuerzo para entender los detalles matemáticos y dedicación para realizar las prácticas. Es impartido por Andrew Ng, gurú en Machine Learning, y está disponible en Coursera.\nText Retrieval and Search Engines y Text Mining and Analytics. Creado por la Universidad de Illinois. Ambos cursos pertenecen al programa de formación especializado en Data Mining y constituyen una buena introducción a las técnicas clásicas de NLP, la analítica de texto, los buscadores y los sistemas de recomendación basados en contenido. Su tiempo de dedicación estimado es de 20 horas cada uno. Además, estos dos cursos introducen el modelo vectorial de documentos, TF-IDF, técnicas de evaluación de clasificadores y buscadores, análisis de sentimiento, clusterización de documentos, modelado de topics y visualización. Son, sin duda, un buen complemento a otros cursos de NLP para Deep Learning que suelen centrarse en problemas supervisados. Disponibles en Coursera.\nPractical Deep Learning For Coders. Creado por el Data Institute en la USF e impartido por Jeremy Howard (Kaggle’s Grandmaster #1 durante varios años), requiere una dedicación de siete semanas. El temario está dividido en dos partes y el enfoque es muy práctico, abordando contenido que va desde “Embeddings”, “Structured Deep Learning”, “Collaborative Filtering” hasta “Natural Language Processing” o “Generative Adversarial Networks”. Además, utiliza una librería de elaboración propia sobre PyToch. Tiene nuevas ediciones año tras año por lo que está muy actualizado a las últimas técnicas en Deep Learning.\n\nNivel avanzado / experto\nLos cursos mostrados aquí están dirigidos para profesionales que quieran ampliar su formación, por lo que se abordan temas más específicos y con mayor complejidad técnica.\n\nProbabilistic Graphical Models.  Creado por la Universidad de Stanford. Un total de tres cursos (“Representación”, “Inferencia” y “Aprendizaje”) componen este programa de especialización sobre modelos probabilísticos gráficos (Bayesian Networks y Markov Networks) con aplicaciones en aprendizaje automático y modelos de decisión. Uno de los mejores aspectos es que los ejercicios (Matlab, Octave) requieren bajar al detalle, lo que en este caso facilita mucho interiorizar los conceptos teóricos de la parte de inferencia y aprendizaje. \nDeep Learning by Google. Creado por Google y pensado para un tiempo de dedicación de tres meses. Se trata de un curso avanzado de introducción al mundo de Deep Learning, muy completo y que aborda temas a muy bajo nivel, por lo que resulta perfecto para aquellos que quieran convertirse en profesionales. Disponible en Udacity.\n\n \n\n"
    },
    {
      "title": "Clasificación de Texto Financiero: un Análisis de los diferentes Métodos deEmbeddingde Palabras",
      "url": "https://www.bbvadata.com/es/4062-2/",
      "date": "6 noviembre, 2018",
      "content": "\nDurante los últimos dos veranos he trabajado como becario en BBVA Data & Analytics, un tiempo en el que he desarrollado un trabajo que me gustaría explorar en este artículo. Lo que sigue es un resumen técnico de los aprendizajes adquiridos trabajando con embedding de palabras para las categorizaciones de pequeñas descripciones en transacciones financieras.\nLa clasificación de texto adquiere gran relevancia en muchos de los retos de negocio que abordamos en BBVA Data & Analytics. En este sentido, la categorización de transacciones en cuentas es una de las soluciones más útiles y aplicables, y ha ido creciendo hasta convertirse en en una de las características más reconocibles de la premiada app móvil del BBVA. Asignar cada una de las operaciones realizadas por un cliente a una categoría de gasto es fundamental para la gestión de una cuenta, ya que permite, entre otras funcionalidades, navegar a través de categorías de importancia, como “gastos del hogar”, “seguros” o “viajes”. Además, esta categorización añade una etiqueta semántica que resulta esencial para el desarrollo de otros servicios -por ejemplo, para la previsión de gasto en cada categoría-.\nEn el aprendizaje autónomo, el problema de la clasificación de texto es definido de la siguiente manera: dada una pareja de datos (sj , cj ), donde sj es una cadena de texto y cj es una de las posibles etiquetas o categorías para el texto dado, nos surge la necesidad de conocer la función de clasificación F atribuyendo etiquetas a una muestra arbitraria de cadenas de texto.\nIntroducción al embedding de palabras\nAl abordar los problemas de clasificación de texto mencionados anteriormente, una de las cuestiones principales que surgen es cómo tratar los textos computacionalmente. En este sentido, los científicos de datos tratan el texto como un objeto matemático: aplicando diferentes técnicas, se han encontrado métodos para transformar las palabras en puntos de un espacio euclídeo con una dimensión conocida. Esta transformación asigna un vector único a cada palabra que aparece en el vocabulario, y es conocida como embedding.\nCon esta transformación matemática somos capaces de realizar nuevas operaciones en texto que no eran posibles anteriormente: en un espacio con D dimensiones podemos añadir, restar y mapear múltiples vectores, entre muchas otras operaciones. Por consiguiente, con la intención de evaluar esta “transformación” (embedding), es deseable que estas “nuevas” operaciones puedan traducirse a propiedades semánticas entre las palabras existentes, propiedades que no estaban claras antes de la transformación. En concreto, en la mayoría de transformaciones el producto escalar de vectores normalizados (similitud coseno) es utilizado como una función de similitud entre palabras. Implícitamente, la mayoría de transformaciones de texto significantes dependen de hipótesis distribucionales, que estipulan que palabras que aparecen en el mismo contexto tienen un significado similar.1)Se puede llegar a conclusiones únicamente cuando se tiene suficiente texto. De hecho, aparecen defectos en los métodos que asumen estas hipótesis cuando no existe suficiente texto disponible.\tjQuery(\"#footnote_plugin_tooltip_1\").tooltip({\t\ttip: \"#footnote_plugin_tooltip_text_1\",\t\ttipClass: \"footnote_tooltip\",\t\teffect: \"fade\",\t\tfadeOutSpeed: 100,\t\tpredelay: 400,\t\tposition: \"top right\",\t\trelative: true,\t\toffset: [7, 7]\t});\nOtro problema relevante que no se aborda en este artículo es cómo combinar vectores de texto para crear representaciones vectoriales de oraciones, enfoque que formó parte de mi investigación en 2018. En este artículo se aborda la simple aproximación de normalizar la media de vectores de texto, ya que se trata de la construcción matemática más natural, aunque no siempre es el mejor método de representación.\nContext-count y context-prediction embeddings e “información privilegiada”2)En el contexto del aprendizaje autónomo se refiere a los datos que utilizan conocimiento adicional durante el entrenamiento.\tjQuery(\"#footnote_plugin_tooltip_2\").tooltip({\t\ttip: \"#footnote_plugin_tooltip_text_2\",\t\ttipClass: \"footnote_tooltip\",\t\teffect: \"fade\",\t\tfadeOutSpeed: 100,\t\tpredelay: 400,\t\tposition: \"top right\",\t\trelative: true,\t\toffset: [7, 7]\t});\nExisten muchas maneras de realizar el embedding de una palabra. Una posible distinción de las diferentes técnicas de embedding se establece entre los embeddings context-count based y context-prediction based, dependiendo de si los vectores son obtenidos a través del recuento de apariciones de palabras o mediante la predicción de palabras en un contexto dado. Los modelos tradicionales dependen mayoritariamente de matrices obtenidas mediante el recuento de apariciones de palabras y los contextos en los que aparecen, mientras que el último intento de predecir una palabra basada en su contexto (o viceversa) es mayoritariamente utilizado por modelos neuronales. Es importante tener en cuenta que estos tipos de embeddings tratan de extraer información de una palabra utilizando su contexto, y por tanto dependen, implícitamente, de hipótesis distribucionales. Una buena explicación sobre la factorización de matrices en métodos de conteo puede encontrarse aquí.\nAdicionalmente, respecto al objetivo de la clasificación de texto, existe otra posible diferencia entre los métodos de embedding. Una vez que la base de datos está etiquetada, se podrían utilizar las etiquetas de oraciones para crear embeddings con mayor significado. En algunas ocasiones la comunidad se refiere a estos métodos en el sentido de tener acceso a información privilegiada, una idea que también es utilizada en visión artificial para ordenar imágenes.\nAlgunos ejemplos de embedding de palabras\nBag of Words\nEl embedding más común es conocido como Bag of Words (BoW), y consiste en mapear cada palabra de un vocabulario V  {wj}j∈N como un vector codificado en caliente. En otras palabras, un vector formado por ceros, excepto un único 1 situado en el lugar correspondiente respecto a la posición que representa esa palabra en el índice.\nHay importantes inconvenientes que siguen a esta definición, aunque este embedding simple es muy eficiente desde el punto de vista del rendimiento, como veremos más tarde en este artículo. Algunas contrapartidas de BoW son la incapacidad de elegir la dimensión del embedding, que es fijada por el tamaño del vocabulario, así como la dispersión y dimensión del embedding resultante, teniendo en cuenta que el tamaño del vocabulario puede ser extremadamente grande. Sin embargo, el factor diferencial más importante de BoW respecto a cualquier otro embedding es que el modelo únicamente aprende del vocabulario y no del texto de la base de datos, haciendo imposible adquirir ninguna noción de similitud o distinguir si dos palabras son diferentes o no (con este codificado, todas las palabras son equidistantes).\nMetric Learning\nUna posible mejora de BoW en un embedding que ajuste los vectores aprendiendo una métrica que sea capaz de diferenciar oraciones en nuestra base de datos considerando las etiquetas, y, de este modo, tener acceso a major información de las similitudes entre palabras parecidas.\nEn resumen, el Metric Learning surge de un “data-free embedding” (como BoW) y propone un embedding transformado. Esta transformación es realizada de manera que los embeddings de las oraciones de la misma clase aparecen juntas, mientras que se encuentran alejadas de los embeddings de las otras clases. Esto da lugar a un embedding que tiene en cuenta la información semántica proporcionada por las etiquetas de las oraciones.\nWord2Vec\nPor otro lado, el embedding no supervisado más ampliamente utilizado es Word2Vec, desarrollado por Tomas Mikolov. Word2Vec aprende embeddings de palabras mediante pruebas de predicción en un contexto dado (Continuous Bag of Words) o intentando predecir un contexto dada una palabra (Skip-Gram), siendo, en consecuencia, un embedding context-prediction dependiente de hipótesis distribucionales. Word2Vec ha logrado muy buenos resultados en tareas relacionadas con texto. Con suficiente texto, la similitud coseno detecta con precisión los sinónimos del texto, así como los embeddings con la información suficiente para resolver preguntas tales como “Hombre es a Mujer, lo que hermano es a…”. Con aritmética vectorial podemos determinar que el resultado es el vector que representa a “Hermana”. Ésto es utilizado con mucha astucia para reducir el sesgo de los embeddings. El ejemplo anterior es, además, ilustrativo de cómo los embeddings puede aprovechar la estructura de un espacio euclídeo para obtener más información de palabras que no se pueden detectar en un espacio original de las palabras.\nPráctica con embedding de palabras\nEn esta sección presentaré algunos resultados sobre el uso de embedding de palabras que aprendí durante mi tiempo en BBVA Data & Analytics:\n\nHeurísticas para conocer cuándo Word2Vec ha aprendido una solución aceptable\nEn bases de datos pequeñas, podemos utilizar metric learning con pequeñas cantidades de oraciones supervisadas y hacer trabajar embeddings con la misma calidad con la que Word2Vec entrena millones de oraciones\n\nImpresiones entrenando Word2Vec\nEstas percepciones se centran principalmente en la elección de la cantidad correcta de epocs durante el entrenamiento de un modelo Word2Vec, que buscan definir una métrica e hipótesis sobre el comportamiento en el aprendizaje de sinónimos de Word2Vec cuando no hay suficientes datos disponibles.\nLa cantidad de epocs que un modelo de Word2Vec tiene que entrenar es un hiper parámetro que puede dificultar el ajuste en algunos casos. Sin embargo, utilizando la evolución de la matriz de similitud MtM, donde M es la matriz normalizada, nos puede servir para conocer si nuestro word2vec ha aprendido la representación vectorial adecuada o no. Por medio de la fórmula de recall (MtM)ij = similarity(wi, wj ) and −1 < (MtM)ij < 1lo deseable es que solo los valores que son sinónimos tengan valores altos. Como se puede apreciar en la figura 1, el cuadrado de la suma de los elementos no diagonales de MtMse incrementa y posteriormente se estabiliza. Hay que destacar que un valor más bajo de esta métrica no significa un mejor entrenamiento.\n\nEste comportamiento puede ser interpretado de la siguiente manera: en primer lugar, el modelo inicia de forma aleatoria los vectores asignados a cada palabra, para luego mover la mayoría de ellos a la misma región del espacio del embedding. Posteriormente, como el modelo encuentra diferentes palabras en diferentes contextos, las separa hasta situar cerca únicamente a los sinónimos.  Algo más de análisis sería necesario para confirmar esta hipótesis sobre cómo la distribución de los vectores afecta al entrenamiento.\nComo hemos mencionado anteriormente, Word2Vec proporciona sinónimos con alta precisión, diferenciando incluso entre representaciones de la misma palabra, como por ejemplo “third” o “3rd”, utilizando la similitud coseno de vectores normalizados. Esto es así porque, cuando el modelo ha explorado suficiente texto, aprende a identificar que los contextos de “third” y “3rd” con muy similares en la mayor parte de las ocasiones. Sin embargo, cuando no hay suficientes datos (o el modelo no es entrenado con suficientes epocs), en ocasiones los sinónimos encontrados (aquellas parejas de vectores con similitud coseno) muestran un comportamiento no encontrado en embeddings totalmente entrenados: las palabras que el modelo ha encontrado siempre juntas son tratadas como sinónimos. Por ejemplo, si una cantidad pequeña de texto suele contener “BBVA Data” pero no contiene ningún ejemplo de “BBVA” sin “Data” a continuación (o viceversa), el modelo no separará los vectores entre estas dos palabras porque siempre aparecen en el mismo contexto. Así, el modelo asignará una alta similitud. Esto puede ser útil para advertir de la necesidad de incluir más texto o más entrenamiento. En conclusión, pensar en el word2vec como una técnica que separa todos los vectores y solo mantiene la cercanía de las palabras similares en lugar de un unificador de palabras similares puede ser útil a la hora de determinar si nuestro embedding está entrenado correctamente.\nEn la siguiente figura animada se muestra la evolución de la matriz MtMen entrenamiento, algo que puede ayudar desarrollar cierta intuición sobre el proceso de entrenamiento. En la animación, los índices bajos corresponden a las palabras más frecuentes, y se observa cómo el modelo es capaz de diferenciarlas con pocos epocs.\n \n\n\nEmbeddings de palabras supervisados frente a no supervisados en pequeños conjuntos de datos\nComplementariamente, se llevó a cabo un experimento para evaluar los embeddings de palabras en una pequeña base de datos de preguntas cortas. En concreto, 20.000 preguntas de Stack Overflow, etiquetadas entre 20 posibles categorías. Los embeddings de BoW and Metric Learning fueron entrenados utilizando las oraciones de la base de datos y se realizó el etiquetado con un clasificado de regresión logística. Además, utilizando preguntas no etiquetadas de Stack Overflow, diferentes Word2Vecs fueron entrenados con diferentes cantidades de oraciones, incluyendo frases del propio conjunto de datos. La media de precisión principal es utilizada como la métrica para comparar entre los clasificadores. Los resultados se pueden comprobar en la figura 2. Como se puede observar, Word2Vec necesita una gran cantidad de oraciones para superar a Metric Learning (incluso entrenando Word2Vec con 2,5 veces más de oraciones de la base de datos original, no se supera por mucho BoW). En conclusión, BoW y especialmente, Metric Learning, son mucho más adecuados en el caso de no disponer de mucho texto. Está bastante claro que los embeddings de palabras basadas en la predicción son superiores a cualquier otro embedding, pero, en este caso particular, (un pequeño conjunto de datos de oraciones cortas) un enfoque más clásico arrojó mejores resultados.\n\nConclusiones y reconocimientos\nPara algunas aplicaciones, no es sencillo adquirir una gran cantidad de texto y, por esta razón, el modelo Metric Learning u otros similares ejecutan bien utilizando únicamente la pequeña cantidad de datos del propio conjunto de datos.\nEn resumen, mis prácticas de verano en BBVA Data & Analytics resultaron en una muy buena experiencia de aprendizaje y me ayudaron a explorar las profundidades del Procesamiento de Lenguaje Natural (NLP), gracias a la orientación de mis mentores. Especialmente, me gustaría agradecer a José Antonio Rodríguez su paciencia y su tiempo supervisando tanto mis prácticas en la empresa como todos los nuevos conceptos que me ha enseñado.\n References   [ + ]    1. ↑ Se puede llegar a conclusiones únicamente cuando se tiene suficiente texto. De hecho, aparecen defectos en los métodos que asumen estas hipótesis cuando no existe suficiente texto disponible. 2. ↑ En el contexto del aprendizaje autónomo se refiere a los datos que utilizan conocimiento adicional durante el entrenamiento.  \tfunction footnote_expand_reference_container() {\t\tjQuery(\"#footnote_references_container\").show();        jQuery(\"#footnote_reference_container_collapse_button\").text(\"-\");\t}    function footnote_collapse_reference_container() {        jQuery(\"#footnote_references_container\").hide();        jQuery(\"#footnote_reference_container_collapse_button\").text(\"+\");    }\tfunction footnote_expand_collapse_reference_container() {\t\tif (jQuery(\"#footnote_references_container\").is(\":hidden\")) {            footnote_expand_reference_container();\t\t} else {            footnote_collapse_reference_container();\t\t}\t}    function footnote_moveToAnchor(p_str_TargetID) {        footnote_expand_reference_container();        var l_obj_Target = jQuery(\"#\" + p_str_TargetID);        if(l_obj_Target.length) {            jQuery('html, body').animate({                scrollTop: l_obj_Target.offset().top - window.innerHeight/2            }, 1000);        }    }\n"
    },
    {
      "title": "Desarrollamos un motor de predicción con incertidumbre en colaboración con Google Cloud",
      "url": "https://www.bbvadata.com/es/bayesian-deep-learning-meets-google-cloud-for-a-better-forecasting-engine-at-bbva/",
      "date": "24 octubre, 2018",
      "content": "\nEl reciente white paper publicado por BBVA Data & Analytics en colaboración con Google Cloud presenta el desarrollo de un modelo de Deep Learning para la predicción de series temporales. El modelo implementado incorpora la incertidumbre en las predicciones, lo que sin duda supondrá una mejora de la experiencia del cliente en diferentes productos y servicios que ofrece actualmente la entidad, como las herramientas de control de gastos. Además, el documento advierte de la necesidad de tener en cuenta los cuatro pilares que permiten el despliegue exitoso de un producto de Inteligencia Artificial en la industria:\n\nEl trabajo presentado en este documento constituye una solución integral lista para ser desplegada a producción. Algunas de las conclusiones de este trabajo son:\n\nIncorporar la incertidumbre como parte de la predicción incrementará la confianza en el resultado ofrecido.\nLas técnicas de Deep Learning son una herramienta muy potente en la predicción de series temporales y en el aprovechamiento de los datos acumulados con el fin de mejorar la precisión del modelo.\nLa plataforma de Google Cloud facilita un rápido despliegue a producción, al tiempo que se garantiza la protección de los datos utilizados y se aprovechan las ventajas de una infraestructura tan potente.\nSe están explorando métodos para implementar soluciones que eviten una filtración de datos deducida a partir de las salidas del modelo.\nNuestra propuesta centrada en la protección de la privacidad permitirá medir el nivel de interpretabilidad del modelo, pudiendo así mitigar el impacto de enfoques engañosos en cuanto a equidad, inclusión y auditado.\n\nEl equipo ha explorado los diferentes escenarios técnicos para ejecutar la Inteligencia Artificial, que son tenidos en cuenta dependiendo de las características de uso:\n\n\n \nPara más información técnica recomendamos ponerse en contacto con nuestros candidatos a doctorado industrial Axel Brando e Irene Unceta. También se puede ampliar la información con los siguientes papers elaborados por los propios autores:\nRoberto Maestre,\nLa equidad en Machine Learning se vuelve “mainstream”\nCómo construir una inteligencia artificial más ética y transparente\nSobre el uso responsable de datos y algoritmos\nJordi Nin,\nCómo los gatitos ayudaron al BBVA a detectar el fraude con tarjetas de crédito\nJose A. Rodríguez y Axel Brando,\nPredicciones con incertidumbre en modelos de aprendizaje profundo\nIrene Unceta,\nLa Interpretabilidad, clave en un mundo controlado por algoritmos\n \n \n\n"
    },
    {
      "title": "Cómo las Iniciativas Basadas en Datos pueden Salvar Vidas Infantiles",
      "url": "https://www.bbvadata.com/es/how-data-driven-initiatives-can-save-young-lives/",
      "date": "10 octubre, 2018",
      "content": "\nMillones de acontecimientos e historias ocurren cada día en Ciudad de México, una de las megalópolis más densamente habitadas del mundo. Cada uno de estos acontecimientos genera datos anónimos que, monitorizados y utilizados de forma adecuada, podrían resultar en una mejora sustancial de la vida de sus habitantes. No sólo esto, sino que además la ciudad podría alentar la participación pública en iniciativas basadas en datos. Uno de esos sucesos le ocurrió a Yesenia González en 2015, cuando su esposo y su hija casi fueron atropellados por un coche que conducía en dirección contraria. Ambos caminaban en dirección al colegio, como todas las mañanas.\nA raíz de este desagradable suceso, Yesenia, especialista en análisis de negocio en BBVA Bancomer, decidió echar mano de los datos para conocer más sobre las estadísticas de niños y niñas involucrados en accidentes en el camino a la escuela, y descubrió que en el lugar donde casi atropellan a su hija, una persona había muerto en un incidente de tráfico unos años antes. Además, descubrió que los incidentes de tráfico son la principal causa de muerte en niños y niñas de 1 a 14 años en la Ciudad de México.\nSu curiosidad analítica la llevó a un viaje en busca de datos que pudieran describir un problema que estaba afectando a los niños de toda la ciudad. En colaboración con la ONG “Liga Peatonal” y un equipo de investigadores de la Universidad Autónoma de México, reveló datos que estaban ocultos incluso para los planificadores urbanos de la Ciudad de México. La primera necesidad era localizar los incidentes de tráfico en los que había estado involucrado algún menor de 14 años. Para dar con estos datos, Yesenia y el resto de colaboradores tuvieron que dirigirse a los hospitales, ya que los datos de la policía no permitían una correcta geolocalización de estos sucesos. De esta forma, se recabaron datos de más de 6.000 accidentes con peatones, aunque sólo utilizaron aquellos que tenían suficiente información para geolocalizar, alrededor del 20% del total. Luego iniciaron el tedioso trabajo de limpieza, estandarización y estructuración de las diferentes fuentes de información que recogen. Incluso el aparentemente fácil proceso de agregar la ubicación de la mayoría de las escuelas en la Ciudad de México no fue un paso sencillo y les supuso una importante tarea de de limpieza, corrección de duplicados y geolocalización de referencias. Una vez que los datos estaban allí, la imagen fue más nítida: al solapar la densidad de los accidentes de tráfico con la de las escuelas, pudieron encontrar cuáles eran los centros con mayor riesgo para los estudiantes.\n\nCon fondos de la Fundación Bernard van Leer, Yesenia colaboró en el esfuerzo para recopilar y presentar datos que estaban ocultos en un engorroso sistema de fuentes. Así nació el sitio web llamado Caminito de la Escuela, en el que califican la seguridad de las zonas próximas a los centros de enseñanza de la Ciudad de México, y cuya navegación invita al lector a seguir los pasos de aprendizaje, evaluación, empoderamiento y transformación en los que se basó el desarrollo de la propia iniciativa. El sitio ha visualizado un problema que estaba oculto en datos dispersos y que ahora ha permitido a las escuelas tomar medidas para limitar los peligros del tráfico denso durante el horario escolar.\n“Nuestro trabajo muestra la importancia de las iniciativas de datos abiertos y gobierno abierto para adoptar políticas responsables que protejan a los más vulnerables de nuestra sociedad”, dice Yesenia en una reunión con BBVA Data & Analytics para hablar sobre el potencial del uso de los datos para el bien común. “La atención de los medios de comunicación creó presión para permitir la intervención en las áreas más peligrosas. Los datos pueden salvar vidas”, añade Yesenia.\nEl trabajo de Yesenia muestra la importancia de contar con datos abiertos y bien estructurados, que los ciudadanos u organizaciones puedan utilizar en conjunto con otras fuentes para enriquecer nuestra visión del mundo y encontrar soluciones basadas en datos. En la actualidad, Yesenia y otros padres y madres continúan luchando por resolver este problema, explorando otras formas de participación y colaboración.\nEn lugares como la Unión Europea, donde el acceso a los datos y la transparencia en las instituciones es exigido por ley, hay más posibilidades de llevar a cabo este tipo de iniciativas. Inspirados por la búsqueda de datos que realizó nuestra compañera Yesenia en Ciudad de México, nos propusimos hacer una prueba para conocer la facilidad de acceso a estos mismos datos en Madrid. En este caso, la Policía Municipal dispone de un base de datos de accidentalidad con bastante nivel de detalle, con la que es posible georeferenciar cada uno de los sucesos. Por otro lado, los datos de localización de todo tipo de centros de enseñanza están abiertos a los ciudadanos, ya sea a través del portal de datos abiertos de la ciudad de Madrid o de la Comunidad Autónoma. Sin embargo, aunque estos datos son accesibles para una ciudad como Madrid, no es así para municipios menores, donde el acceso a esta información es mucho más limitada, así como otras instituciones públicas, en las que aún queda camino por recorrer.\n\n"
    },
    {
      "title": "Self-Service Performance Tuning for Hive",
      "url": "https://www.bbvadata.com/es/self-service-performance-tuning-for-hive/",
      "date": "1 octubre, 2018",
      "content": "\nHive is a very powerful data warehouse framework based on Apache Hadoop. The two together provide stable storing and processing capabilities for big data analysis. In this article, we will analyze how to monitor metrics, tune and optimize the workflow in this environment with Dr. Elephant.\nHive is designed to enable easy data summarization, ad-hoc queries, and big data analysis. It provides SQL capabilities to users, which make the exploration of the structured data saved in HDFS (Hadoop Distributed File System) easily and fast.\nGiven the high complexity of this large and complex framework, it is really important to ensure that every component performs optimally. While we can always optimize the underlying hardware resources, network infrastructure, operating system, and other components of the stack, users only have control over optimization of the jobs that run on the cluster.\nAt BBVA Data & Analytics, we have staff with different levels of expertise with Hadoop, using different frameworks to run their Hive jobs. In order to help users understand and optimize their flows, we scheduled some regular training sessions about how to tune Hive jobs.\nLately, we have been testing a tool that helps users to understand, analyze, and improve the performance of their flows: Dr. Elephant, open sourced by Linkedin. This is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all metrics, runs analysis on them, and presents results in a simple way for easy consumption. Its goal is to improve productivity and increase cluster efficiency by making it easier to tune the jobs. \nWe have identified three key features that may help anyone tuning their jobs using this tool with Cloudera Hive 1.1.0 and MapReduce as the engine.\nTuning number of mappers and reducers\nWhen the number of mappers or reducers are not correctly adjusted the task will suffer from performance issues and the task will slow down.\nNumber of Mappers\nLet’s start our analysis with some common problems which involve a wrong definition of the number of mappers. An excessive number of mappers can be produced by small files in HDFS. Small files cause HDFS file system to not reach configured block size, which is a known HDFS performance problem. The harmful effects will be:  NameNode service overloaded due to the high number of processes, task performance slows down due to container instantiation and finally, an excessive number of small output files, so the problem aggravates.\nOn the other hand, an insufficient number of mappers may be caused by huge files in HDFS so the job does not have enough parallelism.\nWithin Dr. Elephant, we can use the MAPPER TIME metrics to detect these problems and solve them. In essence, these metrics specify the problem severity and attach an link “Explain that details the possible causes and suggests solutions.\nWhen the metrics show a low “Average task runtime” (in the order of several seconds or few minutes), a low “Average task input size” (only a few MB or KB) and a high “Number of tasks” (maybe a few thousands) it means that the number of mappers is excessive.\n\nWhen the metrics show a high “Average task runtime” (around one hour or more), a high “Average task input size” (several GB) and a low “Number of tasks” (some tens) it means that the number of mappers is not enough.\n\nThe suggested solution for Hive is tuning the parameters that adjust the input file size:\nmapreduce.input.fileinputformat.split.maxsize\nmapreduce.input.fileinputformat.split.minsize\nWhen the files are small, by setting the correct  mapreduce.input.fileinputformat.split.minsize the files will be merged to reach at least the size indicated in the parameter before the data will be passed to the mappers process. \nTip: Review that the parameter hive.input.format is correctly set to permit the file merging.\nhive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\nNumber of reducers\nNow, let’s focus on the number of reducers. The excessive or insufficient number of reducers cause the task to slow down. At the same time, an excessive number of reducers can generate small files in HDFS perpetuating the problem with mappers. In Dr. Elephant, the REDUCER TIME metrics help to detect these problems. \nWhen the metrics show a low “Average task time” (only several seconds or few minutes) and a high “Number of tasks” (maybe a few thousand) it entails that the number of reducers is excessive.\n\nWhen the metrics show a high “Average task time” (one hour or more) and a low “Number of tasks” (some tens or less) it means that the number of reducers is not enough.\n\nThe suggested solution of Dr. Elephant is to explicitly set the estimated number of reducers or the number of bytes to process for every reducer:\nmapreduce.job.reduces\nhive.exec.reducers.bytes.per.reducer\nTip: If you need a lot of reducers make sure that the parameter hive.exec.reducers.max is not limiting you.\nhive.exec.reducers.max=1999\nMerge small files\nA nice feature in Hive is the automatic merging of small files, this solves the problem of generating small files in HDFS as a result of the number of mappers and reducers in the task. It can be set only in map tasks (parameter hive.merge.mapfiles) and mapreduce tasks (parameter hive.merge.mapredfiles) assigning a true value to the parameters below:\nhive.merge.mapfiles=true\nhive.merge.mapredfiles=true\nIn the same way, we need to set the threshold that launches an “automerge” stage with the parameter hive.merge.smallfiles.avgsize. If the average size of the resulting files is below the value set, a new stage will be launched to merge the output. You can use the block size, in the example below 128MB (134217728 in bytes).\nhive.merge.smallfiles.avgsize=134217728\nThe files are merged to be at least of the size in bytes indicated with the value of the parameter hive.merge.size.per.task. It must generate files of several blocks, in the example below 8 blocks of 128MB.\nhive.merge.size.per.task=1073741824\nTip: If the data are stored in ORC format you must review that the parameter hive.merge.orcfile.stripe.level is set to false.\nhive.merge.orcfile.stripe.level=FALSE\nIdentifying wasted memory\nA common mistake made by users when trying to increase the performance of their queries is tuning the memory assigned to mappers and reducers to high values, even if it is not necessary. Dr. Elephant can surface these mistakes with two groups of metrics, \nMAPPER MEMORY, and REDUCER MEMORY\n\nIncreasing mappers and reducers’ memory does not necessarily entail an increase  in query performance. In order to find out how memory is allocated in our jobs, we can check the metric “Requested Container memory”, which shows the memory size of memory reserved by the mappers and reducers containers. Another useful metric is the “Max Physical Memory”,  that tells us the max size of memory actually used by the mappers or reducers. \nBoth values should be similar to ensure a proper use of the requested memory. For instance, a task is clearly ill-defined when the memory usage is much lower than the requested memory, mainly because the task is wasting memory. \nIn these situations, Dr. Elephant helps to tune the memory of mappers and reducers. Using the “Max Physical Memory” value defined before plus 1GB — Yarn containers are JVM processes and need extra memory for code and internal structures — we can set the mappers and reducers initial memory size to that value and adjust again in the next execution. So, looking at the above example, where the upper limit of memory for mappers and reducers was 686MB and 357MB respectively, we could now suggest a 1,5GB (1536MB) values for both memory parameters:\nmapreduce.map.memory.mb=1536\nmapreduce.reduce.memory.mb=1536\nTip 1: Remember that mappers and reducers’ size is limited by the yarn container size. Do revise the value of the parameter yarn.scheduler.maximum-allocation-mb before you increase the size too much. As an example, the following line defines a container with 16GB of maximum size so you can not rebase that limit:\nyarn.scheduler.maximum-allocation-mb=16384\nTip 2: Our recommendation is to set the JVM heap as 80% of the container memory. You may change the ratio for all container types (mappers and reducers) with the parameter:\nmapreduce.job.heap.memory-mb.ratio=0.8\nor set different values for mappers and reducers setting Java opts parameters:\nmapreduce.map.memory.mb=4096\nmapreduce.map.java.opts=-Xmx3276m\nmapreduce.reduce.memory.mb=6144\nmapreduce.reduce.java.opts=-Xmx4915m\nDealing with skewed data\nData skew comes up when a small number of containers (either mappers or reducers) have to process almost all the amount of data. For instance, skewness is characterized by two groups of data, one with a high number of tasks and small average size, and the other with a few numbers of tasks and high average size.\nSkewed data impacts the global execution time of the tasks, as the volume of data among processes is distributed unequally.\nDr. Elephant has two groups of metrics that help you detect data skewness in map and reduce stages. These are MAPPER DATA SKEW and REDUCER DATA SKEW groups.\n\nConsider first the case in which the mapper data is skewed. Here you can set the parameters that split or combine the input file according to the “Tuning number of mappers” section. In the same way, you can use the “slowstart” parameter (mapreduce.job.reduce.slowstart.completedmaps) to mitigate the delay at the beginning of the reducer stage.\nIn the reducer case, skewness could be caused either by duplicated keys in joined queries or by aggregation keys in “group by” queries. At this point, you could analyze the source data and try to solve the problem rewriting the query. In other cases, you may use the Hive skewed join or skewed “group by” optimization. In particular, this optimization detects the skew and launches a new stage that processes only the skewed data set in a more parallel and distributed way.\nConcerning parameter configuration, you may enable the skewed join optimization using the following parameters:\nhive.optimize.skewjoin=true\nhive.optimize.skewjoin.compiletime=true\nYou also need to define a skewness threshold, defined as the number of records from which a data set can be considered  skewed (it depends on the size of the data set!):\nhive.skewjoin.key=100000\nMoreover, in the “group by” skewed optimization you need to activate the below-indicated Hive parameter. Besides that you should also set the number of records as before:\nhive.groupby.skewindata=true\nhive.skewjoin.key=100000\nFinally, to “fine” tune the number of mappers to use in the new stage you should use hive.skewjoin.mapjoin.map.tasks, and hive.skewjoin.mapjoin.min.split parameters to define the desired parallelism and the size of the fragments in which the skewed data are divided. In the next example these two parameters take the default values of 10000 mappers and 32MB respectively:\nhive.skewjoin.mapjoin.map.tasks=10000\nhive.skewjoin.mapjoin.min.split=33554432\nThat’s all! We hope that this brief introduction will serve as a starting point to benefit from the “diagnoses” provided by Dr. Elephant.\nReferences:\nOpen Sourcing Dr. Elephant\nGitHub Dr. Elephant\nDr. Elephant user guide\nDr. Elephant tuning tips\nApache Hive\nApache Hive Language Manual\n\n"
    },
    {
      "title": "Desarrollando Software de Código Abierto en una Gran Empresa",
      "url": "https://www.bbvadata.com/es/building-open-source-software-in-a-large-corporation/",
      "date": "19 septiembre, 2018",
      "content": "\nEl mundo funciona con datos, pero sin la naturaleza dinámica, accesible y adaptable del Software de Código Abierto (OSS, por sus siglas en inglés — Open Source Software), el ritmo de explotación de los campos ricos en datos sería dolorosamente lento. Imagine un mundo de ciencia de datos sin Linux, Python, Anaconda o Tensorflow, sólo por citar algunos ejemplos relevantes de software de código abierto.\nEn los últimos años, la tendencia a utilizar OSS para el procesamiento y análisis de datos ha ido en aumento hasta el punto de que la mayoría de las soluciones actuales son OSS. Las corporaciones más grandes, incluyendo Google, Microsoft o Facebook, están contribuyendo activamente a ello. A pesar de las historias de éxito, la adopción de la filosofía de código abierto requiere desafiar muchas ideas establecidas, especialmente si queremos atraer a colaboradores activos en lugar de sólo usuarios que no están aportando conocimiento al proyecto, o usuarios libres.\nIdentificamos tres temores que impiden que las empresas adopten el OSS:\n\nMiedo a que los competidores se beneficien de sus inversiones.\nLas hojas de ruta de la producción corporativa no favorecen el desarrollo a largo plazo de las herramientas necesarias para crear productos futuros.\nNo controlar la narración. Las corporaciones heredadas están acostumbradas a construir una narrativa perfecta y no les gusta la competencia abierta a pesar de sus afirmaciones.\n\nOSS 101 para ejecutivos\n\nContribuir al OSS siempre es rentable. En un estudio reciente de la Unidad de Estrategia de la Escuela de Negocios de Harvard, el profesor Frank Nagle afirma que: “Pagar a los empleados para que contribuyan a dicho software [OSS] aumenta la productividad de la compañía al usar el software hasta en un 100%, en comparación con la competencia libre” . Los colaboradores activos producen más y mejor código.\nEn hojas de ruta de producción: Las herramientas necesarias para crear aplicaciones suelen dar lugar a grandes proyectos que, hoy en día, son en su mayoría OSS. Lo que las corporaciones desarrollan son típicamente aplicaciones de negocio construidas sobre eso. Los horarios y los incentivos son diferentes en cada caso. Las comunidades de OSS son meritocráticas, mientras que en el legado de desarrollo corporativo, la jerarquía, los presupuestos y los calendarios dan forma a los productos.\nEn OSS puedes controlar la experiencia de aterrizaje creando introducciones, tutoriales y documentación, pero no puedes controlar la narrativa. Los usuarios lo prueban y deciden si se adapta a sus necesidades. Todos los reclamos pueden ser verificados. Para las corporaciones esto puede sonar como una pesadilla, pero no lo es. En realidad, la transparencia es una de las principales razones por las que el OSS ya ha reemplazado el antiguo modelo de negocio. Los usuarios técnicos están saturados de narrativas sobrevaloradas e inverificables y buscan transparencia.\n\nLa comunidad\nSolíamos pensar en el OSS como un fragmento de código estable, útil y bien documentado que se libera continuamente. Ese es el resultado visible en un caso de éxito. Pero la mayoría de los proyectos de OSS fracasan antes de esa etapa. Las razones son diversas, incluyendo la creación de un producto que nadie quiere, pero una razón crucial es no construir una comunidad. Bootstrapping una comunidad es la parte más difícil y es un requisito para el éxito. Los fundadores deben invertir muchos recursos para producir algo lo suficientemente utilizable y fiable como para atraer a los primeros adoptantes en una etapa en la que no hay muchos incentivos para utilizar el producto. Aquí es donde las grandes corporaciones tienen el poder de apoyar nuevos esfuerzos. Pueden ayudar apoyando el desarrollo de la marca, la comunicación, la infraestructura, los contribuyentes a tiempo parcial y el presupuesto. Las empresas deben visualizar el OSS como una oportunidad para invertir en proyectos ambiciosos en su propio beneficio por una fracción insignificante de lo que invierten en soluciones de negocio. Esto mejora la motivación y la productividad de los empleados. También les da el poder de elegir lo que apoyan o recortar sus fondos sin costo alguno en cualquier momento.\nEn BBVA D&A\nComo ejemplo, BBVA lanzó Jazz en diciembre de 2017 como OSS. Jazz era originalmente un almacén de datos distribuido de alta eficiencia con un cliente R (ahora en cran) desarrollado para uso interno en BBVA Data & Analytics. Tras su lanzamiento como OSS, su principal autor decidió rediseñar el producto, convirtiéndolo en una plataforma de IA que aborda problemas que no están bien o no están soportados en absoluto en las plataformas principales.\nJazz ha ido avanzando a lo largo de los primeros nueve meses de 2018, la refactorización ya está completa y se está abordando una lista de tareas pendientes. Proyectos igualmente ambiciosos tuvieron un período de dos a cuatro años “bajo el radar” antes de que fueran relevantes. Para esos estándares, Jazz es todavía joven. El tiempo dirá si se convierte en otro proyecto abandonado o en una historia de éxito de OSS de una gran corporación, en la que algún día podría contar a sus creadores: Esta misión es demasiado importante para mí como para permitir que la pongas en peligro.1)Hal in 2001: A Space Odyssey, 1968\tjQuery(\"#footnote_plugin_tooltip_6694_1\").tooltip({\t\ttip: \"#footnote_plugin_tooltip_text_6694_1\",\t\ttipClass: \"footnote_tooltip\",\t\teffect: \"fade\",\t\tfadeOutSpeed: 100,\t\tpredelay: 400,\t\tposition: \"top right\",\t\trelative: true,\t\toffset: [7, 7]\t});\n References   [ + ]    1. ↑ Hal in 2001: A Space Odyssey, 1968  \tfunction footnote_expand_reference_container() {\t\tjQuery(\"#footnote_references_container\").show();        jQuery(\"#footnote_reference_container_collapse_button\").text(\"-\");\t}    function footnote_collapse_reference_container() {        jQuery(\"#footnote_references_container\").hide();        jQuery(\"#footnote_reference_container_collapse_button\").text(\"+\");    }\tfunction footnote_expand_collapse_reference_container() {\t\tif (jQuery(\"#footnote_references_container\").is(\":hidden\")) {            footnote_expand_reference_container();\t\t} else {            footnote_collapse_reference_container();\t\t}\t}    function footnote_moveToAnchor(p_str_TargetID) {        footnote_expand_reference_container();        var l_obj_Target = jQuery(\"#\" + p_str_TargetID);        if(l_obj_Target.length) {            jQuery('html, body').animate({                scrollTop: l_obj_Target.offset().top - window.innerHeight/2            }, 1000);        }    }\n"
    },
    {
      "title": "Predicciones con Incertidumbre en Modelos de Aprendizaje Profundo",
      "url": "https://www.bbvadata.com/es/improving-predictions-in-deep-learning-by-modelling-uncertainty-2/",
      "date": "13 septiembre, 2018",
      "content": "\nEn BBVA trabajamos desde hace tiempo para aprovechar los datos transaccionales de nuestros clientes y los modelos de Deep Learning para ofrecer una experiencia bancaria digital personalizada y relevante a nuestros clientes. Nuestra capacidad de prever los ingresos y gastos recurrentes en una cuenta es una de las más diferenciadoras de la industria. Este tipo de previsión ayuda a los clientes a planificar presupuestos, a actuar en un evento financiero o a evitar sobregiros, reforzando al mismo tiempo el concepto de “tranquilidad” que un banco como BBVA quiere transmitir.\nLa aplicación de técnicas de Machine Learning para predecir la recurrencia de un evento y la cantidad de dinero involucrada permitió el desarrollo de dicha funcionalidad. Como complemento a este proyecto, en BBVA Data & Analytics también invertimos en investigación para estudiar la viabilidad de métodos de Deep Learning en la previsión de cuentas, como ya explicamos en “There is no such thing as a certain prediction”. El objetivo no era tanto la mejora del sistema actual como la generación de conocimiento para validar el potencial de dichas técnicas.\nComo resultado de dicha investigación, hemos observado que el uso de técnicas de Deep Learning permite reducir los errores de previsión. Sin embargo, también observamos que sigue habiendo muchas situaciones donde los gastos no son previsibles, y los modelos de Deep Learning para regresión no disponen de un mecanismo para determinar la incertidumbre de la predicción, es decir, su fiabilidad..\nPor ello, no sólo es importante hacer buenas predicciones en términos generales, sino también detectar los casos en los que la predicción puede tener amplios límites y por tanto nos interesa modelar esa incertidumbre. Esto sería útil para mostrar a los clientes sólamente predicciones fiables, o para priorizar acciones relacionadas con la predicción. Para ello, hemos extendido la investigación a modelos de Deep Learning Bayesiano que modelan la incertidumbre a la vez que las predicciones.\nMedir la incertidumbre para ayudar a los clientes\nPero, ¿qué es exactamente la incertidumbre? Aunque este concepto se encuentra a debate en los campos de la estadística y la economía, suele ser clasificado en dos categorías: la incertidumbre aleatoria – aquella que se refiere a la variabilidad de soluciones correctas dada la misma información- y la incertidumbre epistémica -la incertidumbre relacionada con nuestro desconocimiento sobre qué método utilizar para solucionar el problema o incluso nuestra ignorancia respecto nuevos tipos de datos que no vimos en el pasado-.\nDesde un punto de vista matemático, lo que hacemos es buscar una función que, a partir de una información dada -el histórico de movimientos de un cliente-, nos devuelva el valor del siguiente movimiento que realizará. Sin embargo, este método tiene una limitación: en nuestro caso, dada la misma información -histórico de movimientos-, el resultado no tiene por qué ser el mismo -imaginemos dos clientes diferentes con el mismo histórico de gastos e ingresos pero con distinto comportamiento en el futuro-.\nLa siguiente figura muestra gráficamente el concepto de incertidumbre, y responde a la pregunta de cuál será el valor del punto rojo en el intervalo de tiempo dado. En la mayoría de situaciones reales, no es fácil decidir cuál será el siguiente valor de la línea roja (punto rojo).\n\nEn este sentido, un modelo con y sin incertidumbre realizaría las siguientes predicciones:\n\nUno de los modelos más avanzados para obtener resultados son los basados en Deep Learning (Aprendizaje Profundo), por su capacidad de aprender funciones muy versátiles de grandes conjuntos de datos. La forma común de construir estos modelos de Deep Learning proporciona una estimación puntual del valor a predecir, pero no refleja el nivel de confianza o seguridad en la predicción. Lo que nosotros buscamos es la probabilidad de que ocurran diferentes resultados a partir de la misma información histórica de diferentes clientes, esto es, la distribución de probabilidad.\n\nPodemos modelizar diferentes tipos de incertidumbre aleatoria. Por un lado, la incertidumbre homocedástica refleja una varianza constante para todos los clientes, mientras que nuestra implementación heterocedástica muestra la desviación respecto a la media para cada cliente, dado su patrón de gastos e ingresos.\n\nComparando este modelo con modelos epistémicos de Deep Learning, detectamos una clara mejora en el nivel de confianza de las predicciones respecto a las soluciones planteadas anteriormente, especialmente al poder medir la variabilidad.\nEl tipo de incertidumbre que hemos tenido en cuenta adquiere gran relevancia a la hora de proponer un nuevo modelo que pueda predecir los próximos gastos e ingresos de los clientes. En el siguiente gráfico, cada punto representa un cliente situado según su error de predicción real y su puntuación de incertidumbre prevista. Podemos observar cómo los puntos amarillos (que representan a la mayor parte de los clientes) tienen un bajo error de predicción y una baja puntuación de incertidumbre, por lo que podemos entender esta puntuación como confiable.\n\nLas posibles aplicaciones de este enfoque a soluciones de negocio van desde usos sencillos como la posibilidad de adelantarse a movimientos con gran precisión o recomendar medidas para mejorar la salud financiera, hasta más avanzados como notificar movimientos inusuales que aumentan la varianza de la predicción en un determinado momento de la vida del cliente. También se ha detectado un potencial enfoque desde el punto de vista del cliente, el cual, utilizando la incertidumbre, podrá valorar diferentes acciones financieras en base a sus propios criterios de riesgo.\nEsta investigación ha sido sintetizada en un artículo aceptado para publicación y presentado en el ECML-PKDD 2018, que está disponible en el servicio e-Print de arXiv.\n\n\n"
    },
    {
      "title": "¿Cómo será el Banco del Futuro?",
      "url": "https://www.bbvadata.com/es/what-will-the-bank-of-the-future-look-like/",
      "date": "31 agosto, 2018",
      "content": "\nNo hace mucho tiempo, mucha gente todavía se presentaba en una oficina bancaria con un cheque de pago de su empleador, recogía el dinero y se iba con una nueva pila de facturas, que nunca se veía entre las columnas del gigantesco edificio hasta el siguiente día de pago. No vieron el caso de uso de una cuenta bancaria, ya que el colchón o el viejo cofre en el dormitorio de la familia serviría como caja fuerte para los ahorros que queden. Esta última función, en la que los bancos se convirtieron en custodios de depósitos, otorgantes y agentes de inversión, fue la más prominente hasta el comienzo del nuevo siglo, interrumpida por la crisis financiera más perjudicial desde El Crash de 1929.\nLos bancos de hoy, fuertemente influenciados por la crisis de confianza desencadenada en 2008, están redefiniendo sus fronteras gracias a las nuevas tecnologías, el big data y la analítica. En BBVA Data & Analytics tratamos de anticipar y dar forma a esta nueva etapa en la que la banca contribuirá a garantizar la tranquilidad de los clientes con una interacción más personalizada, más transparencia y eficiencia, tratando de adaptarnos a las necesidades y prioridades de una sociedad que cambia con rapidez.\nEl propósito del nuevo banco irá mucho más allá de salvaguardar los depósitos, monitorear el riesgo sistémico o ser intermediarios clave en el sistema financiero mundial. Lo más probable es que el banco del mañana no se defina por el dinero, sino por las muchas maneras en que los activos nos ayudan a alcanzar las metas de la vida. Además, el banco del mañana no se definirá por las interacciones de hoy con un cajero, un gestor de activos, un cajero automático o una aplicación. Esas líneas serán borrosas e incluirán un fuerte componente de Inteligencia Artificial que personalizará las interacciones y las hará ubicuas, y no forzará al cliente a interacciones que consumen mucho tiempo con el proveedor de servicios, sólo a interacciones significativas en las que sus deseos y metas son el principal impulsor de las decisiones financieras.\nComo un servicio centrado en el cliente, la banca confiará en la Inteligencia Artificial para automatizar la toma de decisiones financieras que apoyen los objetivos del cliente. Para esta comprensión, los deseos, las cargas y el comportamiento del cliente son clave, y eso es lo que la analítica avanzada está ayudando a hacer al aprovechar los datos individuales y colectivos en muchos frentes. Comprender estas dinámicas puede proporcionar cuentas autodirigidas que predigan sus necesidades, inversiones inteligentes para maximizar los ahorros o decisiones financieras que apoyen los valores en la sociedad futura en la que usted cree. Al mismo tiempo, esta comprensión del complejo sistema del que forman parte integrante los bancos permitirá a éstos corregir y ser transparentes en lo que respecta a la fijación de precios justos, los sesgos, la gestión de riesgos y el apoyo a objetivos más ambiciosos, como la inclusión financiera, el desarrollo económico o la adaptación al cambio climático.\nEl banco del futuro tiene que entender que el dinero será una mercancía que no estará presente en forma de billetes o monedas, sino como un medio para tener nuevas experiencias. Debido a esto, el banco del futuro tendrá que adaptarse rápidamente y encontrar su lugar en un ecosistema que tendrá a la ciudad inteligente como su escenario más importante y en el que las interacciones de pago serán perfectas. La logística, la salud, el transporte y la educación también serán impulsadas por la Inteligencia Artificial y se entremezclarán con la banca en formas difíciles de prever.\nAdemás, el futuro del trabajo en una economía impulsada por Inteligencia Artificial obligará a los bancos a proponer soluciones y formas de apoyar un mundo en el que la mayor parte del tedioso trabajo mecánico será realizado por robots y en el que esos objetivos de vida podrían ser radicalmente diferentes a los que tenemos hoy en día.\n\n"
    },
    {
      "title": "La Equidad en Machine Learning se Vuelve “Mainstream”",
      "url": "https://www.bbvadata.com/es/fairness-by-design-in-machine-learning-is-going-mainstream/",
      "date": "10 agosto, 2018",
      "content": "\nLas consideraciones de equidad o “fairness” en el desarrollo de soluciones basadas en Machine Learning (aprendizaje automático) están cobrando fuerza como aspecto clave de la inteligencia artificial y la modelación de comportamientos sociales. Esta semana, el Harvard Business Review publicó un artículo escrito por los responsables de un proyecto de análisis en sanidad que utiliza Machine Learning para detectar a las personas con propensión a tener una enfermedad cardiovascular.\nLos investigadores detrás del Stroke Belt Project han detallado algunos consejos útiles para aquellos que quieren evitar los prejuicios sociodemográficos, raciales o de cualquier otro tipo introducidos por aquellos que preparan los datos, construyen el modelo o optimizan los resultados.\nEn un esfuerzo por promover el concepto “Fairness by Design”, que desde BBVA Data & Analytics tratamos de tener siempre presente a la hora de modelar y muestrear datos, prescriben algunos consejos:\n\nUnir científicos de datos con expertos en ciencias sociales para introducir una perspectiva humanista a los datos y a la solución a implementar.\nAnotar los datos con precaución. Hacer que los responsables de anotar los datos que van a alimentar un modelo de aprendizaje de la máquina sean conscientes de sus posibles sesgos.\nMedir la equidad. Utilizando métricas para medir la imparcialidad que ayuden a corregir el sesgo.\nNo  concentrarse sólo en la representatividad. Una vez que se hayan establecido las medidas de equidad, encontrar un equilibrio entre la representatividad de los casos futuros y la infrarrepresentación de las minorías.\nTener en cuenta la eliminación del sesgo y, si es necesario, evitar la categorización basada principalmente en variables sociodemográficas.\n\nLa equidad es uno de los principales conceptos que BBVA Data & Analytics viene trabajando para incorporar a modelización financiera. Hemos publicado recientemente un artículo científico, “Reinforcement Learning for Fair Dynamic Pricing (Maestre, 2018)”, sobre la importancia de integrar la imparcialidad como principio de diseño algorítmico cuando se confía en la Inteligencia Artificial para hacer proyecciones financieras (véase la Sección B del documento):\n\nAdemás, hemos propuesto métricas para cuantificar la equidad, como señala el artículo de HBR (punto 3 del artículo de HBR):\n\nTambién hemos escrito varios artículos, asistido a conferencias para hablar sobre este tema y ayudar a introducir este debate en la transformación digital que está experimentando BBVA. Se debe prestar especial atención a la segmentación de los grupos (puntos 4,5 del artículo del HBR). Intentamos conseguirlo con nuestros modelos client2vec.\nComo se ha subrayado en el mencionado artículo de HBR, la introducción de “Fairness by Design” no es un obstáculo para la innovación, sino una forma de hacer que la analítica avanzada sea más sólida, más fiable y con mejores resultados en los diferentes segmentos socio-demográficos.\n\n"
    },
    {
      "title": "Investigación y Desarrollo: Cómo se Presenta el Futuro para la UE",
      "url": "https://www.bbvadata.com/es/research-and-innovation-what-the-future-looks-like-for-the-eu/",
      "date": "3 agosto, 2018",
      "content": "\n¿Cuáles son las tecnologías que perfilarán el futuro de la Unión Europea?¿Quiénes son los jugadores implicados en la creación de las innovaciones que están por llegar? Con el propósito de dar respuesta a estas preguntas hemos analizado las fuentes de datos abiertos de la Comisión Europea para entender cuáles son las prioridades estratégicas del futuro más allá del 2020. La fusión nuclear, el grafeno, la supercomputación, los coches autónomos, la movilidad sostenible, la atención sanitaria avanzada o la seguridad energética lideran la carrera hacia el futuro.\nComo parte de nuestro esfuerzo de estar en contacto con la vanguardia de la innovación en la intersección entre finanzas y tecnología, hemos examinado 25 años de inversión europea en innovación desde el programa marco FP4 (iniciado en 1994) hasta el más reciente Horizonte 2020.\nAl observar las mayores asignaciones de fondos del programa H2020 podemos apreciar que la fusión nuclear, el grafeno, la Inteligencia Artificial y la salud son las prioridades principales para la UE. Un avance en cualquiera de estos ámbitos tendrá un gran impacto en el sector de las finanzas y la analítica de datos.\n\n\nDurante más de medio siglo la Unión Europea ha crecido hasta los 28 países miembro, unidos a través de acuerdos comerciales, programas de inversión y, también, estrategias de investigación y desarrollo. Sin embargo, no todos los jugadores implicados son iguales; existen diferencias según el tipo de organización predominante en cada país, los roles que desempeñan, la cantidad de fondos asignados o las temáticas abordadas.\nLos datos muestran miles de proyectos que integran participantes transfronterizos con un aumento de participación de entidades privadas. Los establecimientos de educación superior también juegan un papel importante, siendo líderes en coordinación por número de proyectos desde 2007, especialmente si los medimos por fondos asignados.\nEl Reino Unido es uno de los países líderes tanto como coordinador como participante en proyectos de investigación. Alemania, Francia, España, Italia y los Países Bajos son también líderes en el campo de la investigación y la innovación en la Unión Europea, mientras otros estados no miembros, como Suiza o Israel, comparten una posición relevante como participantes.\nLa Unión Europea ha hecho una importante apuesta por las energías limpias, con la fusión nuclear y el transporte impulsado por hidrógeno como alternativas a una sociedad actual dependiente del carbono. En cuanto a la ingeniería de materiales, la UE se está moviendo para ser líder en el grafeno, revolucionario para la aeronáutica, la computación o la atención sanitaria. La biomedicina y la investigación médica también están siendo potenciadas con importantes inversiones en el programa marco H2020, en muchos casos a través de proyectos que ya recibieron financiación durante el programa anterior (FP7). Otros proyectos, como los relacionados con el sistema de posicionamiento global europeo -a punto de completarse con la puesta en órbita de nuevos satélites- también fueron una prioridad en programas anteriores.\nEl tipo de entidad más presente en los programas marco FP7 y Horizonte 2020 -que abarcan los proyectos de investigación desde el año 2007 hasta la actualidad-  son las empresas privadas (67,5%) seguidas de lejos por los institutos de investigación (10,3%), las instituciones públicas (6,8%) y las entidades de educación secundaria o superior (6,6%). Los países con mayor número de organizaciones participantes durante la última década son Alemania, Reino Unido y España, aunque los primeros puestos varían según el tipo de organización. Alemania y Francia destacan en número de entidades de educación secundaria y superior, mientras que España e Italia son primeros en instituciones públicas participantes.\n\n\nHay que tener en cuenta que nos referimos a entidades únicas, y por esta razón el número de empresas privadas está sobredimensionado respecto al número de participaciones en concreto: una misma institución pública, instituto de investigación o entidad educativa participa de media en muchos más proyectos que una empresa privada. De hecho, si nos fijamos en número de participaciones, las entidades de educación secundaria o superior son las que más veces colaboran (36,8%), seguidas por empresas privadas (30,6%) e institutos de investigación (23,7%).\nDel mismo modo, en el programa marco Horizonte 2020, los institutos de investigación concentran grandes cantidades de fondos, aunque son menos en número de entidades. El Centre National de la Recherche Scientifique acapara más de 550 millones de euros, mientras que la empresa privada con mayor asignación, Siemens, no llega a los 50 millones de euros. Dentro de España destaca el Consejo Superior de Investigaciones Científicas (CSIC), con 161 millones de euros de asignación, y la empresa privada Atos, S.A., con 45 millones de euros.\n\nPor otro lado, las organizaciones participantes en los diferentes proyectos pueden desempeñar diferentes roles: los dos predominantes son el de coordinador y el de participante. Lo más habitual es la presencia de una entidad que coordina el proyecto mientras el resto -en algunos proyectos son muchas- se integran como participantes. Si echamos un vistazo a los datos acumulados de los últimos 25 años, Italia es el país con mayor porcentaje de colaboraciones con rol de participante, mientras que en el Reino Unido una de cada cuatro se llevan a cabo con el rol de coordinación. En datos absolutos, las entidades alemanas son las que más han participado, mientras que, aunque Italia supera a España en el total, nuestro país ha desempeñado más veces el papel de coordinación -por muy poca diferencia-.\n\nEste análisis ha sido posible gracias al compromiso de transparencia de la Unión Europea. Desde BBVA Data & Analytics apoyamos las iniciativas de datos abiertos que facilitan la forma en la que analizamos y entendemos el mundo.\n\n"
    },
    {
      "title": "Destilando Visiones Comunes: el Diseño de Ficciones, Medio de Diálogo entre Negocio, Ciencia de Datos y Diseño",
      "url": "https://www.bbvadata.com/es/finding-a-common-vision-design-fiction-to-establish-a-dialogue-among-business-data-science-and-design-in-finance/",
      "date": "27 julio, 2018",
      "content": "\nEn el pasado hemos hablado de cómo la práctica de design fiction en BBVA D&A ha ayudado a alinear y romper fronteras entre los equipos de científicos de datos y nuestros stakeholders, en torno a los potenciales de la analítica avanzada aplicada a todos los ámbitos del desarrollo de productos. En el marco actual de transformación digital del sector financiero, donde somos punta de lanza,  es una competencia clave, en la que científicos de datos y diseñadores tienen la oportunidad de explorar conjuntamente el desarrollo de productos y servicios basados en datos más allá de la interacción habitual enfocada en la visualización o el prototipado.\nInteresados por seguir descubriendo aplicaciones de las design fictions en entornos productivos, en BBVA D&A estamos explorando cómo esta práctica puede ayudar en una fase aún más temprana del proyecto, cuando tanto los stakeholders como el ámbito de actuación se están definiendo y es necesario consensuar las diferentes expectativas, capacidades y composición de los grupos de trabajo, de cara a desarrollar colaborativamente soluciones innovadoras basadas en datos.\nVisiones y revisiones\nCon el propósito de establecer una visión común entre negocio, ciencia de datos y diseño, se han integrado equipos multidisciplinares que identifican áreas de oportunidad para la producción de productos basados en datos.\nEn este contexto, la producción previa de materiales de ficción (prototipos diegéticos) es de gran valor, ya que comunica narrativamente cómo serían posibles futuros si la solución imaginada existiese. Además, permite presentar de manera tangible una visión en la que los involucrados pueden proyectarse y debatir acerca de si esa realidad es realmente la que se pretende construir.\nEl proceso de elaboración de design fictions, inherentemente flexible, permite iterar de manera rápida y refinar la visión al integrar los comentarios generados tras sus revisiones, hasta alcanzar un equilibrio entre las partes involucradas. Esta dinámica ayuda a esclarecer y definir el camino por el que se pretende avanzar.\nLas ficciones no buscan proponer soluciones, si no envisionar un faro en un horizonte temporal que sirva de guía para abrir el diálogo interdisciplinar, crear sinergias, encauzar esfuerzos y finalmente tangibilizar potenciales.\nAdemás, desde el punto de vista de la ciencia de datos, el proceso de destilación de visiones a través de design fictions también ayuda a identificar las principales hipótesis a testar, la viabilidad técnica y las capacidades que se requieren para abordar las soluciones deseadas.\nDiagrama que muestra el proceso iterativo de la creación de ficciones\nFicción/Acción\nA pesar de la ubicuidad y profunda integración de productos y servicios digitales en la sociedad, creemos que los clientes ven un valor diferencial en el trato humano que la tecnología no puede sustituir fácilmente. ¿Cómo es un futuro en el que los avances en Inteligencia Artificial y de las capacidades avanzadas de análisis de datos se ponen al servicio de nuestros gestores para brindar un mejor servicio y una experiencia diferencial a nuestros clientes?\nUna de las opciones que exploramos desde las perspectivas de negocio, ciencia de datos y diseño es la creación de design fictions como una herramienta útil para abandonar posiciones encontradas o clichés. Por ejemplo, la tendencia actual está enfocada a la automatización de la atención al  cliente, pero, ¿es eso lo que realmente se desea?¿Hay alternativas de diseño? ¿Tiene más sentido usar la tecnología para facilitar el trabajo de los gestores que de los usuarios finales?¿Qué implicaciones supone el despliegue de este tipo de funcionalidad? Estos cuestionamientos nos permiten aprender e iterar sin escribir una sola línea de código, y evitar posibles fricciones con el usuario final.\nImagen de video producido donde se presenta una posible funcionalidad futura dentro de una herramienta real. Dicha funcionalidad permitiría generar respuestas automáticas personalizadas y reducir tiempos de gestión.\nEn este contexto la producción de design fictions cimenta un >diálogo productivo más allá de lo que parece posible o viable, algo que generalmente suele ser diferente para cada una de las partes implicadas en un proyecto. Al no centrarse en una funcionalidad específica sino en la descripción de una solución futura, ponen el foco en el contexto y favorecen la exploración de soluciones.\nFinalmente, desde la perspectiva de la disciplina técnica la creación de design fictions puede servir también para identificar los problemas y riesgos en el uso de los datos, disponibilidad, etc. Como en el caso anterior, revisar el problema con una perspectiva amplia permite pivotar sobre las posibles soluciones y explorar opciones al elegir el tipo de interfaz del usuario, su diseño, cómo recoger el feedback o los datos generados durante el uso. Si la aceptación de ciertas funcionalidades está sujeta a debate, algo que la propia dinámica hace aflorar, es posible definir experimentos que permitan testar si las decisiones de diseño son adecuadas.\n\n"
    },
    {
      "title": "Los Mejores Másteres de Ciencia de Datos para Considerar este Verano",
      "url": "https://www.bbvadata.com/es/the-best-data-science-masters-in-spain-and-online-to-consider-this-summer/",
      "date": "19 julio, 2018",
      "content": "\nSi te dedicas a las TIC sabes que estar actualizado es parte del trabajo diario y que hay que estar al día de las nuevas tecnologías, líneas de desarrollo o técnicas analíticas que surgen constantemente. Llega el verano y muchos nos planteamos realizar algún programa formativo para el curso académico siguiente. La oferta es muy amplia, pero no todos buscamos lo mismo y no todos nos ofrecen lo mismo.\nDesde mi experiencia, como Data Scientist desde hace algunos años, muchas veces me llega la pregunta: “Quiero hacer un máster en Big Data (o Data Science), ¿cuál me recomiendas?” y mi respuesta es siempre la misma: “depende, ¿qué es lo que buscas?”.\nLo primero que tenemos que pensar es si nos interesa incidir en la parte tecnológica o de arquitectura, lo que yo considero “Big Data”, o nos queremos centrar en el modelado estadístico y resolución de problemas de negocio basados en datos, “Data Science” en sentido estricto. También tenemos que considerar si queremos obtener un enfoque más aplicado, o practitioner, o conocer a fondo cómo funcionan los algoritmos de Machine Learning, cuándo convergen y cuáles son las condiciones, por ejemplo.\nEn este post vamos a tratar de responder a esa pregunta desde varios puntos de vista, a partir de mi experiencia personal y de la de otros compañeros de BBVA Data & Analytics, en los que hemos participado bien como alumnos o bien como profesores. No pretende ser un listado exhaustivo de másteres, sino la opinión personal de los autores, así que como tal debe considerarse. También hemos incluido algunos que no conocemos de primera mano, pero de los que tenemos buenas referencias. Por razones de sesgo geográfico, son principalmente programas de Madrid o Barcelona.\n> Descubre también nuestra recomendación de cursos online\nCaso 1: Trabajo como Informático/a desde hace tiempo, y quiero empezar en el mundo de los datos\nEn este caso, entendemos que ya sabes programar, tienes además algo de experiencia, y quieres orientar tu carrera hacia la Ciencia de Datos. En este caso suponemos que, de momento, te interesa conocer a alto nivel qué es, qué enfoques hay, qué herramientas y algunos casos de éxito. Y esperas aprender la base para que luego tú puedas trabajar en este mundo.\nAquí recomendamos algunos programas que dan bastante importancia a la contextualización, motivación, y herramientas, sin entrar muy a fondo en detalles técnicos, o conceptos y pruebas matemáticas. Aquí es, probablemente, donde más oferta hay, de muchas y variadas instituciones. Nosotros proponemos una lista de aquellos que conocemos de primera mano.\n\nCursos online. Los MOOCs, o cursos online, son, en este caso, un buen punto de partida. Siendo Coursera la plataforma pionera, hay también otras como Udemy, con cada vez más adeptos. Con un coste asequible, de relativamente corta duración y mucha variedad, permiten elegir aquellos aspectos en los que nos gustaría aprender. Hay especializaciones, como la de Data Science de Coursera, que tocan casi todos los ámbitos de un proyecto de DS. Recomendamos elegir aquellos de universidades de prestigio, y leer bien el syllabus o índice antes de inscribirse, para ver si el contenido nos encaja.\nCurso en Big Analytics, de UC3M, Universidad Carlos III de Madrid. Con 120 horas, y unos 6 meses de duración, durante los fines de semana (jueves y viernes por la tarde, y sábados por la mañana), incluye una parte de tecnologías Big Data, así como lenguajes de programación para Data Science; todo con un enfoque práctico donde profesores de perfil académico, pero con experiencia en la industria son los encargados de impartir el temario. Se espera que tras finalizarlo tengas las herramientas necesarias para comenzar a ser Data Scientist. Nuestros compañeros Amanda Garci y Rafael Hernández han formado parte del claustro de profesores en su primera edición y consideran que el temario es bastante acertado.\nCurso en Data Science KSchool. En su 11ª edición, tiene un perfil muy similar al anterior, con 230 horas durante 8 meses en horario de fin de semana. Pone el foco en aprender las herramientas y algoritmos para comenzar a dedicarnos profesionalmente a la Analítica de Datos, con un enfoque práctico, impartido por profesionales del mundo de Big Data y Data Science. Incluye una parte sobre Big Data, pero el grueso es Data Science. Aquí también algunos de nuestros compañeros han impartido clases en algunas de las ediciones pasadas de este programa, y en particular Israel Herráiz es el director del máster. En general, el nivel de los alumnos es bastante bueno, y terminan bastante contentos.\nPrograma de Máster, Experto y Especialista en Big Data y Data Science de la UAM, Universidad Autónoma de Madrid. Este programa, que ya va por la tercera edición, tiene una duración de dos años y se imparte los fines de semana (60 ECTS). A diferencia de los anteriores, en este caso aproximadamente el 40% de los temas van sobre tecnologías Big Data -y el 60% sobre Modelado Estadístico y Data Science-, lo que puede resultar en una buena opción para quien vaya buscando más formación en esa parte. Dada la duración, permite profundizar algo más en los temas y tratarlos más a fondo que los anteriores. Aquí he colaborado impartiendo una Master Class cada año, presentando casos de uso de éxito de Data Science en la industria.\n\nCaso 2: Me gustaría comprender los fundamentos teóricos del Aprendizaje Estadístico.\nPuede ser que estés terminando la carrera y quieras enfocarte hacia el mundo de los datos; o ya has trabajado como Data Scientist, pero crees que te falta algo de base teórica/matemática en muchos aspectos y te atrae profundizar más. En este caso, nuestra experiencia es que esto sólo lo vas a obtener en programas oficiales de Universidades. En ellos, se estudian los modelos desde un punto de vista teórico, pizarra, papel y boli, y las tecnologías quedan en un segundo plano. En concreto, nosotros conocemos los siguientes:\n\nMáster Universitario en Inteligencia Artificial de la UPM, Universidad Politécnica de Madrid. Se puede realizar part- o full-time. Lo imparten profesionales reputados en temas clásicos de AI, o en temas muy avanzados e innovadores, para estudiar computación no convencional. Nuestro compañero Roberto Maestre lo conoce y tiene una opinión bastante buena sobre él.\nMáster Universitario en Ingeniería Matemática, de la UC3M. Máster full-time de 2 años de duración. Tiene una primera parte (un cuatrimestre) troncal, y luego se puede elegir la especialidad en Estadística o en Matemática. Es bastante teórico, e impartido por profesores reputados y alta experiencia de esta universidad. Amanda Garci ha sido alumna, y considera que ha sido muy útil para conocer esa parte matemática de los algoritmos de Machine Learning.\nMáster Interuniversitario de Inteligencia Artificial de Cataluña, (UPC, URV, UB, UOC). Uno de los másteres pioneros en España en esta disciplina. Axel Brando>, doctorando industrial con nosotros, realizó aquí sus estudios y nos dice que el nivel es muy elevado, donde se requiere trabajar duramente pero que los resultados son totalmente los que esperaba. Como hándicap, al desarrollarse en varios campus de cataluña, requiere dedicación completa.\nMáster en Investigación e Innovación en TiC, de la UAM, Universidad Autónoma de Madrid. Con varios itinerarios, la rama de Inteligencia Computacional está enfocada en entender y desarrollar programas informáticos inteligentes, incluyendo asignaturas como Machine Learning, Recuperación de Información o Métodos Bayesianos. Como alumna de este máster, puedo decir que es un programa puramente teórico, donde se ven a fondo algoritmos de clasificación, clustering, SVMs, Sistemas de Recomendación, entre otros; y se entiende completamente su funcionamiento ya que se programan desde cero. Tiene una duración de un año, en horario de tarde, pero puede realizarse part-time en dos años, para poder compaginarlo con el mundo laboral, si así se desea. Los profesores son docentes con bastante experiencia, e investigadores de primer nivel, por lo que es un punto muy positivo.\n\nAdemás, estos programas habilitan a estudiar el doctorado, si queremos continuar la carrera investigadora.\n\nPor último, y aunque no es un máster, queremos incluir también uno de los primeros grados en Data Science en España, en la UPC. Con mucha competencia (hay plazas para 30 alumnos), está dirigido por personal muy reconocido en este ámbito.\n\nCaso 3: Si busco algo intermedio…\nSi quieres salir con herramientas que te habiliten para trabajar, pero también tienes inquietud por entender bien los modelos y algoritmos que estás aplicando, hay también algunos programas que te aportarán ambas cosas. Si bien es cierto, que son algo menos frecuente, conocemos el siguiente:\n\nMáster en Fundamentos de Data Science de la UB, Universidad de Barcelona. Como su nombre indica, trata de abordar los fundamentos de Data Science, pero sin perder de vista el enfoque práctico. Tiene un año de duración, en horario de tarde, pero los 60 ECTS también pueden realizarse en dos años en part-time. Los instructores son investigadores de alto nivel, y profesores con experiencia, por lo que la calidad de las asignaturas es elevada. Nuestro compañero Jordi Aranda está terminando este año y reconoce que ha cumplido las expectativas de lo que estaba buscado: profundizar en las técnicas estudiadas, entender las diferencias y similitudes entre diferentes enfoques, y a la vez verlo todo aplicado con las tecnologías y librerías que se utilizan actualmente. También nuestro compañero Jordi Nin va a impartir el módulo de Big Data, repasando las tecnologías que más se utilizan y aplicando su conocimiento de años de experiencia en la industria.\n\n\nMaster en Data Science y Big Data (Afi Escuela de Finanzas): Máster de un año. Su cuarta edición comienza en septiembre. Este máster está diseñado para aquellos que quieran conocer, no solo la base teórica de la ciencia de datos, pero también para aprender cómo implementarlo a casos de uso. Aunque incluye un módulo de Big Data, se enfoca en los fundamentos de la ciencia de datos, con lo que tras la graduación los estudiantes tendrán las herramientas y conocimientos para un trabajo de ciencia de datos aplicada. Los ejemplos y trabajos se centran en casos reales. El módulo final es impartido por profesionales de diversos sectores. Elena Alfaro forma parte de este módulo anualmente. Buena política de prácticas en empresas. (Recomendado por Juande, científico de datos de BBVA Data & Analytics).\n\n\nMáster en Data Science de la URJC, Universidad Rey Juan Carlos. El Máster en Data Science de la Universidad Rey Juan Carlos (1 año de duración, 60 ECTS) proporciona una formación exhaustiva tanto en tecnologías de Big Data como en técnicas de análisis estadístico. Integra conocimientos de ingeniería de datos (Spark, Hadoop, arquitectura en la nube, recopilación y almacenamiento de datos) así como análisis de complejas bases de datos (modelos estadísticos, machine learning, optimización y simulación, análisis de gráficos, visualización y comunicación). De esta forma, el Máster ofrece un camino de aprendizaje que prepara a los y las estudiantes para aplicar la ciencia de datos a múltiples áreas, ya sea en el sector de la industria, la investigación o la innovación. Nuestro compañero de BBVA Felipe Alonso participa como profesor en este programa, mientras que varios científicos de datos de BBVA han asistido a este curso.\n\n\nMáster en Big Data para Finanzas (CUNEF): Máster especialmente enfocado para aquellos perfiles mixtos que quieran combinar los conocimientos de ciencia de datos y finanzas. Tiene una importante orientación aplicada a casos como gestión de activos, estudios de mercado o pricing. Su duración es de un año (60 ECTS) con prácticas en empresas al finalizar. Nuestro compañero Alejandro Vidal imparte Desarrollo de productos de datos y Técnicas de Visualización en este Máster.\n\nEsperemos que este post sea de utilidad para quien quiera iniciarse o profundizar en el apasionante mundo del Científico de Datos; si hay alguno que se nos haya quedado fuera, y queráis comentarnos, o preguntarnos, no dudéis en contactarnos en hello@bbvadata.com o en nuestras redes sociales.\n*Esta lista ha sido elaborada teniendo en cuenta valoraciones y experiencias propias, por lo que puede haber otros másteres muy competentes no reflejados en este artículo.\n\n"
    },
    {
      "title": "La Interpretabilidad, Clave en un Mundo Controlado por Algoritmos",
      "url": "https://www.bbvadata.com/es/lifting-the-veil-of-interpretability-in-a-world-controlled-by-algorithms/",
      "date": "12 julio, 2018",
      "content": "\nCon la cada vez más rápida adopción de la Inteligencia Artificial (AI) y el aprendizaje autónomo (machine learning) en importantes ámbitos de la sociedad, la falta de transparencia en torno a estos procesos se ha convertido en una preocupación perceptible entre la comunidad científica, las autoridades públicas y la sociedad en su conjunto. Desde el mundo financiero al sector de la medicina, pasando por el sistema judicial, la tendencia en los últimos años ha sido delegar la toma de algunas decisiones en modelos de procesamiento automatizado, tanto en el ámbito público como en el privado. Sin embargo, mientras éstos son capaces de superar a los humanos en muchas tareas -operan como eficaces cajas negras- no proporcionan una justificación comprensible sobre el cómo y, quizás lo más preocupante, el por qué toman las decisiones. Esta situación exige una acción inmediata de todos los actores implicados con el fin de hacer cumplir un uso responsable del aprendizaje autónomo y la AI.\nDescifrar el intrincado misterio de cómo obtiene un resultado una red neuronal es, en la mayoría de los casos, una hazaña imposible de resolver por su complejidad, opacidad y la falta de normas y estándares generales. La gran cantidad de cálculos y el sistema distribuido de entradas y salidas en varias capas hace que los mecanismos internos del aprendizaje profundo (deep learning) sean difíciles de comprender, cuando no resultan directamente inaccesibles para la comprensión humana.\nEsta opacidad aleja a compañías, instituciones y profesionales de una plena comprensión de los modelos que despliegan. Además, también impide la identificación y corrección de prejuicios y prácticas injustas. Como señala Kate Crawford, investigadora líder de Microsoft Research y fundadora de AI Now, “los algoritmos son tan buenos como los datos con los que han sido entrenados, y los datos a menudo son imperfectos”. Los que se entrenan con datos etiquetados son susceptibles de reproducir los prejuicios que incorporan. No en vano, los casos en los que los modelos de aprendizaje autónomo han reproducido discriminaciones existentes son muchos y muy variados, y podrían traer graves consecuencias.\nEn la medida en que las decisiones automatizadas tienen cada vez mayores implicaciones en el sistema judicial, sanitario y político, muchas voces están demandando un nuevo abordaje para hacer estos procesos transparentes, comprensibles y desligados de los prejuicios sociales. Los algoritmos son utilizados en la actualidad, por ejemplo, para identificar potenciales peligros para las personas, decidir quién debe ingresar en prisión provisional -porque existe alta probabilidad de fuga- o detectar noticias falsas en las redes sociales. El riesgo de tomar tales decisiones incorporando sesgos en contra de atributos sensibles como el género, la raza o la orientación sexual es demasiado grande como para ser pasado por alto. Por esto, la necesidad de entender los datos con los que se entrenan los algoritmos y la lógica que siguen para distinguir entre posibles resultados es aún más evidente en la medida en que la responsabilidad es hoy, más que nunca, un deber ético.\nPese a que muchas estrategias pueden ser viables, entender cómo trabajan los modelos es una condición necesaria hacia una automatización segura y justa. Debido a la importancia de la interpretabilidad en el aprendizaje autónomo, la disciplina científica trata de explicar los procesos que llevan los sistemas de AI a tomar una decisión determinada. Estos procesos se pueden comparar con una “caja negra”, en la que ingresan las entradas y las salidas salen sin una comprensión clara de lo que sucede entre los dos extremos. La interpretabilidad pretende iluminar lo que ocurre dentro de estas “cajas negras”.\nUna línea de investigación de BBVA Data & Analytics está trabajando precisamente en una forma de interpretar modelos que no fueron creados en su origen para ser transparentes, como las redes neuronales antes mencionadas. Los beneficios de la interpretabilidad son claros: la habilidad de corregir resultados engañosos y proporcionar a los interesados información útil que les permita reivindicar sus derechos. De hecho, vale la pena recordar que quizás no sea tan importante hacer las predicciones correctas sino más bien asegurarse de que “las predicciones correctas estén fundadas sobre las razones correctas”.\n\n"
    },
    {
      "title": "Una Historia de Fútbol Internacional: Cómo Analizar un Juego Colectivo; Individualmente",
      "url": "https://www.bbvadata.com/es/an-international-football-story-how-to-analyze-a-collective-game-individually/",
      "date": "29 junio, 2018",
      "content": "\nMuchas consideraciones juegan un papel importante en el largo y sinuoso camino hacia la historia del fútbol internacional. Un solo jugador -el astuto Maradona, el enérgico Pelé o el elegante Johan Cruyff- puede cambiar el destino futbolístico de todo un país; un solo error también puede poner de rodillas a un equipo muy bien preparado contra todo pronóstico -Corea del Norte venció a Italia en 1966 y quedó segundo de su grupo-.\nPredecir el resultado de un partido o de toda la competición que en estos días tiene lugar en Rusia es difícil, ya que en cada uno de los partidos de 90 minutos de estas 4 semanas pueden ocurrir tantos acontecimientos imprevisibles fuera de modelo.\nPara ayudar a aquellos que intentan usar alguna combinación de datos, instinto futbolístico e inclinación innata a invertir dinero de forma imprudente para predecir el resultado exacto de un partido en una quiniela hemos dado un nuevo uso a un algoritmo de clasificación desarrollado en BBVA Data & Analytics que nos permite ver el “Deporte Rey” desde otros ángulos.\nSi quieres saber más, echa un vistazo a nuestra historia de datos.\nSi quieres saber más, echa un vistazo a nuestra historia de datos.\n\n"
    },
    {
      "title": "Un click más cerca de un administrador de cuentas de Inteligencia Artificial",
      "url": "https://www.bbvadata.com/es/one-click-closer-to-an-ai-powered-account-manager/",
      "date": "22 junio, 2018",
      "content": "\nMientras preparas el café de la mañana, corres junto al río o justo antes de subirte al coche. Ahora puedes completar una transferencia o pago, o administrar la salud de tu cuenta sin perder de vista tu rutina, o llenar la tediosa lista de números de un formulario de transacción. BBVA Data & Analytics ha desarrollado para BBVA Bancomer en México un nuevo motor que permite predecir cuándo y qué tipo de transferencia o pago se va a repetir. Cuando los usuarios acceden a su cuenta bancaria digital, este motor puede adivinar con éxito por qué están ahí y ofrecerles un atajo para completar la tarea en un solo click.\nEn lugar de tener que cumplimentar un formulario de información cada vez que quiera completar una transacción, “One Click”, una nueva herramienta disponible ahora para los clientes mexicanos, les permite elegir entre un conjunto de transacciones recomendadas para mantener su cuenta actualizada. \n“Hemos desarrollado el primer motor de recomendación para transacciones financieras minoristas. Piensa en ello como el carrusel que ves en Netflix cuando te recomiendan películas, pero en su lugar con una lista de pagos recurrentes y transacciones actualizadas casi en tiempo real”, explica Christhian Jonathan Flores, Científico de datos en BBVA Data & Analytics en Ciudad de México. \nLa nueva herramienta pone en común a todos los canales digitales que utiliza una persona para interactuar con su cuenta (móvil, ordenador y cajeros automáticos). Los datos recogidos dibujan una imagen muy detallada de las transacciones recurrentes completadas por un cliente hasta con 8 meses de anterioridad y extrae información sobre la frecuencia, la cantidad y el concepto de una transacción.\nEsta aplicación responde a las particularidades de la cultura financiera en México, un país donde no mucha gente tiene sus cuentas asociadas a pagos domiciliados para evitar las consecuencias de phishing o errores de facturación. Es por eso que cada semana, cada dos semanas, cada mes, o quién sabe con qué frecuencia hay que pagar las facturas, las transferencias entre cuentas tienen que ser satisfechas para que las cosas sigan funcionando como de costumbre. “One Click” puede seleccionar estas transacciones aparentemente aleatorias y frecuentes y recomendarlas una vez que sea necesario repetirlas.\nEl conjunto de algoritmos que está detrás de “One Click” es capaz de identificar transacciones recurrentes con alta precisión e ir aún más lejos. El modelo puede recomendar un rango por el monto de esa transacción, para que el cliente pueda reducir sus modificaciones al mínimo. La herramienta también utiliza técnicas de Procesamiento del Lenguaje Natural para identificar el tipo de transacción, categorizarlas y personalizar las preguntas sobre cómo procesar la transacción recurrente. “¿Quieres pagar el campamento de verano hoy? es una de las preguntas que un cliente puede recibir con el fin de completar una transacción.\nLos clientes que han utilizado “One Click” están muy impresionados con la capacidad de este conjunto de herramientas de Inteligencia Artificial para predecir sus necesidades, dando a este motor de recomendación casi 10 de cada 10 en utilidad. Los siguientes pasos mejorarán la interactividad con el cliente para simplificar la autorización de un pago recurrente.\nEsta herramienta se enmarca en el impulso de BBVA a la personalización de la experiencia del cliente en los canales digitales. “One Click” es un paso hacia una interacción más intuitiva, humana y personal con cada cliente. El futuro de la banca será definido por las empresas que pueden ofrecer una respuesta significativa a una pregunta financiera y que finalmente le ayudará a alcanzar sus objetivos de vida sin preocuparse de tomar la decisión financiera equivocada en el camino.\n\n"
    },
    {
      "title": "El Verdadero Potencial de un Futuro Asistido por Inteligencia Artificial",
      "url": "https://www.bbvadata.com/es/bringing-the-artificial-intelligence-to-its-true-potential/",
      "date": "29 mayo, 2018",
      "content": "\nDesde que el término Inteligencia Artificial fue acuñado por John McCarthy en la Conferencia de Darmouth de 1956 hemos alcanzado grandes logros en este ámbito: hemos enseñado a las máquinas a ver, reconociendo imágenes e incluso texto. Les hemos enseñado a leer, a escuchar, a hablar e incluso a traducir. Y sin embargo nada de ello lo hacen del modo en que lo logramos los humanos, pues no hay comprensión por su parte sino mera concatenación de procesos mecánicos: la potencia de cálculo frente a la creatividad, la combinación masiva de posibilidades frente a la chispa de la consciencia. Si alguien te ha comentado que la singularidad está a la vuelta de la esquina, probablemente carezca de fundamento: hay consenso entre los expertos en que aún faltan décadas para ello, si es que alguna vez se alcanza este hito.\nY sin embargo la revolución de la Inteligencia Artificial es imparable. La cantidad de recursos y talento que le dedican las mayores empresas mundiales está fomentando una nueva revolución industrial en la que la velocidad de adopción es cada vez más rápida que el ritmo de adaptación de la sociedad.\nSegún la consultora Accenture, se espera que las empresas estadounidenses inviertan 35.000 millones de dólares en tecnologías cognitivas antes de 2035, sin tener en cuenta a otros grandes actores, como Europa, China o Japón. Los gobiernos, como recientemente hizo el de Francia, reconocen la importancia clave de este elemento en la economía y en la sociedad del futuro, y en España, el MINETAD coordina un grupo de sabios que participan en la redacción del libro blanco sobre la inteligencia artificial.\nEl hecho de que la automatización cada vez más compleja se aplique a problemas que antes eran únicamente resueltos por el ser humano abre la puerta a grandes oportunidades, pero también a riesgos y amenazas. Si bien, como decíamos previamente, no hemos de preocuparnos en el corto plazo porque las máquinas cobren consciencia y se rebelen contra sus creadores, sí hemos de estar alerta ante la componente humana en su desarrollo, la ética ha de guiar el cómo y el para qué.\n\nLa ética en la programación: la lucha contra los sesgos y la discriminación, la transparencia y auditabilidad de los modelos matemáticos empleados -y un respeto escrupuloso de la privacidad de los usuarios de soluciones basadas en inteligencia artificial- han de guiar los avances técnicos en este campo.\nLa ética en las aplicaciones: como toda innovación, la inteligencia artificial puede ser aplicada para los mejores propósitos, como puede ser la consecución de los objetivos de desarrollo sostenible, o para los peores, como es el caso de las aplicaciones bélicas. La profundidad de los dilemas que todo ello plantea excede a los técnicos que están trabajando en esta disciplina, pero tampoco puede ignorar su conocimiento detallado de las nuevas capacidades.\n\nSi bien en torno a las cuestiones éticas van surgiendo iniciativas para alcanzar compromisos de autorregulación, como son los 23 principios de Asilomar, y además reflexiones que provienen del ámbito gubernamental, como la generada por el CNIL en Francia, hay también voces relevantes que echan en falta un acuerdo a nivel supranacional que refleje un acuerdo y unas directrices mínimas en este nuevo ámbito. En este sentido la Unión Europea está dando sus primeros pasos, aún en el ámbito de la reflexión.\nMáquinas asesinas\nYa en 1942, Isaac Asimov imaginaba que el hecho de que la inteligencia artificial pudiera ser utilizada para tomar decisiones sobre la vida y la muerte requería normas éticas básicas que, desafortunadamente, nunca abandonaron el campo de la ciencia ficción. La tentación de utilizar la ventaja estratégica de convertir la IA en armas (por ejemplo, ver Proyecto Maven y Google) ha sido demasiado grande, pero es una amenaza obvia para la humanidad.\nEn un futuro próximo viviremos en un mundo de Internet de las Cosas, aún más conectado, mucho más eficiente, pero quizás también más vulnerable. Las máquinas inteligentes se van a integrar en todas las infraestructuras, desde el tráfico aéreo, la red eléctrica o el urbanismo. La inminencia de la autoconducción de automóviles, camiones o aviones de pasajeros suscita una preocupación pública comprensible y requiere salvaguardias sólidas para evitar la piratería informática, la toma de decisiones injustas o la respuesta adecuada a acontecimientos inesperados.\nEl gobierno, la sociedad civil, la academia y las empresas deben establecer canales de comunicación para implementar medidas que reduzcan el impacto y la ocurrencia de errores. Además, tienen que crear plataformas en las que medir y corregir las decisiones basadas en algoritmos que perpetúan o amplían la desigualdad y promueven los sesgos. En esta etapa temprana de las máquinas enseñadas por el hombre, existe una alta posibilidad de que los diseñadores de aprendizaje de máquinas transmitan a propósito o inadvertidamente sesgos que hacen que la sociedad sea más desigual en lugar de movernos hacia adelante.\nPara evitar malentendidos, será esencial adoptar medidas orientadas a aumentar la transparencia. El uso de “cajas negras” en la creación de modelos de aprendizaje de máquinas podría empeorar si se confía en que las máquinas inteligentes tomen aún más decisiones. La capacidad de sus programadores para “explicar” el razonamiento detrás de tal decisión mejorará la responsabilidad.\nMáquinas trabajadoras\nEl siguiente reto es cómo integrar la IA en un mundo en el que coexisten diferentes niveles de desarrollo económico, prioridades políticas y mano de obra cualificada. El G20 ya está trabajando en medidas para mitigar el impacto de la automatización del trabajo en los países desarrollados y en ciertos sectores que probablemente estarán altamente automatizados en las próximas décadas. Ideas como la renta básica o los “robots impositivos” deberían ser consideradas, al menos, por los responsables políticos para corregir desigualdades extremas y enviar a un gran número de ciudadanos a los márgenes de la sociedad y sin los recursos para ser incorporados a la economía de la IA.\nComo dijeron los economistas Anton Korinek y Joseph Stiglitz: “la innovación podría conducir a unos pocos individuos muy ricos, mientras que la gran mayoría de los trabajadores ordinarios podrían quedar rezagados, con salarios muy por debajo de lo que eran en el apogeo de la era industrial”.\nAumentados por máquinas\nPero el potencial creativo de la IA es mucho mayor y debería compensar el impacto negativo. La capacidad de los seres humanos para realizar tareas de manera más eficiente podría brindar a los trabajadores habilidades casi sobrehumanas para realizar tareas complejas, que aprovechen las enormes cantidades de información, y que estén personalizados a un nivel que incluso el cliente no conoce es posible.\nUn trabajador de AI “aumentado” puede completar transacciones o servicios mejor distribuidos y más justos, que podrían llevar al mundo a una nueva era de desarrollo e inclusión. No sólo se eliminarán las tareas tediosas, sino que los seres humanos tendrán más tiempo para realizar tareas más creativas y reflexionar sobre el verdadero propósito de las nuevas tecnologías.\nEl diagnóstico médico puede llegar a ser casi impecable, el proceso de prevenir o encontrar soluciones en posibles escenarios publicitarios puede ser analizado en un tiempo récord. Los servicios de educación, salud o finanzas (componentes clave en una economía impulsada por la IA) se volverían más accesibles, incluso para aquellos que han tenido un acceso desigual hasta hoy, porque se podría construir un camino personalizado para la inclusión de cada persona. Al mismo tiempo, los seres humanos no tendrán que estar expuestos a puestos de trabajo peligrosos o en erosión, lo que creará un tipo de mano de obra sólo de servicios.\nEl uso de la inteligencia artificial para el bien social nos ayuda a identificar las necesidades ocultas de las partes vulnerables de la población, a comprender la forma en que se transmite la riqueza y circula por el sistema económico, a luchar contra la corrupción, el fraude y la delincuencia. La IA también puede facilitar canales para una democracia más directa y participativa, acelerar los procesos burocráticos y mejorar la priorización presupuestaria. Para que todos estos objetivos sean alcanzables, estas herramientas de IA deben incorporar elementos cada vez más presentes en el debate sobre el desarrollo de la automatización: “Privacidad mediante el diseño”, “Ética mediante el diseño”, “Equidad mediante el diseño”, “Sostenibilidad mediante el diseño”, “Transparencia mediante el diseño”, etc.\nEn definitiva: las capacidades aumentadas de las que nos dotará la Inteligencia Artificial mejorarán nuestra vida en general, eliminará las tareas tediosas y mecánicas en el ámbito laboral, aumentando la productividad en el medio plazo, y probablemente liberando esfuerzos que quedarán disponibles para tareas más creativas, o simplemente para reducir la carga de trabajo per cápita. Los problemas que esto pueda causar han de acometer desde perspectivas no únicamente tecnológicas, sino socioeconómicas y éticas. El diseño de marcos políticos responsables y sistemas de rendición de cuentas es una necesidad si queremos extraer todo el potencial de las tecnologías de IA.\n\n"
    },
    {
      "title": "Implicaciones Éticas en el Uso de los Datos Personales",
      "url": "https://www.bbvadata.com/es/ethical-implications-of-personal-data-usage-from-corporations/",
      "date": "22 mayo, 2018",
      "content": "\nHace apenas dos meses, el CTO de Facebook, Mike Schroepfer, a través de un comunicado, reconocía que “los datos de más de 87 millones de personas habían podido ser compartidos de forma indebida con Cambridge Analytica”. Cambridge Analytica habría creado perfiles de personalidad en base a esos datos sustraídos indebidamente de la plataforma. La información podría haber servido supuestamente para influir en el resultado de la campaña presidencial de Donald Trump, perfilando al votante como consumidor de un producto (el candidato), lo que permitiría adecuar ese “producto” en base a los perfiles de personalidad generados con los datos de los usuarios. \nQue Facebook, y otras aplicaciones que utilizamos a diario, utilizan nuestros datos con fines comerciales no es nada nuevo, el problema reside en cómo terceros pueden utilizar varias fuentes de datos para usos poco éticos. Muchas redes sociales ofrecen la posibilidad a las empresas interesadas de segmentar audiencias en base a preferencias explícitas que el usuario ha indicado, pero también usando la actividad del mismo. Pero los últimos acontecimientos han provocado la reapertura del debate acerca del uso responsable de datos, y la protección necesaria de los mismos tanto por parte de gigantes sociales como Facebook, como de otras organizaciones, dado que en muchas ocasiones, se puede revelar información sensible con el mero uso de sus aplicaciones o servicios.\nEl presidente de BBVA, Francisco González, insistió recientemente en la presentación del décimo libro de la colección anual que BBVA publica dentro de su proyecto OpenMind, en la necesidad de que exista una regulación específica “que establezca un marco de responsabilidad para los proveedores digitales respecto al mal uso de sus servicios. Y, sobre todo, que dé a las personas más derechos y control sobre sus datos. Debe ser una regulación que englobe a todos los países y a todas las industrias, incluyendo, claro está, la financiera”.\nEste viernes, 25 de mayo de este año, entra en vigor el Reglamento General de Protección de Datos de la Unión Europea (GDPR), cuyo objetivo es dar a las personas más control sobre cómo se utilizan sus datos personales, ya que como veíamos anteriormente, muchas empresas intercambian el acceso a los datos de las personas para el uso de sus servicios. Al reforzar la legislación sobre protección de datos e introducir medidas de aplicación más estrictas, la Unión Europea espera mejorar la confianza en la economía digital emergente.\n¿A qué nos referimos al hablar de datos personales?\nLos datos personales son toda información relativa a una persona física, identificada o posiblemente identificable (directa o indirectamente). La Unión Europea ha ampliado sustancialmente la definición de datos personales en el marco del GDPR, y por ejemplo identificadores online o las direcciones IP ahora son ya considerados datos personales, así como la información económica, cultural o de salud mental, también se consideran información de identificación personal.\nCuando hablamos de datos anonimizados, estamos hablando de datos, que para su posterior análisis, han sido tratados de manera que no se relacione a un sujeto concreto con ellos, pero de igual forma, éstos también pueden estar sujetos a las reglas de GDPR, dependiendo de lo fácil o difícil que sea identificarlos.\nPrincipios del nuevo Reglamento General de Protección de Datos\nEl artículo 5 establece los principios que deben seguirse en el procesamiento de datos. Muchos de estos principios afectan directamente al procesamiento de Big Data, e incluso antes, a los métodos de recopilación y retención de datos.\nLos principios son los siguientes:\na) Legalidad, equidad y transparencia: A menudo, los usuarios no están plenamente informados y no están en condiciones de comprender correctamente la política de privacidad de los servicios (por ejemplo, de las apps) que recogen sus datos. El interesado debe, entre otras cosas, estar en condiciones de acceder fácilmente a la información relativa a sus datos, ponerse en contacto con el responsable de la protección de datos y conocer los métodos y fines del tratamiento.\nb) Limitación de la finalidad: La persona que recoge los datos debe informar al interesado de las finalidades para las que se recogen los datos. Posteriormente, los datos personales sólo podrán ser tratados para estos fines y no podrán ser utilizados con objetivos distintos a los informados. \nc) Minimización de datos: Sólo se pueden recopilar los datos personales que sean necesarios para los fines informados, es decir, no se puede recopilar información personal si no está estrechamente relacionada con esos fines. El objetivo es limitar la cantidad de datos personales tratados debe limitarse al mínimo posible.\nd) Exactitud y actualización: Los datos deben actualizarse y rectificarse constantemente en caso de solicitud por parte de la persona interesada. \ne) Limitación del almacenamiento: Los datos sólo pueden conservarse durante el tiempo necesario para su tratamiento. Pueden almacenarse durante períodos de tiempo más largos, en la medida en que los datos personales se traten únicamente con fines de archivo en aras del interés público, con fines de investigación científica o histórica o con fines estadísticos.\nf) Integridad y confidencialidad: El responsable del tratamiento debe garantizar la seguridad adecuada de los datos personales mediante las medidas técnicas y organizativas adecuadas, incluida la protección contra el tratamiento no autorizado o ilícito, y contra la pérdida, destrucción o daño accidental.\n\n"
    },
    {
      "title": "Eurovisión 2018. Una Excusa más para Demostrar el Valor de los Datos",
      "url": "https://www.bbvadata.com/es/eurovision-2018-just-another-excuse-to-demonstrate-the-value-of-data/",
      "date": "11 mayo, 2018",
      "content": "\nLos datos son cada vez más específicos, con una dimensión más profunda y mayores niveles de riqueza para que individuos u organizaciones puedan sacar conclusiones. Los datos son especialmente granulares y estructurados cuando afectan a las operaciones básicas de empresas, gobiernos u otras organizaciones. Sorprendentemente, esto también aplica a eventos que aglutinan las pasiones de aficionados de todo el mundo: el fútbol, el béisbol o el concurso musical europeo más importante de todos: Eurovisión.\nEl Festival de la Canción de Eurovisión comenzó hace más de 60 años, reuniendo a audiencias de la Unión Europea y más allá de sus fronteras para un maratón internacional de música televisada. Mucho se ha escrito sobre la política del voto, la influencia positiva de la vecindad, los cambios en la preferencia de género, o el bombo mediático. En BBVA Data & Analytics nos encantan los datos y (algunos de nosotros) somos expertos en Eurovisión. Así que con un poco de ayuda de Spotify, una de las compañías que mejor aplica la analítica avanzada y el aprendizaje automático a la música, y algunos modelos estadísticos básicos, hemos analizado la historia de los ganadores de Eurovisión y cómo esto podría ayudarnos a establecer alguna medida de propensión sobre quién podría estar en la cima del concurso de este año en Lisboa, que se espera atraiga a más de 180 millones de espectadores, mucho más que la Superbowl.\nLos datos son la clave\nDel mismo modo que BBVA trata de ampliar la sensorización de datos de sus operaciones y de las experiencias de sus clientes para mejorar el desarrollo de productos y servicios, Spotify ha desplegado un amplio sistema de aprendizaje automático para extraer un gran número de características de canciones, artistas, álbumes o listas de reproducción, que soportan un excelente sistema de recomendación.\nHemos decidido confiar en estos datos para desarrollar un ejemplo de cómo los mismos pueden ayudarnos a entender eventos, tendencias, preferencias, etc. Hemos extraído con fines puramente de investigación nueve rasgos genéricos que describen el género de una canción (sonoridad, bailabilidad, valencia, tempo, energía, en vivo, tono, acústica y ratio de letras) de todas las canciones disponibles que han participado en Eurovisión desde el año 2000. No utilizamos el conjunto completo de datos disponible desde 1956 para evitar distorsiones debidas a los cambios en las preferencias de género en décadas anteriores (parece razonable asumir que el estilo y las preferencias en música pop han cambiado significativamente en el tiempo y hay investigaciones  que lo confirman empíricamente: como se muestra en este trabajo). Luego etiquetamos esas canciones para mostrar si estaban en el top 5 de los participantes o no.\nLa visualización es el prisma\nAl visualizar los datos obtenidos, pudimos identificar tendencias, valores atípicos y determinar la importancia de algunas características sobre otras. Valence, que describe lo positiva o triste que es una canción, ha estado inclinándose hacia el lado pesimista desde el año 2000, con canciones más tristes siendo premiadas desde 2008. Este punto coincide con el comienzo de la crisis económica, por lo que sería interesante investigar más sobre si existe una verdadera relación causa-efecto.\n\nLa canción típica de Eurovisión es básicamente suave, relativamente acústica y cada vez menos positiva que el promedio de éxitos de canciones del verano en países como España. Los ganadores tienden a tener una sonoridad más alta que el promedio (con la notable excepción de Portugal -Salvador Sobral- el año pasado, que fue especialmente suave, lenta y triste), pero no necesariamente una canción para tocar en la pista de baile.\nTambién hemos extraído y analizado lo que hace que una canción sea un éxito de verano en España, y hemos determinado que la receta que crea un gran éxito musical en verano no funciona muy bien con el público de Eurovisión. Esto podría significar que los espectadores leales de Eurovisión, y los que votan y participan, no se ven tan afectados por consideraciones de bailabilidad o de positividad de la canción, sino por otras variables: por ejemplo, el grado de romanticismo de la canción, si entienden el mensaje de la letra (el inglés se ha convertido en un idioma común en el concurso) o si la nacionalidad o la excentricidad del cantante influye en el resultado final.\nEncontrar los “tapados” de Eurovisión de este año\nLos favoritos de este año, “Toy” (Israel) y “Fuego” (Chipre), no parecen seguir la receta de una canción típica de Eurovisión, mientras que “That’s How You Write a Song” (Noruega) parece ser el gran “tapado” por la que el público tradicional de Eurovisión votaría. Hungría, Irlanda, España, Austria o Estonia son, según nuestro análisis, los concursantes que podrían sorprender a los espectadores cuando se emitan los votos finales. No obstante, si de hecho existe una causalidad entre el crecimiento económico y el gusto por las canciones más alegres y si analizamos la propensión de las canciones de Eurovisión a convertirse en éxitos de verano este año, Israel, Alemania o Chipre estarían a la cabeza de la lista.\nA pesar de sus limitaciones, hemos utilizado un clasificador de Random Forest, para entender qué características de audio hacen de una canción un potencial ganador de Eurovisión. Esta solución de algoritmo clasificador aplica una combinación de varios Árboles de Decisión, un método que criba y finalmente clasifica una muestra como un ganador potencial o no. El clasificador sopesa si una canción puede seguir hacia el nivel inferior del árbol si tiene un cierto valor de un conjunto de características (por ejemplo, tempo o bailabilidad). Eventualmente aquellos que pasan las suficientes pruebas binarias del clasificador serán los más similares a los ganadores anteriores y por lo tanto tienen propensión a ganar con la “fórmula de Eurovisión”.\nLos dos modelos de propensión con las canciones de este año (Éxito de Eurovisión y Canción del Verano)\nDestacamos el término “propensión” , ya que hay factores para ganar Eurovisión que obviamente no están representados en nuevo variables descriptivas del sonido, pero nos interesa conocer algo más sobre la propensión que tienen ciertos tipos de canciones sobre otras para hacerse con el certamen.\nEste sábado sabremos con seguridad si lo que se ajusta al gusto de la mayoría para bailar en verano está alineado con lo que puede gustar a la audiencia, muy especial, de Eurovisión.\nEcha un vistazo a nuestra historia de datos\n\n\n"
    },
    {
      "title": "Formación, Una Ventaja Esencial para Retener un Perfil de Científico de Datos",
      "url": "https://www.bbvadata.com/es/training-an-essential-perk-to-retain-a-data-scientist-profile/",
      "date": "3 mayo, 2018",
      "content": "\nEl primer hito de BBVA Data & Analytics se alcanzó en 2011, cuando la Unidad de Innovación de BBVA creó un pequeño equipo de investigación, comprometido con el uso de las fuentes de datos existentes para resolver los retos de negocio. Varias colaboraciones externas con instituciones de investigación, otras startups, corporaciones de telecomunicaciones e instituciones públicas, ofrecieron resultados prometedores, como la identificación masiva de clientes altamente rentables anteriormente etiquetados como clientes estándar, e hicieron visible la necesidad de un enfoque estratégico sólido para el descubrimiento y análisis de datos. Así se constituyó BBVA Data & Analytics, en febrero de 2014, una subsidiaria propiedad de BBVA, principalmente por dos razones: queríamos comercializar productos no financieros basados en datos y contar con un modelo más ágil y diferenciado de atracción y gestión del talento, tanto desde el punto de vista operativo como cultural.\n¿Qué necesitábamos en ese momento?\nPor ello, queríamos atraer a personas altamente cualificadas y con talento, capaces de extraer valor de grandes cantidades de datos, hacer recomendaciones, identificar patrones, proponer nuevas explicaciones a las tendencias y nuevas soluciones a los problemas, y traducirlas a un lenguaje comprensible para nuestros homólogos de las unidades de negocio de BBVA. Necesitábamos científicos de datos. Y queríamos los mejores Científicos de Datos del mercado. Nos imaginamos que todos habéis oído que ésta es la profesión más sexy del mundo. Más que sexy, era un perfil realmente nuevo y desconocido, en aquel momento, y muy difícil de encontrar y definir, sobre todo a la hora de detectar a los mejores profesionales del sector. No es fácil lograr una combinación equilibrada entre estadística, programación, herramientas Big Data, conocimientos de aprendizaje automático, capacidad de resolución de problemas, capacidad de comunicación y una buena intuición para comprender y explorar los problemas del Banco. Por ello, era necesario diseñar un modelo de gestión del Talento y Cultura enfocado a atraer y retener ese talento, una vez encontrado.\nNuestro enfoque\nEl primer paso que dimos fue adquirir un profundo entendimiento de lo que significa trabajar como un Científico de Datos y cuáles son las aspiraciones de los mismos. Durante 8 meses entrevistamos a un promedio de 2 a 3 candidatos al día, incluyendo miembros del personal en el proceso. Pronto tuvimos una idea bastante clara de qué conductores son la clave para ellos. Por ello, diseñamos políticas y procedimientos a la medida de sus necesidades y expectativas de una manera efectiva y atractiva. Entendimos que la gestión a medida, el empoderamiento y la flexibilidad eran primordiales.\nEl mercado es demasiado exigente y paga demasiado por este tipo de perfil escaso y los científicos de datos reciben ofertas de trabajo a diario. Sin embargo, en términos generales, ¡no se trata sólo de dinero! Un salario más alto no es la clave si no viene acompañado de desafíos analíticos y un ambiente de trabajo flexible, algo que hemos aprendido escuchando a nuestros Científicos de Datos.\nAsimismo, mantenerse al día es una necesidad para su desarrollo profesional y una de sus principales preocupaciones. Ser capaces de ampliar sus conocimientos, llevar a cabo investigaciones, publicar artículos científicos y, en última instancia, impulsar el estado del arte de la disciplina.\nTodo esto mientras se trabaja en una empresa con proyectos a tiempo real significa un gran atractivo para estas personas, y está disponible en muy pocas empresas.\nSe trata de perfiles con conocimientos e intereses muy diferentes, a diferentes niveles. Además, estamos convencidos de que cada uno de los miembros de nuestro equipo es el más adecuado para identificar y elegir la formación que mejor se adapte a sus necesidades, motivaciones y nivel de conocimiento. Y así, les entregamos esa decisión. Una de nuestras competencias clave es la Responsabilidad Flexible; nos gusta aplicarlo a todo lo que hacemos, por lo que el plan de formación no iba a ser una excepción. Así, en lugar del clásico catálogo de formación general, que no cubre sus necesidades especiales, hemos diseñado un plan de formación básica, individual y extraordinaria basado en tres ejes, asignando un presupuesto para cada uno de ellos.\nLa formación básica cubre las necesidades transversales del equipo, tales como el dominio de idiomas, el desarrollo de habilidades o alguna herramienta técnica que sea utilizada por todos. Así mismo, cada empleado tiene asignado dinero por cada año para formación individual, basada en su elección personal, siempre y cuando se relacione con nuestra actividad. La formación extraordinaria se presupuesta para eventos y conferencias mundiales, u otra formación que exceda el presupuesto de formación asignado individualmente, de la que el equipo en general puede beneficiarse. Compartir conocimientos, tanto en sesiones en profundidad como en talleres, es una condición principal para obtener la aprobación de su solicitud.\nCómo eligió nuestro equipo su formación a lo largo de los años\nDesde 2015, cuando nuestro modelo de gestión del Talento se hizo realidad, nuestros Científicos de Datos tuvieron la oportunidad de elegir cómo gastar el dinero asignado por persona, cada año, a la formación individual y extraordinaria.\nEl 93% de nuestros Científicos de Datos aprovecharon el pasado año su presupuesto de formación individual, en comparación con el 82% que lo hizo en 2016, y a lo largo de 2017, el 56% de nuestros empleados gastó por encima del 90% del presupuesto asignado.\nEn 2015, los cursos online eran la primera opción de formación, con un 30% sobre el total, mientras que actualmente los cursos presenciales son los preferidos, representando el 59% del total.\nDado que los datos forman parte de nuestra cultura, echemos un vistazo a algunas cifras cuantitativas. En el gráfico siguiente se puede explorar la evolución de las categorías de gasto a lo largo de los años:\n\n\n\n\n\n\n\n                    \nvar divElement = document.getElementById('viz1525338765883');                    \nvar vizElement = divElement.getElementsByTagName('object')[0];                    \nvizElement.style.width='1000px';vizElement.style.height='700px';                    \nvar scriptElement = document.createElement('script');                    \nscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    \nvizElement.parentNode.insertBefore(scriptElement, vizElement);                \n\nAl mismo tiempo, nuestro equipo tiene la posibilidad de mantenerse al día con el estado del arte en Ciencia de Datos asistiendo a numerosas conferencias de su elección. Abajo puedes explorar dónde hemos estado:\n\n\n\n\n\n\n\n                    \nvar divElement = document.getElementById('viz1525338522762');                    \nvar vizElement = divElement.getElementsByTagName('object')[0];                    \nvizElement.style.width='1000px';vizElement.style.height='800px';                    \nvar scriptElement = document.createElement('script');                    \nscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    \nvizElement.parentNode.insertBefore(scriptElement, vizElement);                \n\nLo que hacemos y lo que somos es lo que nos define\nTodo el trabajo que hemos realizado forma parte de una filosofía de aprendizaje continuo y proceso de desarrollo personal, concebido para ser ágil, basado en la escucha activa y bidireccional, y la acción inmediata. Con todas las políticas, procedimientos y estilo de gestión que hemos desplegado aquí, hemos creado una cultura de empresa que es el resultado de lo que hacemos y lo que somos, y no de lo que decimos que deberíamos ser.\nNuestro equipo ha propuesto de forma espontánea posibles temas de formación, así como la posibilidad de invitar a Científicos de Datos de Alto Nivel a participar en charlas relacionadas con técnicas de vanguardia y cómo aplicarlas a nuestros problemas en sesiones de trabajo con nosotros. Algunos de estos científicos de alto nivel son: DJ Patil (Anteriormente Científico Jefe de Datos de la Oficina de Políticas de Ciencia y Tecnología de la Casa Blanca), Usama Fayyad (Director General de Datos y Director General del Grupo en Barclays Bank), Le Song (Profesor Asistente en el Instituto de Tecnología de Georgia), Kalyan Veeramachaneni (Investigador Científico del MIT), Roberto Paredes (Profesor Asociado de la Universidad Politécnica de Valencia), Jordi Vitrià (Profesor en la Universidad de Barcelona e Investigador Senior en Centro de Visión por Computador), Carmen Cadarso Suárez (Profesor en Biostatech, Universidad Santiago de Compostela), Fernando Pérez-Cruz (Profesor Asociado en la Universidad Carlos III de Madrid), Miguel Romance (Profesor Asociado en la Universidad Rey Juan Carlos de Madrid), Regino Criado (Profesor Asociado en la Universidad Rey Juan Carlos de Madrid), Lisa Gansky (Directora Ejecutiva de 11FS), Barbara Wixom (Principal Investigadora Científica del MIT CISR), Alex Arenas (Profesor en la Universidad Rovira i Virgili,), JJ Ramasco (Investigador en IFISC), Yuri Engelhardt (Diseñador de Información), Eneko Agirre (Investigador en el área de Procesamiento del Lenguaje Natural de la Universidad del País Vasco), y recientemente, José Vicente Rodríguez Mora (Investigador para The Alan Turing Institute), por nombrar algunos.\n\n"
    },
    {
      "title": "Ningún Problema Demasiado Grande; Ninguna Solución Demasiado (Computacionalmente) Pequeña",
      "url": "https://www.bbvadata.com/es/no-problem-too-big-no-solution-too-computationally-smal/",
      "date": "26 abril, 2018",
      "content": "\nEn BBVA Data & Analytics abordamos constantemente los problemas de negocio con matemáticas aplicadas, estadística o econometría. No hay problema demasiado grande; pero resulta que la solución a veces puede ser demasiado grande. Esta premisa llevó a dos de nuestros científicos de datos, Jordi Nin y Jordi Aranda, a explorar una forma de mejorar la calidad de los conocimientos que ofrece Commerce 360, una aplicación que ofrece a las pymes analizar su ecosistema de negocio y sus patrones de compra.\nLo que Nin y Aranda estaban tratando de elaborar era un sistema en el que se pudiera proporcionar la información más desagregada posible sobre negocios similares y circundantes sin violar las reglas de privacidad de ninguno de los elementos del conjunto y subconjunto. Esto habría permitido a las empresas compararse con la competencia sin violar la privacidad que todo cliente de BBVA debe haber protegido.\nNin y Aranda, han colaborado con el matemático Javier Herranz de la Universitat Politècnica de Catalunya en la búsqueda de una solución algorítmica a este problema para dotar de mayor flexibilidad al uso de los datos por parte de las pequeñas empresas, haciéndolo así mucho más significativo. “Hemos creado una hermosa y detallada formulación matemática para este problema”, explica Nin desde las soleadas oficinas de BBVA Data & Analytics en el centro de Barcelona. El enfoque consistió en dos fases: una que crea todos los subconjuntos prohibidos de negocios, porque no satisfacen, individualmente, una condición de privacidad específica. Más adelante, una fase en línea, que se ejecuta para cada nueva consulta, y valida si la información agregada solicitada puede ser liberada de forma segura, teniendo en cuenta todas las consultas anteriores aceptadas.\nEl problema es que con un universo tan vasto de datos de negocio y una formulación con tiempo de ejecución exponencial, comprobar individualmente las condiciones de privacidad de cada elemento de un conjunto para crear un subconjunto de consultas válidas tiene un tiempo de ejecución de O (2n). Así que el tamaño del subconjunto que define las condiciones de privacidad hará que los tiempos de procesamiento sean casi imposibles de manejar a menos que la computación cuántica se convierta en una realidad.\nEl tamaño del conjunto de subconjuntos prohibidos por el sector privado, y por lo tanto el tiempo de procesamiento, aumenta exponencialmente más allá de N=40\nLo que el científico encontró es que para prevenir la información no agregada de las compañías como lo requieren las leyes de privacidad y al mismo tiempo permitir una búsqueda fina sobre la competencia es casi un problema de NP para el universo de datos que estamos usando, por lo tanto necesitando un tiempo de procesamiento computacional tan enorme que no es factible hoy en día.\n“Compartir información agregada de manera privada es un problema importante y desafiante en productos y aplicaciones basados en datos. Aunque es posible aumentar el rendimiento de los algoritmos propuestos, en un futuro próximo nos gustaría estudiar formas alternativas de reducir el coste de cálculo de la lista de subconjuntos prohibidos”, dice Nin.\nLa conclusión de este trabajo experimental sobre cómo abordar problemas computacionales extremadamente desafiantes está siendo publicada después del proceso de revisión por pares en Proceedings of COMPSAC 2018 y será presentada este mes de julio en Tokio (Japón) durante la 42ª conferencia del IEEE sobre Informática, Software y Aplicaciones.\n\n"
    },
    {
      "title": "Añadiendo Equidad a Precios Dinámicos con Reinforcement Learning",
      "url": "https://www.bbvadata.com/es/adding-fairness-to-dynamic-pricing-with-reinforcement-learning/",
      "date": "17 abril, 2018",
      "content": "\nMaximizar las ganancias es una premisa que siempre ha estado presente en la mente de cada comerciante y vendedor desde los primeros días del comercio; o eso pensaban. En realidad, cada actor en busca de ganancias durante un intercambio de bienes o servicios tiende inconscientemente a agregar alguna medida de justicia al trato para cultivar la confianza de un cliente; el problema era que esta confianza no estaba siendo medida, pero ahora el Reinforcement Learning (o Aprendizaje por Refuerzo) puede ayudarnos a medirla.\nUn balance entre ingresos y equidad\nUn grupo de científicos de BBVA Data & Analytics acaba de publicar un artículo científico en el que explican cómo, mediante el uso de Reinforcement Learning, como añadir a las políticas de precios una medida específica de equidad, aumentando así la confianza en la forma en que el banco se relaciona con sus clientes. El algoritmo, que se ajusta en tiempo real a las condiciones de los clientes, no sólo considera la maximización del beneficio en sus cálculos, sino también una medida de equidad basada en el índice de Jain, en el que la política de fijación de precios se decide homogéneamente para un número heterogéneo de grupos dados.\nÍndice de Jain para dos grupos de clientes. Por favor, tenga en cuenta que el índice es igual al porcentaje de personas para las que una política de precios es justa.\nEl algoritmo de RL considera dos expresiones, una que trata de maximizar los retornos en cada grupo de clientes y otra que trata de mantener la equidad al más alto nivel posible entre los grupos de compradores.\n“Demostramos que RL proporciona dos características principales para conseguir la imparcialidad en la fijación dinámica de precios: por un lado, RL es capaz de aprender de la experiencia reciente, adaptando la política de precios a entornos de mercado complejos; por otro lado, integra la imparcialidad como parte fundamental del modelo”, dice uno de los principales científicos de datos detrás de esta investigación, Roberto Maestre.\nPrincipios de Diseño de Equidad\nLas políticas de precios injustas han demostrado ser una de las percepciones más negativas que los clientes pueden tener con respecto a los precios, y pueden resultar en pérdidas a largo plazo para una empresa. Esto es aún más relevante en un mundo en el que la información puede ser compartida rápidamente y la evidencia de diferencias en la fijación de precios dinámicos es evidente para los consumidores, pero -hasta ahora- el nivel de equidad en la política no se media.\nA lo que RL nos ayuda, es a equilibrar en tiempo real la equidad y el beneficio en un entorno complejo y difuso de fluctuaciones de precios. RL es ideal para este tipo de problemas porque aprende por ensayo y error interactuando con el entorno, a diferencia de aprender con datos etiquetados, es decir, no es necesario un conocimiento previo sobre cómo funciona el medio ambiente.\n“Con este nuevo algoritmo podemos tener transparencia en la fijación de precios y transmitir esta medida de imparcialidad al cliente, mejorando así la confianza en nuestros servicios”, afirma Maestre.\nLos científicos de datos de BBVA Data & Analytics realizan un experimento en el que consideran diferentes comportamientos en función de una política de precios, algunos de los cuales son muy sensibles al cambio de precios -con una probabilidad muy baja de aceptación de ofertas a partir de un determinado umbral-, mientras que otros no muestran una respuesta significativa a los cambios de precios y otro en realidad aumenta su probabilidad de aceptación cuando los precios suben (al igual que el comportamiento de los compradores de artículos de lujo). Para lograr el máximo de ingresos y equidad, el modelo de RL tiene que recurrir a la maximización de los beneficios, manteniendo al mismo tiempo la diferencia de precios entre los grupos al mínimo. Este enfoque es además capaz de eliminar los sesgos en las políticas de precios, ya que consigue equidad a todos los grupos de clientes considerados por igual (por ejemplo: por género, raza, nivel de ingresos, etc.).\nTransparencia y responsabilidad\nA pesar de la simplicidad del entorno propuesto, el documento en sí mismo permite efectivamente abrir una rica discusión relacionada con lo que es justo en materia de precios, comenzando con una forma específica de medirlo. Además, este estudio nos permite comparar las políticas de precios conseguidas, en un entorno abierto como The Turing Box. Dado que estas plataformas permiten a cualquier profesional, no sólo a científicos de computación o investigadores en ML, estudiar la IA; nos beneficiamos de una valiosa retroalimentación.\n\nRoberto Maestre presentará las conclusiones de su artículo científico durante la Conferencia sobre Sistemas de Inteligencia que tendrá lugar en Londres el próximo mes de septiembre.\n\n"
    },
    {
      "title": "Una Introducción “peculiar” al Deep Learning",
      "url": "https://www.bbvadata.com/es/a-weird-introduction-to-deep-learning/",
      "date": "12 abril, 2018",
      "content": "\nHay increíbles introducciones, cursos y entradas en blogs sobre Deep Learning. Voy a nombrar algunas de ellas en las secciones de recursos, pero esta es una clase diferente de introducción: una introducción peculiar. ¿Pero por qué peculiar? Tal vez porque no sigue la estructura “normal” de un post de Deep Learning, donde se empieza con las matemáticas, luego se pasa a los documentos científicos, la implementación y luego a las aplicaciones. Creo que contar una historia puede ser mucho más útil que lanzar información y fórmulas por todas partes. Así que comencemos.\nCronología del Deep Learning\nEl Deep Learning es un campo muy importante para la Ciencia de Datos, la Inteligencia Artificial, la Tecnología y nuestras vidas en este momento, y merece toda la atención que está recibiendo. Por favor, no digas que el aprendizaje profundo es sólo añadir una capa a una red neuronal, y eso es todo; ¡mágico! Espero que después de leer esto tengas una perspectiva diferente de lo que es Deep Learning.\n\nCreé esta línea de tiempo basada en varios artículos y otros recursos con el propósito de mostrar que el Deep Learning es mucho más que sólo Redes Neuronales. Ha habido avances realmente teóricos, mejoras de software y hardware que eran necesarias para que llegáramos a este día.\nAvances en el Deep Learning y el aprendizaje de la representación\nComencemos por definir la palabra aprendizaje. En el contexto del Aprendizaje Automático, la palabra “aprendizaje” describe un proceso de búsqueda automática para una mejor representación de los datos que está analizando y estudiando (por favor, tenga esto en cuenta, no es hacer que una computadora aprenda). ¿Qué es una representación? Es una forma de ver los datos.\nEsto es algo muy importante a tener en cuenta, el Deep Learning es el aprendizaje de la representación utilizando diferentes tipos de redes neuronales y optimizando los hiper-parámetros de la red para obtener (aprender) la mejor representación para nuestros datos. Esto no sería posible sin los asombrosos avances que nos llevaron al estado actual del Deep Learning. He aquí algunos de ellos:\nConcepto # 1: Propagación inversa.\n\nConcepto # 2: Mejor inicialización de los parámetros de las redes. Algo a tener en cuenta: La estrategia de inicialización debe seleccionarse según la función de activación utilizada (véase el siguiente concepto).\n\nConcepto # 3: Mejores funciones de activación. Esto significa, mejores maneras de aproximar las funciones más rápidamente, lo que conduce a un proceso de entrenamiento más rápido.\n\nConcepto # 4: Dropout. Mejores formas de prevenir el sobreajuste y más.\n\nConcepto # 5: Redes Neuronales Convolucionales (CNNs).\n\nConcept # 6: Redes residuales (ResNets).\n\nConcepto # 7: CNNs Regionales. Utilizado para la detección de objetos y más.\n\nConcepto # 8: Redes Neuronales Recurrentes (RNNs) y LSTMs.\n\nPor cierto: Liao y Poggio (2016) mostraron que ResNets == RNNs, arXiv:1604.03640v1.\nConcepto # 9: Redes Generativas Adversariales (GANs).\n\nConcepto # 10: Redes de Cápsulas.\n\nHaciendo cosas con Deep Learning\nUno de los momentos más importantes para este campo fue la creación y apertura de TensorFlow.\nTensorFlow es una librería de software de código abierto para computación numérica usando gráficas de flujo de datos. Los nodos del gráfico representan operaciones matemáticas, mientras que los bordes del gráfico representan los conjuntos de datos multidimensionales (tensores) comunicados entre sí.\nLos tensores, definidos matemáticamente, son simplemente matrices de números, o funciones, que se transforman según ciertas reglas bajo un cambio de coordenadas.\nPero en el ámbito del Machine Learning y del Deep learning un tensor es una generalización de vectores y matrices a dimensiones potencialmente superiores. Internamente, TensorFlow representa a los tensores como matrices n-dimensionales de tipos de datos base.\nPensando en el futuro de Deep Learning (para programar o construir aplicaciones), creo que las interfaces gráficas de usuario (GUI) y AutoML son el futuro cercano para hacer las cosas con Deep Learning. No me malinterpretes, me encanta codificar, pero creo que la cantidad de código que escribiremos los próximos años se descompondrá.\nPasamos muchas horas programando las mismas cosas en todo el mundo una y otra vez, así que creo que estas dos características GUIs y AutoML ayudarán a los Científicos de Datos a ser más productivos y a resolver más problemas.\nGracias por leer esta extraña introducción a Deep Learning. Espero que te haya ayudado a empezar en esta increíble área, o tal vez simplemente descubrir algo nuevo.\n\n"
    },
    {
      "title": "La Inteligencia Artificial será Omnipresente en el Futuro",
      "url": "https://www.bbvadata.com/es/artificial-intelligence-will-be-omnipresent-in-the-future/",
      "date": "10 abril, 2018",
      "content": "\nUna combinación de factores, como por ejemplo los avances científicos disruptivos en los últimos cinco años en recopilación y procesamiento de datos, ha contribuido para aumentar la propia percepción humana con el uso de la Inteligencia Artificial. Ya existen campos avanzados como la clasificación de imágenes o reconocimiento de objetos en ellas, o numerosos progresos en la traducción automática, entre otros muchos; así como avances de los sistemas autónomos, que han introducido dicho término en nuestras conversaciones diarias, convirtiéndolo más en presente, que en futuro. \nDel mismo modo, el auge del “Big Data” y la Ciencia de Datos, así como la democratización de las herramientas de software especializadas, ha provocado que cada vez más empresas traten de hacer uso de los datos para generar productos más innovadores o mejorar servicios. El uso de los datos será esencial en el futuro de la Inteligencia Artificial. \nRecientemente, José Antonio Rodríguez-Serrano, uno de nuestros Lead Data Scientist y responsable del equipo innovación, participó en un podcast para BBVA, donde trató de arrojar algo de luz sobre qué es y para qué sirve la Inteligencia Artificial. José Antonio nos ofreció su visión acerca de los servicios o novedades de los que podremos disfrutar los usuarios en un futuro no muy lejano. \n \n \n\n \n \n\n"
    },
    {
      "title": "Urban Discovery, una Visión para Repensar las Ciudades",
      "url": "https://www.bbvadata.com/es/urban-discovery-a-vision-to-rethink-cities/",
      "date": "4 abril, 2018",
      "content": "\nA finales del año pasado, Urban Discovery mostró una nueva forma de representar a las ciudades basada en el comportamiento de compra. Este enfoque permitió a BBVA Data & Analytics examinar Madrid, Barcelona y Ciudad de México bajo una nueva luz. Los conocimientos extraídos mediante el análisis de patrones de comportamiento económico nos ayudaron a rediseñar las ciudades más allá de los límites administrativos tradicionales.\nLa visualización creada por BBVA Data & Analytics y CARTO, por ejemplo, nos muestra que en un distrito puede haber zonas de diferente importancia económica. En el caso de Barcelona y Madrid, analizamos más de 130 millones de transacciones y descubrimos la movilidad económica de una ciudad, los vínculos entre zonas de la ciudad y cómo el comportamiento económico crea lo que llamamos macrocomunidades.\nJuan de Dios Romero, uno de los principales científicos de datos de este proyecto, ha explicado el potencial de esta historia de datos, que utiliza datos parcialmente disponibles a través de la API de Paystats de BBVA, en diferentes foros. El mes pasado, invitado por el  programa de radio PiperLab de Capital Radio, analizó el potencial de conocer mejor comportamiento económico de una ciudad para los gobiernos y los planificadores urbanos. Introdujo el concepto de inteligencia urbana, un conjunto de herramientas, prácticas y metodologías que podrían ayudar a los planificadores urbanos y las pequeñas empresas a entender las preferencias de consumo por área, las relaciones con otras zonas y la movilidad urbana.\nRomero invitó a las personas interesadas en Urban Discovery a descargar el conjunto de datos anónimos utilizados para Urban Discovery, abierto a nuevas exploraciones y nuevos descubrimientos.\nA continuación puedes escuchar el audio completo de su intervención:\n\n\n"
    },
    {
      "title": "Designing Visions for Busy Organizations. Part II",
      "url": "https://www.bbvadata.com/es/designing-visions-busy-organizations-part-ii/",
      "date": "28 marzo, 2018",
      "content": "\nPutting Design Fiction into practice\n \nThe banking industry is facing major changes with the digitalization of its core business that impacts the management of risk, trust, identity and decision-making, among other things. As you have seen in Part I of this series of articles. At BBVA Data & Analytics, we spearhead BBVA’s transformation into a data-driven financial group. We operate like an ambidextrous organization with two lines of inquiry: the Inside, which is about delivering data engines to optimise and sustain the established banking industry, and the Edge, which is about exploring new growth engines (e.g. cost reduction, revenue generating, new experiences) through algorithmic research and experiments linked to business models.\n\nAt BBVA Data & Analytics, we believe the knowledge derived from financial data can transform the banking industry, its relation with customers and its role in the world. We design fictions to question our analytical capabilities through ‘What if’ scenarios.\nCreating momentum with the team\nOur data scientists are embedded within multidisciplinary teams from other divisions and business units. They are proactive partners who think ahead of the curve beyond incremental changes and consider the implications of their work. They participate in the framing of the problems and in the conception of ideas. Together, we examine information with an objective of regularly updating an actionable vision for our Edge agenda. They are tasked with delivering compelling insights, and experimenting with new technologies—without constraints—to contemplate these impacts and how they might best be progressed or mitigated. From this line of applied algorithmic research, we build “What if” scenarios.\n\nBy using Design Fiction, we can build speculative evidences and focus on tangible things for an organization to consider, in addition to static slides, demos and presentation documents. \n\nWhile we like to “think outside the box” relative to a large financial group, we also strive to work within the vision of the organization and create understanding about it. By using Design Fiction, we can build speculative evidences and focus on tangible things for an organization to consider, in addition to static slides, demos and presentation documents. These materials directly feed the visions as part of BBVA transforming into a digital and data-driven organization.\nTypically, we start mapping the ongoing lines of investigation as defined by our vision. Next, we project their evolution into two or three iterations and create stories about what could be: Potentially, what could the resulting technology feel like? Where could it be used? Who would use it, and for what type of experience? Then, we can group them into some future scenarios to explore in more depth.\n\nVisions are dynamic and re-calibrated, from mapping ongoing lines of investigation to seeking feedback from their dissemination in public.\nAs a provocation to tell stories about these potential technologies, rather than jumping into the development of an actual product or service, we might create a fictional advertisement, videos that share a concept before we experiment it, release notes, or other “props” in order to gain some practical answers to questions which might arise. For instance: Is this now, or will it ever be legal? What kind of technology do we need to master to make it happen? Does this raise new social responsibilities for us as a business? Do we really want to create this as a team? Who within the organization would want this future and who would not?\nEngaging the stakeholders\nAn organization is only as strong as its teams’ ability to collaborate with each other. We like to think that Design Fictions act as a totem or boundary objects for shared understanding, discussion and evaluation of changes that could bend visions and trajectories.\nAs an example, let’s say that a team has developed an advanced recommender engine to create personalized and targeted promotion campaigns for small shop owners. We can imagine the business model which might be helpful for this type of product, but we need to understand what it will feel like. It helps the different stakeholders of a project to engage with essential questions, to understand what the desired experience might be, and why the team should design a recommender system one way and not another. \n\nThe mission of our Payments team is to create a world where services around money transactions become even more personalized and fair. A tangible vision stretching the imagination in context can open the door to new idioms and vocabulary.\nWhat are the potential unintended consequences for a small shop owner to run its own promotion campaigns? What can she expect from the results? What will she be allowed to do? What won’t she do anymore? How does she interact with that technology the first time, and then routinely after a month, one year, or more? How does that service become a habit? Inventive and appreciable observations from these questions can expose fruitful insights before a project even starts production, simply by creating fictional customer reviews, a user manual, a press release, or ads in a language familiar to the organization.\nIn essence, our data scientists construct visions, with a vested interest in conveying to the organization that their experimental technologies can exist in the real world, in the near future. We are asking others to participate by sharing their thoughts on the main features, attributes, and characteristics of the experience from our point of view, (the possible, the ethical), the user’s point of view, (the desirable, the beneficial) and the business’ point of view (the profitable, the legal). This approach not only makes the vision more meaningful and resilient, it makes the organization behave as a learning organization, and that may just be part of its vision. \nConvincing the decision makers\nDesign Fiction helps convince others of the benefits of a potential technological capacity. Feedback from people with different perspectives provides space for anticipating new opportunities and challenges. In effect, narrow frames of inquiry constrict vision, which can prevent people from exploring all aspects of an idea, thus limiting results, and often missing out on a great opportunity in favor of a workable one.\nDecision makers, like most humans, crave certainty. They need to be told a story about what will happen next. Design Fictions provide a way to do that by asking strategic questions. They can push an organization to step into an uncomfortable zone which prompts them to explore the possibilities of world changing around value, risk, reputation, competition, and customers. \nThe visions that give an understanding of what an organization could aspire towards can also surface reactions as evidence of a barrier to change. Indeed, this type of reaction often validates the vision—as true innovation is rarely comfortable. Decision makers are often stuck in the modes of thinking and working which brought success in the past. Basically, Design Fiction can help get a feel of “what’s next” to mitigate feelings of uncertainty and anxiety. The goal is to leave people feeling inspired, not agitated.\nFor instance, a couple of years ago, our teams of data scientists became really good at detecting abnormal financial situations based on customer behaviors. We extrapolated that capacity into a vision around the possible advantages for customers with a bank account that can “drive itself” and automatically correct anomalies. We called this idea ‘Peace of Mind’, and used Design Fiction internally to express this potential by embedding the concept into a short story which explained how, and when, it would work. That exploration became one vision for 2017 at BBVA as CEO Carlos Torres explains at a Money2020 event:\n\nCarlos Torres, CEO of BBVA, articulating his vision of “Peace of Mind”\nTransforming the vision into a mandate\nIf response to a new concept is “What would we do next?”, we take it as a sign we’re on the right track. The near future will soon be the present, so there is a “need to constantly adapt and ask: what are we doing?, where should we go? and, what are others doing? Additional investigation through complementary tools such as the Business Model Canvas, Systems Thinking, Three Horizons, or Agile Vision, can also provide enriching answers to these questions and move from fiction to facts and reality.\nWhat if the design of automation together with Machine Learning techniques can lead to a dynamic and fair definition of prices? We want to make sure our technological ambition and its consequences are fully articulated within potential value propositions.\nTypical actions associated with Design Fictions involve building credibility, enlisting support, countering skepticism, creating momentum and sharing a common vision. A desired outcome of a Design Fiction is a team mandate to make the vision become reality.\n\nWe are living in an era when social rules and business models are between the “no more” and the “not yet”.\n\nShaping and sharing tangible visions\nDesign Fiction offers a set of values which advocates learning through curiosity, interrogation, and provocation using broadly connected themes. Like Science Fiction or an episode of Charlie Brooker’s Series Black Mirror, it can be a concrete demonstration of what emerging technologies can lead to, as well as unanticipated consequences. Design Fiction creates artifacts that deliver strategic insights to important questions. It can tell us more about the types of societies, and lives, an organization might help shape. It can tell us about the evolution of technologies, the world they might live in, the language used to describe them, people’s appropriations, the rituals, and the frustrations. It tells us about the ethics of the professionals who create these technologies, and the larger vision behind discovery. It tells us those who benefits from techno-science advances, and those who might be left behind.\nWe are living in an era when social rules and business models are between the “no more” and the “not yet”. Now is the time to make an effort to set up the right expectations and convince others to expand their capacity for better understanding of technological opportunities and their implications. \nDesign Fiction is an easy way to enhance the imagination and vision of overburdened organizations, teams and executives. This, in turn, creates a capacity to execute and take mandates to the next level.\n\n"
    },
    {
      "title": "Designing Visions for Busy Organizations. Part I",
      "url": "https://www.bbvadata.com/es/designing-visions-busy-organizations-part/",
      "date": "27 marzo, 2018",
      "content": "\nClarify the present with Design Fiction: Shaping and sharing tangible visions for the future\n \nFor the last few years at Near Future Laboratory and BBVA Data & Analytics, my teams and I have been focused on clarifying the present and anticipating desirable, feasible, and profitable futures for organizations. Establishing a vision, then investigating that vision through Design Fiction, we proactively explore what we might do next, discuss implications of potential opportunities and seek to understand what we need to achieve today to create the preferable future we envision.\nThe future is a moving target\nThe time horizons of innovation have shortened. New technologies and business models are emerging and expiring faster than ever before. The pace by which ideas circulate through networks, and the availability of exceptional scales of funding and resources for even the craziest of ideas magnify the importance of framing problems in time scales which are much shorter than the old standard of ten years. Shaping a clear vision is pivotal for organizations, because there is less clarity for their members about where things are heading, and why. Leaders are requested to have clear and compelling visions, and be better at communicating these visions in an effective way.\n\nOrganizations need new approaches to explore futures based on the pace of modern human creativity—to find possibilities in unexpected places that can’t be known in advance.\n\nLeaders of today’s digital giants, such as Amazon, Tesla, and Google, clearly articulate visions, even in the face of huge short-term uncertainties. However, many other large organizations and governments struggle with newer methods for anticipating and adapting to change, as they don’t work well with old-fashioned methodologies, where visions of the future are often projected through powerpoints, written reports, white papers, or, at best, a promotional video visualizing a seamless future. Organizations need new approaches to explore futures based on the pace of modern human creativity—to find possibilities in unexpected places that can’t be known in advance.\nDesign Fiction is just such a tool. Organizations can use Design Fiction to:\n\nEnhance an imagination jammed in a world of sameness.\nSpark conversations and anticipate unevenly distributed futures.\nCheck the sanity of a vision for an anticipated future.\nSet priorities for today which can make a preferred future attainable.\n\nThe future is often characterized as the state of not knowing what will happen next. For an organization, it is a moving target with multiple potential paths and unfolding spaces. The creation and application of new technologies bring with them new opportunities to imagine “What if…” scenarios to better explore complex implications, and to mollify anti-change agents (e.g. those left behind by change). Extrapolating from past trends is useful, but often limiting. The future is mostly not predictable—it can’t be nailed down, so prioritizing investments which help frame future-facing insights allows an organization to understand what it will need to move towards its vision. To prepare for “not knowing,” the best an organization can do is aim to experiment and learn.\nTools and building blocks\nOur work has developed across multiple methodologies, some of which are in quite common use, others less so. For clarity, here’s a short explainer for some of the terms and ideas used in this article:\n\nVision describes the state of an organization across its functions, but does not stand in as a summary of the organization as a whole. Visions should provide a sense of aspiration to stretch the imagination and provoke exploration of new possibilities. A vision should be accessible, inclusive, and cooperative. Vision drives discussion, learning, and strategy, and allows space for leaders to consider necessary organizational transformations. \nScenarios are the frameworks or stories within which an organization considers why and how a future technology might make sense, or be useful. \nDiegetic prototypes or “props” exist as a functional piece of technology within a fictional world. They are tangible concept depictions which act as a stand-in for a “thing” that an organization would like to showcase, question, consider, and learn from. They are often representative of uses for new technologies, and provide an accessible way for others to interrogate practicality, propriety or value. \nDesign fiction is the use of “props” to help scaffold narratives about a slightly changed world. \n\n\nThe future is something an organization must constantly shape and adapt to.\n\nExploring futures is a learning process, not an output\nExecutives—most of them with deep knowledge of their domain, their markets, their customers, and even their organizations— sometimes lack the means to experiment with and shape their vision beyond the next quarterly plan. Frenetic and misaligned incentives within organizations too often dictate they must manage the existing business in a reactive mode, and consequently, they are routinely following a blinkered path, rather than anticipating a need for change. As managing the present takes oxygen from experimenting with futures, their time to enhance corporate imagination and strategy shrinks.\nThe future is something an organization must constantly shape and adapt to. A compelling vision should have the facility to formulate mandates that a team may use to gain a clearer view of what really needs to be done and—equally important—what may prevent them from doing it. \nThe bottom line then is to recognize the complexities of the business, society and technologies to create visions for areas that are meaningful to internal and external stakeholders, and to make sure these visions bend or adapt the overall mission, purpose, objectives and values of an organization.\n\nThe video of the Amazon Go shows what shopping looks like when a supermarket has no checkout. Extrapolating the capacities of technologies and anticipating behaviors, Amazon shared this fiction before the concept went into a trial period. \nExploring futures to set today’s priorities\nFurthering the vision through a Design Fiction approach helps executives and teams increase knowledge on a new technology, display an ambition on its applications, build credibility, enlist support for its execution, counter skepticism, create momentum and incorporate common ideals. Feedback from people with different perspectives anticipates expanded junctures for opportunities and challenges. \nWith Design Fiction, the audience is asked to hold a certain suspension of disbelief as to what is possible, and focus on the implications for the organization, the competition, a product, its customers, markets, and society in general, because the related promise made possible by new technology does far more than replacing old technology and behaviors. \nToday, large organizations use Design Fiction to learn and iterate even before prototyping, wireframing or writing a line of code. Insights from these explorations can become mandates for research, a user study, a usable prototype, a strategy, or some combination of these. Amazon often applies the approach of working backwards. For instance, videos of Amazon Go or Amazon Prime Air were used for anticipating and demonstrating the near-future capabilities of the company. They tell provocative stories of a potential tomorrow offering the company a feedback mechanism for valuable insights into the public’s consent or desire for certain technological applications. Design Fiction without some frictions or closed to critiques can easily fall into corporate publicity showcasing sterile ‘flat-pack futures’.\nGoogle’s Project Soli video was developed to conceptualize future applications of applied research capabilities regarding the development of a new interaction sensor using radar technology. Explaining the scientific “magic” behind these tiny gesture sensors helps researchers clearly articulate their ambitions and make the world aware of Google’s belief of the potential around this technology. The video provided a way to perform “micro” future studies that pay particular attention to our everyday life rituals, behaviors, and frictions, as short-term evolutions for exploring the standard objects or services that might fill these possible futures.\nDesign Fiction is not just a new way of sharing research outcomes, but also a new forum for communication and collaboration with a variety of partners. The Ikea Catalog from the near future was produced in a group effort bringing together collaborators from public sector bodies, academic institutes, and major organisations – including IKEA and Ericsson. It’s not the technology featured in the catalog, but the suggested implications of new benefits and rituals in our future lives which elicit an emotional response. This type of prompt or provocation tackles future-oriented problems or opportunities with an eye towards addressing concerns, not just about a concept’s viability, but the wider consequences of that viability. \nIt is not a specification, nor is it a prediction. The micro-fictions embedded in the Ikea Catalog from the near future are where Design Fiction makes subtle suggestions about how the near future may be a bit different from today. They are meant to provoke the imagination and spark a conversation regarding the futures of connected things.\nEven governments have used Design Fictions to incentivise future-forward policy by engaging citizens with positive perceptions of innovation, acceleration, and change. Dubai, in the UAE, has established the Dubai Future Foundation which, among other initiatives, curates an annual Museum of the Future to facilitate community investment in the future, through immersive and participatory experiences.\nFor us, a successful Design Fiction means that a large audience within an organization can feel, touch, understand, and discuss near future opportunities. It provides convincing material for exploring unknowns, working through turbulent alternatives, contesting the status quo, or setting priorities for strategic alternatives. It creates a believable bridge from the current universe of an organization to its potential futures. \nIn Part II of this article, we describe how we put Design Fiction into practice at BBVA Data & Analytics.\n\n"
    },
    {
      "title": "RecSys, un Debate sobre Cómo Ampliar Nuestro Mundo y Explorar otros Nuevos",
      "url": "https://www.bbvadata.com/es/recsys-debate-widen-world-find-new-ones/",
      "date": "22 marzo, 2018",
      "content": "\nEn BBVA Data & Analytics creemos que el futuro de la tecnología debe ser continuamente cuestionado, debatido y considerado desde diferentes puntos de vista. Con esa filosofía, organizamos esta semana un encuentro poco convencional de artistas, juristas, diseñadores, creadores y científicos para explorar las implicaciones de los Sistemas de Recomendación (RecSys).\nEstos algoritmos de Machine Learning definen cómo se forma nuestro mundo, cómo se acortan las distancias y cómo se crean nuevas conexiones. Apoyado por el trabajo experimental de Iskra Velichkova RecSys: ¿Qué es una Recomendación en la Era del Machine learning? tuvimos una conversación en un encantador y antiguo piso en Madrid. Lo que sigue es una imagen de las conexiones y las ideas que se encontraron allí.\n¿Qué es realmente una recomendación?\nLos Sistemas de Recomendación nos ayudan a conectar con productos, servicios o lugares, pero además descomponen un universo de emociones y conexiones personales. “Los algoritmos de Machine Learning nos permiten acceder al conocimiento de toda una ciudad y viajar en un universo multidimensional donde podemos encontrar algo que despierta una emoción”, explica Velichkova, experta de BBVA Data & Analytics en Visual Thinking.\nHace un siglo, el autor francés y burgués atormentado Marcel Proust explicaba en sus novela “En busca del tiempo perdido” con gran clarividencia el proceso que conecta experiencias y productos con nuestras más íntimas emociones. Velichkova usa ese ejemplo de Proust y su famosa magdalena a la hora de imaginar cómo las máquinas acabarán visualizando y entendiendo nuestro universo de experiencias pasadas y posibles conexiones en el futuro.\nLa fábula creada por BBVA Data & Analytics muestras las preguntas y deja a la audiencia buscar sus respuesta. En nuestra presentación invitamos a Francisco Vieira, presidente del Tribunal Superior de Justicia de Madrid; la responsable de innovación de restaurante Celler Can Roca, Heloïse Vilaseca; al director de arte y ganador de un Goya por Blancanieves, Alain Bainée, y al diseñador de interacción Javier Cañada.\nVieira reflexionó sobre cómo la ubicuidad de la tecnología que percibe (es miembro del consejo que regula el respeto a la privacidad de las cámaras de seguridad en Madrid) pueden “interferir en nuestras relaciones sociales; en nuestra espontaneidad”.\nBaineé y Vilaseca explicaron cómo intentan conjurar en su obra las mismas emociones que la magdalena de Proust le provocó hace cien años. Uno de los goles de Messi -en el caso de Vilaseca- o un oscuro recuerdo de la infancia -en el caso de la versión de Blancanieves de Baineé- puede conjurarse con recreaciones materiales y sutiles, de la misma manera que los sistemas de recomendación podrían algún día darte lo que buscas incluso antes de que lo pidas.\n \n\nCañada, un diseñador de interacción, ofreció una visión más sobria de cómo conocer las preferencias de los consumidores y su configuración ayudó a las empresas y diseñadores a ofrecer algo útil y conveniente. La cantidad de datos generados hoy en día pueden“medir” a todo el mundo de una manera nunca antes alcanzada. Por ejemplo, conocer el “contexto emocional” de una persona es esencial para comprender la interacción de esa persona con el producto. A medida que el debate avanzaba, Cañada planteaba una nueva pregunta:“pero, ¿qué pasa si poder entregar el producto más conveniente que se ajuste perfectamente a las necesidades de uno nos convierte en seres idiotas?”\n\nEntonces, ¿contribuyen los Sistemas de Recomendación a conectarnos a todos de una manera más eficiente y significativa?\nEjemplos como el uso de Facebook como cámara de repercusión para reforzar los sesgos políticos y la polarización durante las elecciones presidenciales de 2016 en EE.UU. hicieron dudar a algunos participantes de que los sistemas de recomendación contribuyeran realmente a enriquecer nuestra visión del mundo.\nBorja Adsuara, abogado especializado en tecnología y privacidad y consultor, cree que la forma en que se diseñan los sistemas de recomendación en la actualidad “la afinidad es lo que realmente se promueve”, no el descubrimiento. Algunos científicos de datos señalaron rápidamente que uno de los retos más importantes de su trabajo con RecSys es “añadir ruido” para permitir a la gente encontrar cosas fuera de su zona de confort, evitando así el “efecto burbuja de las recomendaciones” y, en última instancia, ampliando nuestro mundo.\nSegún Adsuara, las recomendaciones se transmiten desde una “posición de autoridad”, y sobre todo con fines de mera comercialización; para “venderle algo”. Juan Murillo, de BBVA Data & Analytics, discrepó: “al menos para nosotros, los vemos como herramientas para proporcionar al cliente lo que necesita cuando más lo necesita. Queremos ayudar a su tranquilidad. Es una situación en la que todos salen ganando”.\nTodos coincidieron en que las industrias que aplican RecSys tienen que evitar utilizar estas herramientas como “manipuladores de la atención” y, por lo tanto, la transparencia y la confianza son los fundamentos básicos de un RecSys “honesto”.\nEl aspecto más productivo del debate creado por RecSys: ¿Qué es una recomendación en la era del Machine Learning? fue la capacidad de ayudar a brotar nuevas ideas en la audiencia y añadir matices a las anteriores. Tal y como reconoció Juan Arévalo, científico de datos de BBVA Data & Analytics: “para nosotros es de suma importancia crear confianza con los datos. En este sentido, es interesante darse cuenta de que, de alguna manera, las recomendaciones podrían ser ofrecidas desde una posición de autoridad”, ya que podría provenir de una empresa que sabe más que usted, que analiza datos que el usuario no necesariamente sabe que están siendo utilizados (aunque él esté de acuerdo), y que usted confía. El debate también llevó a Vieira a presentar un nuevo concepto: “lealtad. El recomendador puede tener que mostrar lealtad al usuario”.\nA medida que la conversación avanzaba, el emprendedor Marcelo Soria encontró en su teléfono una recomendación procesada por el cerebro humano; una nueva forma de expresar cómo un RecSys puede acortar espacios (físicos o de otro tipo), devolver emociones y crear conexiones significativas en las palabras del poeta uruguayo Mario Benedetti:\nMi táctica es mirarte, \naprender como sos,\nquererte como sos.\nMi táctica es hablarte\ny escucharte, \nconstruir con palabras\nun puente indestructible.\nMi táctica es\nquedarme en tu recuerdo,\nno sé cómo ni sé\ncon qué pretexto,\npero quedarme en vos.\nMi táctica es ser franco\ny saber que sos franca\ny que no nos vendamos simulacros\npara que entre los dos\nno haya telón ni abismos.\nMi estrategia es,\nen cambio, \nmás profunda y más simple.\nMi estrategia es,\nque un día cualquiera,\nno sé cómo ni sé\ncon qué pretexto,\npor fin me necesites.\nTal vez las respuestas sobre cómo construir tecnología que nos ayuden a explorar más conexiones humanas han estado ahí todo el tiempo, y sólo tenemos que leer más poesía.\n\n"
    },
    {
      "title": "Como Tener un Entorno con Spark en Menos de 30 Segundos Gracias a Docker",
      "url": "https://www.bbvadata.com/es/environment-spark-less-30-seconds-thanks-docker/",
      "date": "22 marzo, 2018",
      "content": "\nSeguro que alguna vez habéis querido “cacharrear” con alguna tecnología, pero vuestro afán de aprender seguro que se ha visto mermado por la dificultad de instalar todo lo necesario en vuestro sistema operativo. En este artículo explicaremos cómo empezar a jugar con spark sin importar el sistema operativo que tengamos y sin necesidad de sufrir ni tener que pegarse ni desquiciarse con la instalación o ejecución.\nInicio rápido\nSi tienes curiosidad sobre Spark o necesitas utilizarlo tan solo necesitas tener instalado docker en tu ordenador, para ello puedes seguir la documentación oficial. En caso de tenerlo ya instalado solo has de seguir cuatro sencillos pasos:\n\nDescárgate de nuestro repositorio el proyecto en .zip.\nDescomprime el proyecto y sitúate con la terminal/CMD y situate en el directorio donde está nuestro docker-compose.yml.\nEjecuta el siguiente comando: $ sudo docker-compose up\nCopia la url que aparezca en la terminal y copiala en tu navegador (preferiblemente Chrome o Firefox) tal y como se indica en el log del terminal/CMD.\nYa tendrás tu entorno preparado para ejecutar con pyspark, te hemos dejado un código de ejemplo en el directorio notebooks para probar a ejecutar.\n\n \nCómo montar tu propia imagen Docker personalizada\nSi tienes curiosidad de como se ha montado o modificar la imagen docker para poder incluir tu mismo los kernels de anaconda que desees o simplemente ya tienes manejo con docker y deseas incluir alguna parte en tu imagen docker, a continuación describiremos cómo ha sido el proceso para que esto sea posible.\nImagen jupyter-debian\nEstos son fragmentos sacados del Dockerfile, donde se describe las diferentes piezas que forman el entorno. En esta imagen docker encontramos configurado a parte de Jupiter Notebook como interfaz de iPython para poder ejecutar código, Anaconda para el control de las librerías python que se van a utilizar, Spark en su versión 2.x y  todas las variables de entorno configuradas para poder empezar a utilizar pyspark sin preocuparse de su configuración.\n \n\n \nCon esto podrás tener un entorno Spark para lanzar tus pruebas sin tener que comerte la cabeza con la configuración ni tener seguir mil tutoriales para conseguir ejecutar algo con Spark, gracias a Docker podemos abstraernos de las tareas tediosas de configuración y hacer pruebas con tecnologías nuevas de forma sencilla sin tener que tener instalado todas las dependencias.\n \nSi quieres saber más sobre Docker y su utilidad puedes visitar su página web oficial.\n \n \n\n"
    },
    {
      "title": "Stephen Hawking, el Genio que Vio el Potencial de un Desarrollo Responsable de la Inteligencia Artificial",
      "url": "https://www.bbvadata.com/es/stephen-hawking-genius-saw-potential-responsible-development-artificial-intelligence/",
      "date": "14 marzo, 2018",
      "content": "\nStephen Hawking (1942-2018) dejó este mundo para dedicarse a las estrellas en Pi Day, celebrado por matemáticos y físicos de todo el mundo. Quienes leyeron “Una breve historia del tiempo” y decidieron explorar el mundo a través de números y ojos científicos están llorando la pérdida de un genio con gran sentido del humor y capacidad de analizar el mundo con la visión y creatividad que sustentaban sus teorías sobre la relatividad del tiempo y los agujeros negros.\nHawking, que compartía sus ideas con estudiantes de Física en las salas y salones comunes de Cambridge y conferencias de todo el mundo, nos advirtió en el último tramo de su vida productiva sobre el riesgo de una Inteligencia Artificial que se desarrolla sin tener en cuenta las necesidades humanas más apremiantes.\nEn una cumbre celebrada en Lisboa (Portugal) el año pasado quedó claro: “el éxito en la creación de una Inteligencia Artificial eficaz, podría ser el acontecimiento más importante en la historia de nuestra civilización”. Pero al mismo tiempo nos recuerda la importancia de poner al ser humano en el centro de esta nueva revolución tecnológica: “A menos que aprendamos a prepararnos y evitar los riesgos potenciales, la Inteligencia Artificial podría ser el peor evento en la historia de nuestra civilización. Trae peligros, como armas autónomas poderosas, o nuevas formas para que unos pocos opriman a la masa. Podría causar grandes trastornos en nuestra economía”.\nEn su opinión, sólo “mejores prácticas y gestión eficaz”, y mantener a la vista en objetivos que beneficien a todos, como erradicar la pobreza y las enfermedades, transformando así la sociedad en formas nunca antes vistas.\n“Soy optimista y creo que podemos crear inteligencia artificial para el bien del mundo. Que puede trabajar en armonía con nosotros. Simplemente tenemos que ser conscientes de los peligros, identificarlos, emplear las mejores prácticas y su gestión y prepararnos para sus consecuencias con mucha antelación”, nos recordó.\nEl Dr. Hawking organizó una fiesta para viajeros del tiempo, sin invitación o aviso previo. Nadie acudió a su reunión. En este día, 3/14, sólo podemos esperar que ahora esté vagando libremente en el espacio-tiempo continuando su exploración. El equipo de físicos, matemáticos y científicos de BBVA Data & Analytics le echará mucho de menos.\n \n\n"
    },
    {
      "title": "Cómo los Gatitos Ayudaron a BBVA a Detectar el Fraude con Tarjetas de Crédito",
      "url": "https://www.bbvadata.com/es/kitties-helped-bbva-detect-credit-card-fraud/",
      "date": "13 marzo, 2018",
      "content": "\nUn grupo de científicos de datos de BBVA Data & Analytics ha desarrollado un modelo de aprendizaje profundo que utiliza un novedoso enfoque para la detección de fraudes con tarjetas de crédito. La metodología ha sido aplicada por los científicos de datos de BBVA, Juan Arévalo y Jordi Nin, junto con los profesores de la Universidad Politécnica de Valencia, Roberto Paredes y Jon Ander Gómez. Se inspira en el trabajo de Andrew Ng (Baidu’s Artificial Intelligence luminary) sobre redes neuronales construidas para hacer más eficiente el reconocimiento de imágenes.\nNin explica que exploraron un enfoque más eficiente para la formación de modelos y la clasificación de pruebas para una gran cantidad de transacciones, de las cuales sólo un puñado (esto es sólo comparativamente porque el fraude con tarjetas de crédito ascendió a 1.300 millones de euros en 2012, a pesar de representar sólo el 0,04% del total) se consideran fraudulentos.\nEn el conjunto de datos utilizado en este experimento de reconocimiento de patrones, sólo 1 de cada 5000 transacciones son fraudulentas. Los mayores desafíos para este tipo de conjuntos de datos desequilibrados son la dificultad de realizar un entrenamiento eficiente de una red neuronal; y, en segundo lugar, la necesidad de un reconocimiento rápido de una transacción sospechosa.\nLa solución fue aplicar el filtrado en cascadas para poder aislar qué transacciones son genuinas y obtener una muestra reducida con un máximo de compras fraudulentas con tarjeta de crédito para entrenar nuestra red neuronal profunda. “Encontramos que dos niveles de filtrado eran suficientes para reducir la proporción entre transacciones genuinas y fraudulentas a niveles aceptables”, señala Nin.\nEsta red neuronal es una representación simplificada de la utilizada en el experimento de aprendizaje automático. El diagrama muestra dos filtros y el proceso final de capacitación del modelo para ayudar a determinar si una transacción es fraudulenta\n \nUna técnica similar fue utilizada por Andrew Ng y un equipo de Google Brain’s al tratar de categorizar un número masivo de vídeos de Youtube por imágenes en miniatura. Presentado con un amplio tipo de imágenes el algoritmo de aprendizaje profundo enseñó a identificar gatos como una forma de hacer más eficiente el proceso de categorización, esto es agrupando la ocurrencia más común para optimizar el entrenamiento y reconocimiento.\nEste principio permite procesar un enorme conjunto de datos y a través de la aplicación del aprendizaje profundo permite que el sistema aprenda de ellos, encontrando los patrones comunes y, por lo tanto, aislando los que no cumplen con un cierto criterio, con lo cual se logra una retropropagación más confiable y se logra un índice de éxito similar al de detección de fraude de tarjetas de crédito de otras soluciones comerciales disponibles para las instituciones financieras.\nPara agilizar el entrenamiento de los modelos aplicamos diferentes técnicas: deserción, tasa de aprendizaje adaptativo, normalización por lotes y regularización de Maxnorm. En total, 4 capas ocultas son suficientes para detectar fraudes con tarjetas de crédito a una tasa aceptable.\nAsí es como los gatitos de Youtube ayudaron a detectar el fraude con tarjetas de crédito, un problema cada vez más complejo y esquivo que ahora tiene que lidiar con nuevas formas de tarjetas, pago inalámbrico y un área común en toda la zona euro.\nPara más información sobre este trabajo consulta nuestro artículo científico\nPara obtener más información sobre gatitos en YouTube, puedes ver esto\n \n\n"
    },
    {
      "title": "Lo que Vimos en ICDM 2017: Abordando los problemas de datos con ejemplos del mundo real",
      "url": "https://www.bbvadata.com/es/saw-icdm-2017-tackling-data-problems-real-world-examples/",
      "date": "6 marzo, 2018",
      "content": "\nEn noviembre de 2017, Nueva Orleans (Louisiana), acogió la conferencia más importante del mundo sobre minería de datos. La 17ª edición de este evento presentó nuevos trabajos sobre analítica gráfica, patrones de series temporales y sistemas de recomendación, entre otras disciplinas.\nTradicionalmente, esta conferencia se ha centrado en la escalabilidad y ejemplos prácticos y aplicaciones de algoritmos, y este año no ha sido decepcionante cuando se trata de ejemplos del mundo real. Para BBVA Data & Analytics, donde los algoritmos se aplican en conjuntos de datos reales y a gran escala para resolver los problemas del mundo real de forma eficiente, esta fue una oportunidad para tener una mejor idea de los retos y soluciones en una amplia gama de industrias y disciplinas. \nLas grandes cifras:\n\n778 trabajos presentados, más del 60% de los estudiantes fueron primeros autores. Se aceptaron 72 trabajos regulares (9,3%) y 83 trabajos cortos. Tasa total del 19,9%. \n4 keynotes\n24 talleres\n\nLa conferencia es precedida por un día de talleres. Este año, 11 talleres de jornada completa y 13 de media jornada se desarrollaron paralelamente en un hotel centenario en pleno corazón de la capital del jazz, el cajún y la cocina criolla. Los talleres incluyeron temas como Análisis de Sentimientos, Análisis de Redes, Aprendizaje Automático y Minería de Datos Espacio-Temporal; así como Ciencias de la Información aplicadas a dominios específicos, como Gestión del Capital Humano, Salud, Seguridad Cibernética o Política.\nEn este último grupo, encontré muy interesante DSHCM: Ciencia de Datos para la Gestión del Capital Humano. Varias charlas mostraron el uso de datos relacionales y representaciones gráficas para analizar las interacciones humanas e inferir información faltante de las conexiones de las entidades. Como conferencista invitado, Qi He de Linkedin habló sobre cómo su Linkedin Knowledge Graph les ha permitido analizar la interacción entre las empresas y las personas y mejorar la información subyacente.\nEn el taller de Minería de Datos Gráficos de Alto Rendimiento y Aprendizaje Automático, Danai Koutra presentó su trabajo sobre Minería Gráfica en el Laboratorio GEMS de la Universidad de Michigan. Recomiendo revisar la página web del laboratorio para saber más sobre sus proyectos, pero en particular, recomiendo el trabajo de resumen de gráficos [1], y la versión de evolución temporal [2]. Estos métodos toman una gran red y extraen de ella estructuras gráficas comunes (cliques, stars, bipartite,…). Esto permite representar la red original en términos de un vocabulario de estructuras conocidas que se relacionan entre sí y presentar al usuario la información más importante, permitiéndole entender la macroestructura del gráfico.\n\n \nRegarding the main conference, it was distributed in 4 simultaneous tracks, covering several data science disciplines currently relevant: Pattern and Text Mining, Recommender Systems, Sequences and Time Series, Deep Learning, Streaming and Online learning, Clustering, or Graph Analytics. It also presented a panel on Ethics and Professionalism in the age of Social Data.\nEn cuanto a la conferencia principal, se distribuyó en 4 temas simultáneos, abarcando varias disciplinas de ciencias de la información actualmente relevantes: Minería de Patrones y Textos, Sistemas de Recomendadores, Secuencias y Series Temporales, Aprendizaje Profundo, Streaming y Aprendizaje Online, Clustering o Análisis Gráfico. También presentó un panel sobre Ética y Profesionalismo en la era de los Datos Sociales.\nDejando a un lado las charlas, vi cuatro temas que lideran los enfoques de las charlas: series cronológicas, análisis de gráficos, escalabilidad y fuentes públicas en forma de librerías. Muchas de las obras, independientemente del paradigma principal al que se dirigían, utilizaron redes para modelar el problema.\nAquí mi lista de documentos científicos que encuentro que vale la pena echar un vistazo. Proponen interesantes soluciones a problemas muy frecuentes en nuestro campo:\n \n\nDescubrimiento de las Reglas de Acción al menor coste en Spark [3]. Una regla de acción es una recomendación que, si se sigue, una variable cambia de Falso (o estado no deseado) a Verdadero (o estado deseado). Los autores presentan un algoritmo Spark que extrae las reglas de acción de un conjunto de datos dado, considerando que hay acciones prohibidas -las que no se pueden lograr- y tienen costes diferentes.\nRedes Neurales Espacio-Temporales para la predicción de Series espacio-temporales [4]. Los autores presentan un algoritmo basado en redes neuronales para la predicción de series temporales que explota las relaciones espacio-temporales entre diferentes series. Un problema muy frecuente en las previsiones de TS cuando no se dispone de datos históricos, pero hay un gran número de series relacionadas.\nDescubrimiento de red basado en hashing escalable [5]. En la neurociencia -y otros campos- es muy común construir redes a partir de series temporales correlacionadas: los nodos representan cada serie temporal y los bordes suelen crearse cuando dos series temporales están altamente correlacionadas. El algoritmo de fuerza bruta para seleccionar esos pares es muy ineficiente. En este artículo los autores presentan un método basado en hashing para construir esa red mucho más rápido y obtener un resultado muy similar.\n\n \n\n \n\nConstrucción de conjuntos de datos a través de Attention for Aspect Term Extraction (ATE) con supervisión a distancia [6]. Los datos anotados son muy frecuentemente un problema cuando se construyen modelos estadísticos. En particular, hay una pequeña cantidad de conjuntos de datos disponibles para la extracción supervisada de Aspect Term y cubren sólo unos pocos dominios. Esto nos lleva a la necesidad de crear nuevos conjuntos de datos para entrenar los modelos. Los autores han desarrollado un algoritmo que explota los datos anotados para construir nuevos conjuntos de datos anotados para la tarea ATE. Utilizan modelos de atención para identificar oraciones que probablemente contengan información relevante para la tarea.\n\n \nEn general, recomiendo encarecidamente asistir a esta conferencia a cualquiera que se ocupe de problemas de datos del mundo real, ya que ayuda a conocer los últimos resultados en algoritmos de datos sofisticados y eficientes a gran escala.\nEspero que disfruten las lecturas!\n[1] http://www.mlgworkshop.org/2016/paper/MLG2016_paper_29.pdf"
    },
    {
      "title": "Client2vec, una manera más eficiente de conocer las preferencias de un cliente",
      "url": "https://www.bbvadata.com/es/client2vec-finding-better-way-understand-clients-preferences/",
      "date": "27 febrero, 2018",
      "content": "\nLa lingüística computacional ha avanzado a pasos agigantados en los últimos años. Ha permitido, por ejemplo, extraer el significado de una palabra simplemente analizando su contexto, sin un diccionario, y usando redes neuronales relativamente sencillas. Tomas Mikolov, científico de Facebook Research, fue pionero en un proceso más eficiente con su trabajo word2vec, ahora una metodología patentada de Google. Su trabajo ayudó a las máquinas a entender las relaciones entre palabras que tienen una estrecha relación semántica. La incorporación del enfoque de word2vec a la traducción de idiomas mejoró la confiabilidad y la agilidad de estos servicios. Posteriormente, muchos métodos para vectorizar palabras han aparecido, aumentando el toolset. Inspirados por este conjunto de iniciativas, en BBVA Data & Analytics hemos experimentado con Client2vec, un trabajo similar para entender el compartamiento de clientes.\nEste enfoque, dirigido a hacer más eficiente y productivo el trabajo en ciencia de datos y descubrimiento de patrones,  permitió al matemático y científico de datos BBVA Data & Analytics Leonardo Baldassini explorar formas en que la banca puede abordar casoscomo la segmentación de clientes.\nClient2vec es una biblioteca interna que utiliza autoencoders de eliminación de ruido apilados marginalizados en datos de transacciones de cuenta corriente para crear vectores que representan los comportamientos de los clientes. Estas representaciones se pueden utilizar y optimizar en una variedad de tareas, como la segmentación del cliente, el perfilado y las recomendaciones.\n¿Qué pasaría si pudiéramos leer las necesidades del cliente de la misma manera que entendemos el significado de las palabras en una oración, no confiando solamente en datos sociodemográficos o el valor de una transacción, y optimizando la agrupación de clientes considerando su comportamiento como consumidores y no solo sus ingresos o código postal?\nBaldassini descubrió en el curso de su investigación, realizada en colaboración con el científico senior de datos de BBVA D & A José Antonio Rodríguez Serrano que agrupar las preferencias de los viajeros según los criterios sociodemográficos era casi tan inexacto como hacerlo ejecutando una agrupación aleatoria. En cambio, al aplicar el enfoque Client2vec, pudo reagrupar a los viajeros mediante patrones de gasto. Para probar su hipótesis, eliminó los datos de la categoría de gasto del hotel y verificó que podía predecir si el alojamiento del hotel iba a estar presente cuando existía un patrón de compra común que identificaba el comportamiento del viajero típico (ver imagen a continuación).\n\nEl desarrollo de esta nueva metodología podría ayudar a inferir con precisión el interés de un cliente en un determinado producto financiero al analizar su comportamiento en otras categorías, de forma muy similar a situar a cada cliente en un entorno espacial georeferenciado relacionado con su comportamiento. Esto podría incluso permitir comprender el perfil de nuevos clientes y agruparlos sin tener que depender únicamente de variables como el código postal, el ingreso o la edad, que pueden ser engañosas.\nBaldassini, quien es coautor de un documento sobre el tema con Rodríguez, cree que su investigación puede traducirse a diferentes casos de uso para la agrupación y la predicción del gasto.\nEl trabajo de client2vec se basa en un modelo bien conocido, el autoencoder marginalizado, para transformar datos transaccionales en una matriz más simple que mantiene la estructura relacional de diferentes comportamientos de compras, lo que permite representar a un cliente como un punto en un espacio multidimensional. Luego, aplicando algoritmos de vecindad a estos puntos, es posible encontrar similitudes entre los clientes en una amplia gama de categorías objetivo.\nHasta ahora, estas técnicas se han aplicado para probar los datos de las preferencias de viaje, pero se pueden escalar con los datos de compra de los clientes de los puntos de venta (TPV) u otras fuentes de datos en tiempo real. Una de las posibles aplicaciones en el futuro es adaptar las recomendaciones de productos a nuestros clientes de una manera más precisa, simplemente buscando respuestas y preferencias en grupos de comportamiento similar. De esta manera, podríamos evitar molestar al cliente con ofertas o recomendaciones que no son relevantes para sus necesidades en un momento dado.\nLea la investigación de Baldassini y Rodríguez aquí.\n\n"
    },
    {
      "title": "Innovación Más Enfocada en el Consumidor. Nuestra Participación en ENTER 2018",
      "url": "https://www.bbvadata.com/es/innovation-will-become-customer-focused-participation-enter-2018/",
      "date": "20 febrero, 2018",
      "content": "\nENTER 2018, organizado anualmente por la Federación Internacional de Tecnologías de la Información, Viajes y Turismo (IFITT), tuvo lugar la última semana de enero en Jönköping (una encantadora ciudad sueca a orillas del lago Vättern). El tema central este año fue “Turismo digital: compromiso, contenido y redes”. Hubo una buena combinación de oradores del mundo empresarial y académico, quienes cubrieron asuntos que van desde el uso de robots en la industria hotelera hasta la optimización de precios de última hora (el programa está disponible aquí).\nSin embargo, dos conceptos principales estuvieron presentes a lo largo de la conferencia: personalización y empoderamiento del cliente. En ambos casos, conocer a los clientes a través de los datos es lo que permite a los agentes de la industria mejorar su relación con ellos. Thierry Gnych de IBM Watson resumió estas ideas y la tendencia actual de manera muy precisa: “cambiar de ser conocido por el cliente a conocer al cliente”.\nNuestras keynotes favoritas\nUna de las cosas que más me gustaron de los keynotes fue la diversidad de perfiles de los oradores. Desde ingenieros hasta antropólogas, todos los ponentes examinaron los desafíos a los que se enfrenta la industria de viajes y turismo. Esta amplitud de conocimientos llevó a enfoques múltiples y enriqueció la discusión. A mí personalmente me gustaron las siguientes presentaciones:\nIrene Ng presentó HAT (Hub-of-All-Things), una tecnología que cede al usuario el control de sus datos personales. En el contexto de GDPR y PSD2, esta plataforma ofrece a los ciudadanos la posibilidad de recopilar su información personal desde aplicaciones y decidir si quieren que otras compañías la utilicen y de qué manera. De esta manera, los usuarios pueden obtener valor de sus datos. Es bastante interesante observar cómo reaccionan los clientes ante el nuevo marco regulador de uso de datos privados y si este tipo de plataformas jugarán un rol tanto para controlar el uso de información personal como para monetizarla. La oradora señaló que esta es una gran oportunidad para que la industria de viajes utilice nuevas fuentes de datos para mejorar su conocimiento de los clientes.\n\nKatarina Graffman, antropóloga de la Universidad de Uppsala, habló sobre la forma en que las nuevas generaciones (Millennials, Generación Z, …) se comportan e interactúan con la tecnología. Su argumento consideraba que, si las empresas quieren conocer a estos clientes y atraerlos, deben reconsiderar los datos que están recopilando. Conceptos tales como inmediatez, seguridad, confianza o personalización no significan lo mismo para las personas nacidas en diferentes décadas y aquellos que se adaptan mejor a cada cliente tendrán una ventaja competitiva.\n\nRotem Shenor, de la Universidad de Agder, mostró una imagen detallada del estado del crowdfunding en los países europeos y cómo las regulaciones han cambiado el panorama en el que operan. Me gustó mucho descubrir que la mayoría de la industria de viajes considera que el crowdfunding es una oportunidad para innovar en lugar de una amenaza.\nNosotros también subimos al escenario. Esto es lo que mostramos\nDetrás del título “Utilizando datos transaccionales para determinar el entorno habitual de los titulares de tarjetas” presentamos nuestra metodología para inferir el área donde los titulares de tarjetas llevan a cabo su vida diaria utilizando las transacciones de tarjetas. Esta información nos permite distinguir entre las transacciones turísticas (aquellas realizadas fuera del área de entorno habitual) y las no turísticas. Podríamos haber utilizado la dirección declarada de los titulares de las tarjetas, pero la nueva metodología es más precisa ya que detecta las rutinas de cada titular. Algunos de los aspectos en los que hemos trabajado han sido la adaptabilidad a diferentes tipos de propietarios de tarjetas (desde los que usan su tarjeta todos los días hasta los que la utilizan puntualmente) y también a diferentes geografías. De hecho, esta metodología ya se ha aplicado a todo el conjunto de titulares de tarjetas BBVA tanto en España como en México con pequeños ajustes.\nAlgunos de los proyectos públicos que utilizaron el atributo ambiental habitual son el análisis de Pueblos Mágicos (el atributo se utilizó para dividir entre residentes y turistas el gasto realizado por los mexicanos) o Urban Discovery (veíamos qué comunidades dentro de las ciudades son más atractivas para los turistas domésticos).\nLa metodología tuvo una acogida favorable y se plantearon muchas preguntas sobre la forma de adaptarla a los diferentes conjuntos de datos, cómo escalarla y el acceso a los datos agregados de BBVA a través de API Market (principalmente procedentes de investigadores universitarios).\n\nNúmero de clientes BBVA cuyo entorno habitual incluye la localidad.\nENTER 2019 tendrá lugar en Nicosia, Chipre. ¡Allí nos vemos!\n\n"
    },
    {
      "title": "Cómo el Big Data ha Transformado BBVA y la Manera en la que Hacemos las Cosas",
      "url": "https://www.bbvadata.com/es/big-data-transformed-bbva-way-things/",
      "date": "15 febrero, 2018",
      "content": "\nFabien Girardin, co-CEO de BBVA Data & Analytics, concedió una entrevista publicada recientemente a la revista Knowmadas en la que analiza la aplicación avanzada de técnicas de análisis Big Data en la banca y los principales componentes que sirven para convertir a un gran banco como el BBVA en una empresa data-driven y con el foco en la innovación.\nHemos elegido las reflexiones más interesantes que explican, no solo la filosofía de BBVA Data & Analytics, pero también metodologías probadas que ayudan a una empresa ha realizar un uso inteligente, responsable y efectivo de los datos.\n“Lo que es nuevo con el Big Data, es la cantidad de datos que nos permite entender el mundo de mejor manera, y cuando digo ‘el mundo’ me refiero a los clientes, empresas, y también como funciona la propia organización como un banco. Eso nos permite realmente medir y entender los procesos, intentar automatizarlos, y ayudar a la toma de decisiones de manera nueva”. — Fabien Girardin\nTres claves en Big Data\nFabien señalaba que las tres patas que han permitido al BBVA ofrecer nuevos productos y servicios a sus clientes gracias al uso de los datos han sido:\n\nLa infraestructura, la arquitectura y  la gestión del dato por parte de ingeniería.\nTalento, especialmente en el ámbito del Data Science, que permite aplicar el método científico a la hora de explorar, utilizar los datos y transmitir los conocimientos.\nGobierno del dato o como promover un uso responsable de los datos, normas de uso y estándares de mantenimiento de la calidad del dato.\n\nCómo BBVA Data & Analytics ideó Commerce360\n“Hay muchas más aplicaciones que ofrecemos a clientes, por ejemplo, de TPV, que es información contextual de como va su negocio, no solamente internamente, si no en función de la competencia que tiene a su alrededor. Esto es información que es completamente nueva para estas pequeñas empresas para gestionar precisamente su negocio”. Puede leer nuestro artículo “The Powerful Fusion of Intuition with Data”, (en inglés), para entender mejor la funcionalidad de Commerce360 o visitar el sitio web de la herramienta.\nLa clave es tener una cultura basada en datos\n“La clave realmente es generar una cultura que esté basada en los datos, no solamente tener talento, pero que hay un “mindset” que tenemos que ser capaces de compartir con el resto de la organización, y esa es una de nuestras labores educativas con analistas, pero también con el resto de compañeros: diseñadores, ingenieros, “product managers”, etc. con los cuales trabajamos e intentamos realmente que ellos entiendan las capacidades que hay aquí, y que nos ayuden realmente a encontrar dónde tienen que llegar estas capacidades dentro de la organización, y para clientes.”\nPuedes ver su entrevista completa aquí:\n\n\n"
    },
    {
      "title": "Lo que Vimos (y nos Gustó) en 2017 — Parte 3",
      "url": "https://www.bbvadata.com/es/saw-liked-2017-part-3/",
      "date": "13 febrero, 2018",
      "content": "\nUn campo más amplio para el Machine Learning: desde los análisis de redes hasta las nuevas interacciones con máquinas\nInteracciones entre máquinas y humanos\n— por Iskra Velitchkova\nA la ciencia, que tradicionalmente ha vivido cómoda en la complejidad, se le exige, por parte de la sociedad en general y de las legislaciones que de forma natural surgen en paralelo a los avances en la algoritmia que subyace al Machine Learning, la habilidad de hacer comprensibles los procesos. Hablamos de cajas negras, de modelos interpretables o de transparencia en las técnicas, pero por encima de todo, de un lenguaje común entre humanos y máquinas.\nLo que a continuación queremos compartir, no son necesariamente contribuciones de 2017, pero sí son referencias que a lo largo del año nos han servido de referencia, de inspiración y de punto reflexionar en esta charla sobre las formas en las que hoy en día interactuamos con las máquinas.\n“¿Qué es una píldora anticonceptiva? Es una píldora. Pero también es un conjunto de componentes químicos. Pero también es un cambio en el papel en la estructura de la sociedad y el papel de la mujer dentro de ella.”\nCon este ejemplo, y alguno más, George Whitesides nos invita a reflexionar sobre la simplicidad en la ciencia. Algo que hoy, más que nunca, en un momento en el que esta permea, sin precedentes, en prácticamente todos los espacios de la sociedad. Y nos preguntamos, por ejemplo: ¿Qué es un algoritmo? Es una instrucción. Pero también es una conjunto de reglas. ¿Puede ser algo más?\nEn esta línea, Ben Cerveny, reconocido diseñador y experto en interacción de usuario, nos vuelve a provocar con una reflexión sobre el tiempo que dedicamos a estas nuevas aplicaciones y sobre las necesidades que cubren (o no) los servicios que crean estos algoritmos. Es más, nos obliga a rendir cuentas a los científicos, a exigirles ser capaces de saber explicar los algoritmos detrás de los procesos que desarrollan. Y nos deja abierta la puerta a cómo lograrlo. ¿Necesitamos nuevas formas de comunicación? ¿Necesitamos crear un lenguaje común? ¿Necesitamos nuevas narrativas?\nSiguiendo en la misma dirección, Cathy O’Neil, a la que muchos conoceréis por su conocido libro “Weapons of Math Destruction”, nos habla en esta charla sobre ética y honestidad. Y es que asumir que algunos algoritmos son cajas negras, implica fe, a veces ciega, y es lo que no somos capaces de solucionar; esto es, saber interpretar la opacidad de ciertos procesos, perdemos algo tan crucial como es la confianza.\nPor último, nos gustaría destacar el material que podemos encontrar en la galería de NIPS Machine Learning for Creativity and Design Workshop 2017. Pues desde el punto de vista de visualización, el arte y la abstracción se están convirtiendo en herramientas muy útiles que acompañan de cerca la exploración científica.\nDatos relacionales, descubriendo el contexto de nuestros clientes\n— por Jordi Nin, Elena Tomás y Pablo Fleurquin\nVivimos en un mundo cada vez más interconectado. En la era digital casi no hay piezas de información aisladas. Los datos hoy son dinámicos y están conectados en una rica red de relaciones complejas que van desde IoT y dispositivos inteligentes hasta redes sociales o empresariales, por nombrar sólo algunos. Desde una perspectiva analítica o científica, la Teoría de Redes Complejas (también conocida como Teoría de Grafos) ha estado floreciendo (otra vez) desde principios del siglo XXI. El campo es vasto y los avances son difíciles de seguir. Para entrar en contacto con los fundamentos teóricos se podría seguir el informe Redes complejas: Estructura y dinámica. Centrándonos en 2017 y haciendo especial hincapié en el subdominio económico y financiero este año, hemos sido testigos de un cambio importante en la perspectiva del modelado macroeconómico con el documento científico premio Nobel de Joseph Stiglitz admitiendo Dónde se equivocó la Macroeconomía Moderna y señalando el camino a seguir para considerar la estructura subyacente en red de los sistemas financieros y económicos: “comprender las estructuras que son más propicias para la estabilidad y las compensaciones centrales (ej. entre la capacidad de resistir choques pequeños y grandes) representa una de las áreas de importantes avances desde la crisis. Éstas eran cuestiones que ni siquiera se planteaban en el marco del DSGE, no podían plantearse porque no se planteaban en ausencia de un sector financiero bien especificado, y no surgirían dentro de un modelo con una institución financiera representativa”.\nEn este sentido, el artículo Caminos hacia la inestabilidad en las redes financieras aborda el modelado macroeconómico desde una perspectiva en red y documento científico del Banco de Inglaterra Un modelo interdisciplinario para la macroeconomía explora nuevas formas complementarias de hacerlo.\nAlgoritmos genéticos\n— por Israel Herraiz\nEntrenar una Red Neural Profunda es un problema de optimización. La red óptima minimizará el valor de la función de pérdida. Tradicionalmente, los métodos de optimización aplicados a este problema de optimización se basan en el descenso de gradientes. Backpropagation, es decir, un método para calcular el gradiente de la función de pérdida contra los pesos en la red, ha sido un gran éxito en el entrenamiento de redes neuronales profundas. Gracias a esto, hemos visto grandes saltos hacia adelante en el aprendizaje supervisado (visión computacional, reconocimiento de voz), así como cuando se utilizan redes neuronales profundas en el aprendizaje por refuerzo.\nEncontrar un valor óptimo en un distribuidor usando el descenso de gradiente tiene algunos problemas teóricos: por ejemplo, podemos quedarnos atascados en un mínimo local. En la práctica, hemos visto resultados lo suficientemente buenos como para hacernos creer que los mínimos locales no son un problema tan grande. Mediante el uso de métodos de optimización basados en gradiente, las máquinas pueden llegar a ser tan competentes como los seres humanos jugando a algunos juegos Atari clásicos. Sin mencionar los éxitos más grandes, como Alphago (ver El Aprendizaje por Refuerzo se supera a sí mismo en el primer post de estas series).\nPero, ¿son estos resultados realmente óptimos? ¿Podemos hacerlo mejor? No podemos decirlo, a menos que encontremos ejemplos empíricos que funcionen mejor. Uber AI Labs ha demostrado que la respuesta a la búsqueda de una mejor optimización puede estar en la forma en que la naturaleza encuentra el camino más óptimo para la selección natural. Mediante el uso de Estrategias de Evolución y Algoritmos Genéticos, estos investigadores han encontrado modelos que se desempeñan mejor jugando algunos juegos clásicos de Atari, en comparación con los resultados anteriores usando Aprendizaje de Refuerzo con Redes Neuronales Profundas (RL profundo). En algunos casos, incluso han encontrado que la búsqueda aleatoria puede funcionar mejor que los modelos RL profundos. ¿Eran estos juegos intrínsecamente duros? ¿O hemos estado usando los métodos de optimización equivocados? ¿La optimización sin gradientes se convertirá en otra herramienta en nuestra banda de aprendizaje profundo?\nProgramación probabilística\n— por María Hernández y César de Pablo\nEl año pasado no sólo hemos visto avances en el aprendizaje profundo, sino que también hemos sido testigos del nacimiento de nuevos marcos de programación probabilística como Edward y Uber Pyro, así como avances en algunos de los ya populares (Stan, PyMC3).\nEste nuevo framework se adapta a estructuras DL como Tensorflow o PyTorch para implementar los recientes avances en el uso de algoritmos basados en el descenso de gradientes estocásticos como algoritmos de inferencia posterior, allanando el camino para la inferencia de cajas negras, pero también interesantes vías para la hibridación del aprendizaje profundo y bayesiano.\nComo las aplicaciones financieras no sólo se benefician por predecir un valor, sino también por ser capaces de estimar la confianza en estas predicciones, esperamos sumergirnos más profundamente en él durante 2018.\nAprendizaje profundo bayesiano\n— por Axel Brando\nHoy en día, aunque los algoritmos de aprendizaje profundo han revolucionado en muchos campos como la visión computarizada, estos algoritmos son generalmente incapaces de saber cuán confiados están sobre sus predicciones. Esto tiene una implicación crítica: Si queremos utilizar las técnicas de aprendizaje profundo para una decisión arriesgada, donde preferimos no predecir si no tenemos la suficiente confianza, estos modelos no son una solución precisa. Para abordar este problema, una solución que ha ganado peso durante el último año y cada día más presente en las Conferencias de Aprendizaje Automático es mezclar el punto de vista Bayesiano (que nos da herramientas para entender el concepto de incertidumbre) con el poderoso poder predictivo de los modelos de Aprendizaje Profundo. Esta rama del conocimiento parece ser llamada Aprendizaje Profundo Bayesiano.\nPara entender la evolución reciente de esta rama, un buen punto de partida es la tesis doctoral de Yarin Gal sobre la incertidumbre en el aprendizaje profundo y los tutoriales del framework Edward. En particular, algunos de los artículos más impresionantes sobre este campo fueron publicados el año pasado, por ejemplo, las propuestas Bayesianas de Aprendizaje Profundo de Alex Kendall para IA segura o la aplicación en un problema real de detección de enfermedades propuesto por Christian Leibig et al. en un artículo de Nature o incluso modelos más cercanos a conceptos como los procesos gaussianos como el tutorial de NIPS 2017 de Neil D. Lawrence. Todo parece indicar que esto no acabará aquí.\n\n"
    },
    {
      "title": "Lo que Vimos (y nos Gustó) en 2017 — Parte 2",
      "url": "https://www.bbvadata.com/es/saw-liked-2017-part-2/",
      "date": "6 febrero, 2018",
      "content": "\nEl uso de Machine Learning para reforzar la confianza\nHoy en día, la creciente huella digital de nuestros clientes y nuestras capacidades analíticas nos proporcionan una comprensión mucho más profunda y una visión compleja de la dinámica de las grandes corporaciones, clientes y ciudadanos. Sin embargo, estas oportunidades tecnológicas también generan riesgos que pueden desembocar en desconfianza. Debemos darnos cuenta de que las innovaciones tecnológicas no son intrínsecamente buenas o malas. Cómo los humanos controlan y usan estos nuevos “superpoderes” plantea profundas cuestiones éticas.\nRiesgos derivados de soluciones disfuncionales de Machine Learning\n— por Juan Murillo\nLos fallos más notorios de 2017 en las aplicaciones de Machine Learning (ML) nos advierten sobre los problemas que requieren más cuidado y rigor en esta disciplina y nos obligan a mantener un ojo escéptico. Detrás de esos casos a menudo encontramos:\n\nproblemas de calidad de los datos ( ejemplo aquí, aquí y aquí)\nsesgos en bases de datos no balanceadas\nerrores metodológicos y malas prácticas en el lado humano\n\nMinimizar estos tres efectos con procesos de refuerzo de la calidad y rendimiento de cuentas algorítmico es un factor clave para asegurarnos que las aplicaciones de los datos contribuye a reforzar la confianza en los proveedores de servicios, y no desemboca en una erosión de esa confianza. Además, tenemos que ser conscientes de las diferentes implicaciones de los fallos en ML (falsos positivos y falsos negativos) en diversos campos, debido a que las consecuencias y los niveles de responsabilidad son totalmente diferentes si estamos recomendando una canción, un producto financiero o diagnosticando una enfermedad.\nLa fiabilidad es la mayor barrera para la expansión de las soluciones basadas en Machine Learning, debido a que una vez la gente detecta que un sistema autónomo ha fallado, probablemente no va a usarlo una segunda vez.\nOpiniones incrustadas en el código: sesgos e imparcialidad\n— por Pablo Fleurquin, Roberto Maestre, Elena Alfaro y Fabien Girardin\nUno de los debates más vibrantes dentro de la comunidad ML/AI se centra en la idea de que, sin una nueva voluntad explícita, el ML puede terminar siendo injusto en su toma de decisiones. Dos de las principales razones detrás de un problema tan generalizado son la disparidad en la representación de minorías y los sesgos humanos codificados en los datos. La primera es fácil de entender, los grupos minoritarios están subrepresentados por definición en muchas muestras de datos, lo que conduce a tasas de error más altas en estos grupos. La última, el sesgo humano, es parte integral de los datos y en la mayoría de los casos es indistinguible de ellos. Los sesgos se manifiestan en muchos tipos: sesgos demográficos, geográficos, de conducta o temporales.\nUn tema tan importante no pasó desapercibido en 2017 NIPS con un exhaustivo tutorial de Moritz Hardt y Solon Barocas (Berkeley y la Universidad de Cornell) sobre la imparcialidad en Machine Learning y una inspiradora charla de la investigadora principal de NYU y cofundadora de Microsoft Research Kate Crawford en The Trouble with Bias. Se pueden encontrar recursos adicionales sobre el tema en la comunidad de rápido crecimiento en torno a la conferencia FAT / ML, en particular, la charla invitada de 2017 de la investigadora principal de Google Margaret Mitchell The Seen and Unseen Factors Influencing Knowledge in AI Systems. A principios de 2017, la revista científica publicó que Semantics derived automatically from language corpora contain human-like biases (A. Caliskan et al.) sobre prejuicios y estereotipos preexistentes en asociaciones de palabras derivadas semánticamente.\nAdemás de detectar decisiones parciales y sesgadas, la comunidad está comenzando a abordar el problema utilizando diferentes enfoques. El grupo MPI-SWS de Max Planck ha establecido algunos enfoques interesantes para abordar este problema (ver Beyond Distributive Fairness in Algorithmic Decision Making: Feature Selection for Procedurally Fair Learning) y puede ser explorado en las ideas compartidas en la FAT / ML 2017 charla del investigador principal de Microsoft, Rich Caruana, Friends Don’t Let Friends Deploy Black Box Models: Preventing Bias via Transparent Machine Learning. Por lo tanto, no solo debemos detectar, sino abordar el aprendizaje desleal y sesgado, cambiar el rumbo para generar confianza y sacudir la conciencia de la comunidad sobre el asunto.\nTransparencia como valor central del Machine Learning\n— por Pablo Fleurquin, Manuel Ventero, Alberto Rubio y Jordi Aranda\nLa transparencia también conocida como Machine Learning interpretability es una parte clave del conjunto de herramientas para abordar la desconfianza en nuestros procesos algorítmicos de toma de decisiones. Se puede utilizar para promover el aprendizaje equitativo y, además, impregna la cultura organizacional con responsabilidad ética. Como lo expresó el gran físico del siglo XX Richard Feynman: “si no puedes explicar algo en términos simples, no lo entiendes”. Esta máxima que es tan aceptada en las ciencias puras, no es tan común en Ciencia de Datos. Implica una asociación bidireccional entre la explicación y la comprensibilidad, que en última instancia se oponen a la transparencia contra el “blackbox-ness”. Sin embargo, debe tenerse en cuenta que los algoritmos “black-box” no son exclusivamente de naturaleza no lineal; los modelos lineales generalizados de gran dimensión y fuertemente afinados también pueden ser muy opacos. Afortunadamente, estos últimos años, se han realizado esfuerzos para desarrollar herramientas que arrojen luz sobre el proceso de decisión algorítmica. Comenzando con marcos agnósticos de modelos como LIME o explorando cómo las características de entrada están asociadas a las predicciones en Deep Learning, los métodos están apareciendo para despejar el camino y desmontar la máquina para explicar sus piezas. Es importante destacar que, como se muestra a fondo por Patrick Hall et al. en Ideas on Interpreting Machine Learning la transparencia del aprendizaje automático comienza en la fase de exploración y varios métodos estadísticos y de visualización pueden proporcionar interpretabilidad global y local, sin necesidad de un marco de interpretación. Para aquellos que buscan una visión en profundidad sobre el tema, el libro Transparent Data Mining for Big and Small Data es un buen referente.\n¿Podría el Machine Learning ser un problema para la privacidad?\n— por Juan Murillo y Pablo Fleurquin\nAl tiempo que nuestra huella digital aumenta, el debate sobre protección de la privacidad es frecuentemente relacionado con problemas de seguridad nacional. El equilibrio entre derechos fundamentales de los ciudadanos y el rol de los estados y las corporaciones está siendo definido con nuevos marcos legales en Europa y China, como principales exponentes. Ya se ha demostrado que el Deep Learning puede ser utilizado para reconstruir caras ocultas mediante métodos que tienen como objetivo proteger la privacidad, como la pixelización o el desenfoque gausiano.\nOtra investigación del año pasado sobre inferencias mostró un modo de determinar si un registro de datos fue utilizado para entrenar a redes neuronales. Como apunta el autor: “saber que el historial clínico de cierto paciente fue utilizado para entrenar un modelo asociado con cierta enfermedad (como por ejemplo determinar la dosis apropiada de una medicina o descubrir su base genética) puede revelar que un paciente tiene esa enfermedad”.\nRiesgos inherentes de la democratización del Machine Learning como un servicio en manos de no expertos\n— por Juan Murillo y Pablo Fleurquin\nEn los últimos años, la mayoría de las grandes compañías tecnológicas han comenzado a ofrecer lo que se conoce como Machine Learning como servicio: Google Prediction API, Amazon Machine Learning, Microsoft Azure Machine Learning, entre otros. Recientemente, hemos sido testigos del lanzamiento del Cloud AutoML de Google, que en palabras de Fei-Fei Li, Chief Data Scientist de Google Cloud AI, es un producto que permite a todos construir su propio modelo ML personalizado sin mucha experiencia en ML. Un gran paso hacia la democratización de la inteligencia artificial.\nJunto con esta “democratización” de la inteligencia artificial, la comunidad de Machine Learning ha comenzado a ser consciente de las posibles dificultades de ML. Quedan desafíos por abordar, desde la privacidad hasta la equidad y la transparencia. Por lo tanto, es de suma importancia que la democratización de la IA se conjugue con la experiencia y la responsabilidad ética y técnica del lado del profesional. La democratización es bienvenida, pero eso no significa que todo valga. Como Cathy O’Neal lo expone en su Ted Talk 2017 The era of blind faith in Big Data must end: “muchas cosas pueden salir mal cuando ponemos la fe ciega en Big Data”. El problema es que para abordar todos estos desafíos necesitamos una especificidad y conocimiento de la disciplina. Sería muy pretencioso y ciertamente imposible encontrar un motor de ML automático que nos proteja contra todos los peligros mencionados.\nOtra lección que la investigación Membership Inference Attacks Against Machine Learning Models (ver ¿Podría el Aprendizaje Automático ser un problema para la privacidad?) nos enseña que se debe poner especial atención en el desarrollo de algoritmos porque overfitting en modelos Machine Learning puede provocar que se filtre información sobre los datos utilizado para entrenar dicho modelo.\nDespués de todo, la competencia es uno de los pilares principales de la confianza; por lo tanto, uno debería preguntarse a sí mismo: como cliente de un banco, ¿confiaría en mi hipoteca, mi salud financiera en un proceso de toma de decisiones basado en algoritmos basado en baja experiencia?\n\n"
    },
    {
      "title": "Lo que Vimos (y nos Gustó) en 2017",
      "url": "https://www.bbvadata.com/es/saw-liked-2017/",
      "date": "31 enero, 2018",
      "content": "\nEn BBVA Data & Analytics nos encanta compartir artículos, blogs y noticias sobre Inteligencia Artificial (AI) y Aprendizaje Automático (ML) para mantener al día nuestro compromiso y pasión por la utilización del Aprendizaje Automático en el campo de los datos financieros. Hace dos años creamos una comunidad en Google Plus que ha crecido hasta convertirse en un lugar de intercambio de novedades y opiniones sobre el futuro de la disciplina en la que trabajamos.\nCon la llegada de 2018, nos gustaría compartir y abrir a discusión los contenidos más comentados de nuestra comunidad interna de expertos a lo largo de 2017. Este compendio refleja tendencias y conocimientos que creemos beneficiosos para BBVA y contiene una gran variedad de contenidos, con un protagonismo especial del Deep Learning, pero estos días quién no le da una relevancia especial a esa rama del ML.\nTras profundas discusiones en nuestro equipo, hemos seleccionado una lista de las contribuciones científicas más disruptivas que se han publicado en 2017 en nuestra opinión. Las hemos agrupado en tres partes: la primera, detalla algunos avances específicos en modelos de Deep Learning. La segunda, ofrece una perspectiva más amplia en la aplicación de ML, riesgo que se deben evitar, o como usar capacidades en este campo para mejorar la confianza y la imparcialidad en ML, un aspecto central en nuestro trabajo en BBVA. Finalmente, en la tercera parte, examinaremos las creciente y compleja relación entre humanos y máquinas, al tiempo que revisaremos los avances en el campo de redes complejas y programación probabilística en Deep Learning, como en el caso de métodos Bayesianos.\nPara una visión más general de eventos en IA relevantes, recomendamos: Danny Britz’s summary, así como Google Brain’s recap of their own contributions.\nParte 1. Contribuciones científicas en DL\nReexaminando la eficiencia de los datos en la era del Deep Learning\n— por Jose Antonio Rodriguez Serrano and César de Pablo\nDe cuando en cuando, la comunidad de ML se enfrasca en una discusión recurrente: ¿invertirías en más datos o en mejores modelos? Aunque podría considerarse un falso dilema, los investigadores de ML normalmente se esfuerzan en la segunda prioridad: obtener modelos sólidos. En ocasiones, aparece un contraejemplo que sugiere que invertir en recopilación masiva de datos puede arrojar resultados sin precedentes. Investigaciones en esta línea incluyen “The Unreasonable Effectiveness of Data”, así como el esfuerzo de la colección de ImageNet.\nEn la era de Deep Learning, es legítima la pregunta de si este debate todavía tiene sentido para este tipo de modelos. El trabajo de Sun et al. (Google) aborda esta cuestión formando una deep neural network con una cantidad de datos sin precedentes (300 millones de imágenes, tres veces más grandes que el conjunto de datos anterior) y muestra que en tareas de aprendizaje comunes, el rendimiento sigue aumentando con más datos y no se satura (siempre que el modelo siga aumentando su capacidad). Otros hallazgos se pueden encontrar en el documento mencionado o en este blog post.\nEl debate sobre el rendimiento de más datos frente a mejores modelos a menudo vuelve a transitar por los algoritmos de búsqueda nearest-neighbor, un ejemplo de método que ofrece soluciones simples pero efectivas, especialmente cuando tratamos con montos de datos masivos. Precisamente este año NIPS organizó el workshop “Nearest neighbors for Modern Applications with Massive Data”, mientras que Facebook AI Research lanzó FAISS, una librería con implementaciones eficientes de búsquedas nearest-neighbor.\nEl reinforcement learning se supera a sí mismo\n— por Roberto Maestre and Juan Duque\nDurante muchas décadas, el antiguo juego de Go fue un desafío imposible para los investigadores de inteligencia artificial de todo el mundo. Una razón es la gran cantidad de combinaciones posibles en el tablero, que hace que una búsqueda exhaustiva de movimientos sea una tarea inviable, ya que hay más configuraciones posibles de tablero que átomos en el Universo. Además, la dificultad de definir una función de evaluación de posición adecuada evita que se pueda definir una búsqueda con predicciones fáciles de calcular.\nCuando en enero de 2016 DeepMind AlphaGo de Google derrotó al campeón europeo de Go Fan Hui por 5 a 0 y se impuso al 18 veces campeón mundial Lee Sedol, ganando 4 de 5 partidas, fue difícil no ver que se había alcanzado un punto de inflexión. Finalmente un algoritmo de IA podía dominar el juego de Go. Para romper esta barrera, AlphaGo combinó redes neuronales artificiales profundas, búsqueda de árboles de Monte Carlo y Reinforcement Learning (RL), entre otras técnicas. Pero lo mejor estaba aún por llegar.\nNo contento con los logros alcanzados, en octubre de 2017 DeepMind reveló que una nueva versión de AlphaGo, llamada AlphaGo Zero, había ganado a su predecesor en el 100% de las partidas.\nInesperado movimiento 37 del segundo partido entre AlphaGo y el campeón del mundo Lee Sedol. Aunque parecía ser un error, este sorprendente movimiento ayudó a AlphaGo a ganar el segundo partido.\nLa nueva versión había aprendido el juego sola y exclusivamente mediante un proceso de prueba y error, jugando contra sí misma, sin el conocimiento adquirido de partidas con expertos humanos. Por si fuera poco, ahora AlphaGo está aprendiendo a dominar en otros juegos de estrategia y tablero. Existe espacio para mejoras, como por ejemplo en la manera en que deep neural networks -en el domino del RL- se entrenan y se extienden. La cuestión central es ¿Qué lejos puede llegar la inteligencia artificial de la mano del Reinforcement Learning?.\nCuestiones experimentales intrigantes y la desmitificación del DL\n— por Leonardo Baldassini\nEl debate sobre el alcance, la metodología, el potencial real y las deficiencias de los sistemas de deep learning está bastante animado. Más allá de proporcionar a la comunidad científica un enriquecedor debate, las discusiones están facilitando una necesaria autocrítica y un proceso de desmitificación de la investigación en Machine Learning.\nEs necesaria una comprensión más profunda de nuestros modelos, como hemos visto en muchos trabajos este año, aprovechando herramientas que van desde la estadística a la información-teórica (y reconociendo que incluso la investigación “explicativa” no está libre de trampas metodológicas). La charla de Ali Rahimi, de Google, después de recibir el Premio de la Test of Time Award en el NIPS de este año fue un recordatorio aleccionador de que incluso las herramientas de optimización más extendidas no siempre son bien entendidas por los profesionales o investigadores del DL. Igualmente reveladores fueron los resultados del mejor trabajo en la International Conference on Learning Representations (ICLR) de este año, que demuestran que las redes neuronales muy grandes tienen la capacidad de memorizar inputs completamente aleatorios. En la misma línea, una investigación mostró que cambiar un solo píxel es suficiente para engañar a un sistema de visión basado en DL.\nPese a que los desarrollos de aplicaciones fuera del ámbito académico se someten a testing más intenso, las investigaciones prometedoras que se están realizando en el campo de las redes antagónicas apuntan a producir modelos más robustos. En un mundo en el que los vehículos sin conductor parecen estar a la vuelta de la esquina y la preocupación por las armas artificialmente inteligentes no está injustificada, la evaluación del alcance y las limitaciones reales de una metodología de investigación se convierte en primordial.\nHay una laguna en las Redes Convolucionales, y las cápsulas podrían tener la respuesta\n— por Alejandro Vidal and Juan Arévalo\nNo mucho después de revolucionar el campo de la clasificación de imágenes con Redes Neuronales Convolucionales Profundas (NIPS 2012), Geoffrey Hinton comenzó a cuestionar la naturaleza de las redes convolucionales, como se muestra en esta conferencia en el MIT de diciembre de 2014 (solo dos años después del famoso trabajo de ImageNet). En 2017, Sara Sabour, Nicholas Fross y el profesor Hinton han lanzado un nuevo paradigma: la cápsula, que proporciona una nueva abstracción para el aprendizaje de representaciones de entidades. Tales cápsulas podrían ser capaces de superar las dificultades de los ConvNets cuando se trata de adquirir la pose tridimensional de objetos (es decir, la relación entre una entidad y el espectador), y parecen ser más resistentes a los ataques antagónicos. En Medium se ha publicado una serie de artículos que explican estas cápsulas y cómo funcionan.\nEjemplo usado por Hinton y Sabour para explicar cómo la rotación afecta nuestra comprensión visual. (NYT)\nLa revolución no será supervisada, y podría haber comenzado en la traducción de idiomas\n— por César de Pablo and Juan Arévalo\nLa mayoría de las aplicaciones exitosas del aprendizaje automático utilizan el aprendizaje supervisado; sin embargo, los datos etiquetados son costosos y relativamente escasos en muchos dominios. Por lo tanto, los avances en paradigmas de aprendizaje que requieren menos supervisión siempre han capturado el interés de la comunidad de Machine Learning, incluyendo avances en el aprendizaje no supervisado de representaciones, aprendizaje semi-supervisado o activo, así como el aprendizaje transferido. En nuestro caso, donde los datos predominantemente no están etiquetados, es probable que la revolución algorítmica no esté supervisada.\nLa traducción automática es una de las aplicaciones que normalmente requiere grandes cantidades de datos etiquetados: un corpus de frases paralelas de dos idiomas (p. ej. inglés-alemán) que le gustaría aprender a traducir. Este año, dos trabajos de investigación de Artexte et al. y Lample et al., de forma independiente y casi simultánea, presentaron resultados prometedores sobre la traducción no supervisada usando arquitecturas neuronales. Ambas publicaciones utilizan una arquitectura de codificador y decodificador secuencial con atención, que comparten de manera crucial la misma palabra incrustada entre lenguajes. Pese a que requiere que las frases sean lo suficientemente similares, el uso inteligente de los datos disponibles, como la denotación y traducción inversa, o la traducción de un idioma a otro y viceversa proporciona un impulso adicional a este método.\nEn otras palabras, son capaces de producir un traductor razonablemente bueno sin un diccionario. No hay necesidad de un corpus con frases traducidas.\nLas arquitecturas neuronales se están simplificando, ya que todo lo que necesitan es atención\n— por Alberto Rubio\nCuando realizamos tareas secuenciales como traducir, nos enfocamos en las palabras y aquellas que las rodean, pero no en la oración completa de una sola vez. Este comportamiento se puede lograr mediante LSTM usando el mecanismo de atención. Para hacer esto, usamos el mismo truco que en Neural Turing Machines, donde cada token de palabra de salida del decodificador ahora depende de una combinación ponderada de todos los estados de entrada, no solo del último estado. Los puntajes se introducen en un softmax para crear la distribución de la atención.\nFigura. Ejemplo de traducción de inglés a francés.\nEste enfoque se puede utilizar con las Convolutional Neural Networks en el subtitulado de imágenes para comprender qué parte(s) de la imagen motivó una palabra determinada en el subtítulo.\n\nDesafortunadamente, tales Recurrent Neural Networks tienen problemas cuando entrenan, porque la entrada debe ser procesada secuencialmente, lo que previene la paralelización completa. Esta falta de rendimiento puede evitarse con CNNs apilados, que son altamente paralelizables. La desventaja de este enfoque es que la captura de las relaciones entre tokens lejanos requieren grandes kernels y más computación.\nTodos estos problemas se alivian en la arquitectura del Transformador – ya que aparentemente, la atención es todo lo que se necesita. Los autores consiguen una mejor puntuación y un buen rendimiento de entrenamiento utilizando una arquitectura de codificador-decodificador, donde cada parte implementa la atención de múltiples cabezales. Abordan el problema secuencial de los CNNs y RNNs eliminando la recurrencia y las convoluciones, y reemplazandolos con atención multicabezal para manejar las dependencias entre la entrada y la salida. De este modo, logra mejorar notablemente el rendimiento a la vez que elimina tanto la recurrencia como la convolución a favor de la autoatención.\nTendremos que estudiar más matemáticas, porque la naturaleza no es Euclidiana\n— por Leonardo Baldassini, Juan Duque and Juan Arévalo\nLa mayoría de las disciplinas científicas comienzan con una mezcla de experimentación y descubrimientos que impulsan la investigación, pero a menudo carecen de un marco teórico completo para explicar sus hallazgos. Por lo tanto, la evolución natural de la investigación científica es que a medida que un campo de estudio madura comienza a buscar explicaciones matemáticas rigurosas de sus hallazgos. De hecho, a pesar de la aparente simplicidad matemática detrás de las Neural Networks (NN), la hipótesis múltiple -que los datos de interés para las tareas de IA yacen en un múltiple de baja dimensión- sugiere que los datos que estamos aprendiendo podrían vivir en espacios no euclídeos bastante complejos. Además, se han propuesto varias conexiones con las propiedades observadas en Física -como simetría, localidad, composicionalidad, logaritmo polinomial o incluso el Grupo de Renormalización-. En este sentido, cabe destacar también los esfuerzos realizados por Stéphane Mallat y Joan Bruna para proporcionar una comprensión matemática de Deep Convolutional Networks.\nEn este último año, hemos sido testigos de un interés creciente por acomodar las arquitecturas NN existentes en el entorno de la geometría de Riemannian -ver por ejemplo el uso de incrustaciones de Poincaré para aprender representaciones jerárquicas. Además, un intento de generalizar (estructurar) modelos neuronales profundos a dominios no euclidianos como gráficos y múltiples, conocidos como Aprendizaje Profundo Geométrico (GDL), está cobrando impulso, como lo demuestra el tutorial de este año del NIPS. Las aplicaciones de GDL van desde ConvNets en gráficas biológicas hasta la terminación de matrices para recomendaciones. Por lo tanto, el campo es relevante para cualquier empresa que se enfrente a datos relacionales, como nosotros mismos.\nTodas estas contribuciones resaltan cómo las geometrías no euclidianas deben tenerse en cuenta e incorporarse en nuestros modelos cuando se trata de datos altamente complejos. Además, una mayor comprensión de la geometría de nuestros datos va de la mano con una intuición geométrica más profunda del trabajo de las redes neuronales. Como tal, predecimos que la geometría riemanniana jugará un papel importante en la comprensión y el desarrollo de las redes neuronales, que se han llegado a definir como “un montón de ideas juntas a un nivel de secundaria”! Para aquellos que quieran comenzar a perfeccionar algunas matemáticas que no sean de educación secundaria, recomendamos el texto introductorio Tu’s introductory text on manifolds. Es una lectura muy interesante, pero definitivamente no para la hora de dormir.\n\nLa semana próxima publicaremos una segunda entrega de las investigaciones más interesantes de 2017 en el campo de ML. Nos gustaría saber tu opinión. Envíanos recomendaciones o comentarios a hello@bbvadata.com.\n\n"
    },
    {
      "title": "Cómo Desentrañar los Efectos Cascada en Sistemas Complejos",
      "url": "https://www.bbvadata.com/es/untangle-cascade-effects-complex-systems/",
      "date": "23 enero, 2018",
      "content": "\nLas aplicaciones de la física estadística y la teoría de grafos a fenómenos sociales están ayudando a descubrir patrones de comportamiento inesperados y que permiten reaccionar de manera más efectiva a eventos desestabilizadores en redes complejas. Las nuevas capacidades en computación y disponibilidad de datos permiten su uso para adelantarse a eventos que pueden afectar al sistema en su totalidad tanto para una población expuesta a una enfermedad o empresas enfrentadas a una crisis de crédito.\n \nPredicción de epidemias en la ciudad de Cali (Colombia) a partir de flujos de movilidad empírica.\n \nHaciendo uso de los datos relacionales se puede poner en contexto eventos que de otro modo estarían obviando patrones de comportamiento y vínculos humanos especialmente relevantes cuando un sistema se enfrenta a un evento desestabilizador.\nEsto es lo que ha llevado a los investigadores españoles de la Universidad de Zaragoza, Jesús Gómez y David Soriano, y Alex Arenas, de la Universitat Rovira i Virgili, a analizar en una publicación este mes en la revista Nature Physics cómo afectan las dinámicas de desplazamiento dentro de una población a la propagación de una enfermedad. Uno de los tres supuestos analizados, teniendo en cuenta las rutinas de desplazamiento entre barrios en la ciudad colombiana de Cali, arrojó una conclusión sorprendente y contraintuitiva. Pese a que se tiende a pensar que para contener una epidemia es mejor pedir a la población quedarse en casa, la aplicación de teoría de grafos a este problema mostró que los  desplazamientos entre zonas de una ciudad, con diferentes densidades de población, pueden contribuir  a que una enfermedad quede contenida.. “Hemos identificado  un régimen mediante un modelo  de reacción-difusión en el que, de manera contraintuitiva, la movilidad va en detrimento de la extensión de la enfermedad”, señala la investigación.\nArenas junto con José Ramasco (investigador del CSIC)  colaboran con  BBVA Data & Analytics en nuevos modelos de propagación de la morosidad entre agentes económicos, en particular en redes cliente-proveedor. Actualmente el equipo investigador integrado también por los científicos de datos de BBVA Data & Analytics Pablo Fleurquin, Elena Tomás y Jordi Nin busca adaptar modelos de tipo epidémicos  para entender la propagación de estrés financiero entre empresas. Mediante esta adaptación buscan entender cómo de diferente es la difusión entre los distintos sectores económicos y dentro de los mismos, midiendo la sensibilidad sectorial y capacidad de transmisión de los impagos.   Otra línea de investigación del equipo es la utilización de modelos que tienen sus orígenes en el modelado de sistemas ecológicos del tipo predador-presa  (Generalized Lotka-Volterra models) para entender el balanceo dinámico de la liquidez de las empresas mediante transacciones económicas y cómo tensiones puntuales en la tesorería por dificultades financieras pueden generar cascadas de falta de liquidez entre empresas.\n\n“El objetivo no es tanto el predecir si una empresa va a caer en mora, sino modelar el comportamiento emergente detectando los puntos débiles del sistema para de esta forma tener un cortafuegos el dia que se repita una crisis sistémica como la vivida en 2008”, explica Fleurquin.\n\nPara conocer más sobre este tema te recomendamos:\nLos viajes cotidianos pueden reducir la incidencia de una epidemia\nCritical HexSIRSize: A contagion process near its critical point[English]\n \n \n \n\n"
    },
    {
      "title": "Lo que Vimos en Data Natives 2017",
      "url": "https://www.bbvadata.com/es/saw-data-natives-2017/",
      "date": "16 enero, 2018",
      "content": "\nEl pasado mes de noviembre asistí a la Conferencia Data Natives, en Berlin. Se definen a sí mismos como “una conferencia para la generación de los datos” y es absolutamente cierto. Conocí a un grupo muy diverso de personas: especialistas de la industria, empresarios, startups de última generación, técnicos, científicos de datos y profesores universitarios.\nDía de Workshops\n \nTuvo lugar en un emplazamiento increíble, el WeWork Sony Center. Elegí dos talleres diferentes que me parecieron muy interesantes (¡y lo fueron!):\n \n\nInteligencia Artificial para mejorar la experiencia del cliente: Un taller de design thinking donde Agnieszka Walorska, co-fundadora de Kickstart.ai, nos hizo traducir las necesidades del cliente en posibles aplicaciones de IA, de una manera práctica.\n\n\n\n \n\nAlgunas Aplicaciones Atípicas de Algoritmos Típicos de Aprendizaje Automático: Por Kirk Borne (@KirkDBorne), un astrofísico que pasó casi 20 años apoyando proyectos de la NASA. Kirk presentó una variedad de casos y aplicaciones de uso atípico (en ciencias y negocios) de algunos algoritmos típicos de aprendizaje automático de libros de texto.\n\n \n\nSi estás interesado en su presentación, puedes descargar sus diapositivas aquí:\n2 Días de Conferencia\n \nHabía más de 50 ponentes divididos en dos salas, dependiendo de la temática de su charla. Pasé la mayor parte del tiempo en el Tech Room, con ponencias sobre tecnologías Big Data, de Aprendizaje Automático, IA y sobre Internet de las Cosas.\nTrack 1: El Negocio de la Tecnología\nAsistí a una emocionante “Batalla de Startups” donde 8 startups presentaron sus increíbles proyectos. Deberías tenerlos en tu radar, estoy segura de que serán famosos más pronto que tarde.\n\nOnwrks: Una empresa de Soluciones Energéticas Inteligentes. Su presentación fue acerca de cómo han resuelto el problema de las desalineaciones de orientación en turbinas usando series temporales y Aprendizaje Profundo (o Deep Learning).\n\n\nSearoutes: Han creado el equivalente de un Google Maps para barcos utilizando Aprendizaje Automático y Teoría de Grafos, basado en rutas de navegación reales de todo el mundo e integrando información meteorológica. Tienen una API gratuita.\n\nBiQMap: Presentó su herramienta fácil de usar para crear mapas, diseñada para periodistas y editores web. ¡Porque los mapas son hermosos!\n\n\nZoe: Un grupo de jóvenes de 16 años ha creado Zoe, un chatbot para ayudar a otros adolescentes con sus asignaturas escolares. Puedes preguntarle sobre definiciones, fórmulas, ejemplos… o simplemente charlar con ella, y te contesta de una manera “adolescente”, facilitando la comunicación. ¡Las nuevas generaciones de techies vienen fuertes!\n\nTrack 2: Tendencias Tecnológicas\nAlan Akbik de Zalando Research habló sobre su trabajo en procesamiento de lenguajes naturales (NLP Processing) y Análisis de Texto Multilenguaje.\n\nTambién, Philipp Kähler de myTaxi explicó cómo funciona su modelo de predicción de tarifas de taxi. También detalló la infraestructura de datos en myTaxi y cómo se procesan y almacenan los datos, que es la base de cada aplicación de aprendizaje automático.\n\nY Dr. Fabian Werner de Lufthansa Industry Solutions habló sobre su modelo de duración de vuelo, cuyo propósito es la prevención del retraso. Su charla se centró en la optimización de hiperparámetros que están haciendo con un modelo Bayesiano de Optimización (Gaussian Processes)..\n\nTambién hubo charlas sobre temas de interés positivo (y más relacionados con el sector bancario) como Detección y Prevención de Fraudes con Aprendizaje Automático, a cargo de Ralf Klinkenberg de RapidMiner. Explicó cómo detectar patrones de fraude, anomalías y comportamientos inusuales.\n\nTodas las presentaciones están disponibles en el siguiente enlace\n"
    },
    {
      "title": "¿Qué Sabe tu Banco sobre ti?",
      "url": "https://www.bbvadata.com/es/what-does-your-bank-know-about-you/",
      "date": "9 enero, 2018",
      "content": "\nLa huella digital derivada de nuestra actividad bancaria (transferencias realizadas, movimientos de tarjetas, etc.), supone una fuente de información rica y en actualización permanente, que permite a tu Banco desarrollar productos y servicios adaptados a las necesidades específicas de cada cliente, aprovechando métodos y tecnologías de analítica avanzada. BBVA Data & Analytics nació en 2014 para extraer inteligencia de todos esos datos, contribuyendo a la transformación de BBVA en una empresa basada en datos.\nEn una reciente entrevista realizada por La Voz de Galicia a Elena Alfaro, responsable de Data Strategy & Data Science Innovation en BBVA nos da su visión acerca del futuro de la digitalización de las empresas.\n“Todas las compañías de éxito que están funcionando hoy día se basan en la personalización extrema del servicio que dan: las películas que me recomienda tal plataforma son de mis gustos, el libro que me sugieren es el que yo andaba buscando, las redes sociales me dicen a quién podría conocer, me recomiendan la música que yo prefiero… Cualquiera que se mueva en un entorno digital y que quiera competir tiene que saber que ha de moverse en un entorno así de personalizado. Es la única manera de competir.”\n— Elena Alfaro\nPuedes ver la entrevista completa aquí:\n\n\n"
    },
    {
      "title": "Las Crónicas del NIPS: Lo que vimos allí",
      "url": "https://www.bbvadata.com/es/nips-chronicles-saw/",
      "date": "3 enero, 2018",
      "content": "\nPuede que se haya convertido en una tradición: diciembre es el momento en que se muestran los frutos maduros de 12 meses de investigación global en el aprendizaje automático y se definen y perfilan las tendencias futuras en lo que parece haberse convertido en la reunión más importante -o al menos la más exagerada- sobre el tema.\nTras el creciente éxito del que ha disfrutado en los últimos años, la 31ª Conferencia sobre NIPS que se celebró este mes en Long Beach fue, por un amplio margen, la edición de mayor venta -y venta rápida-, con más de 8.000 entradas vendidas poco después de que se abriera el registro y aceptando 679 papers de los 3240 que se presentaron este año.\n\nSi bien estas cifras reflejan fuertemente que la frontera entre los simposios académicos, las convenciones industriales y los eventos de reclutamiento está más borrosa que nunca, también dan un claro testimonio del papel cada vez más central que Machine Learning, Inteligencia Artificial y las tecnologías y aplicaciones que giran en torno a ellos están destinadas a ocupar tanto en el mundo académico como en la industria. Por lo tanto, es fundamental que una organización centrada en los datos como la nuestra esté presente en tales eventos para poder seguir los desarrollos del estado de la técnica lo más cerca posible. En este post, por lo tanto, pretendemos esbozar las tendencias más relevantes que percibimos durante NIPS 2017.\nMenos es más\nQue el enorme número de parámetros que ofrecen las complejas y profundas arquitecturas de redes neuronales ayuda al poder expresivo de estos modelos no es un tema de discusión. El hecho de que esta bendición pueda convertirse en una maldición también ha sido evidente desde que el interés en el aprendizaje profundo comenzó a alcanzar su punto máximo hace algunos años. A lo largo de la semana del NIPS, quedamos gratamente impresionados por lo que nos parece un esfuerzo cada vez mayor para despojar a las arquitecturas profundas de complejidad innecesaria, explorando a través de experimentos y estudios teóricos cómo una red puede ser simplificada en gran medida conservando toda su potencia predictiva y, lo que es más, aumentando la generalización. Nos han gustado especialmente los siguientes trabajos:\n\nSobre la complejidad del aprendizaje en redes neuronales, mostrando que no todas las redes neuronales pueden dar buenos resultados (sin un entrenamiento increíblemente largo)\nNet-trim, un método que, debido a su requisito de tener que entrenar dos veces una red, puede carecer de aplicabilidad práctica en muchos casos, pero muestra de manera efectiva cómo se puede reducir en gran medida la complejidad de una red.\nRuntime neural pruning, en la que los autores proponen una sparsification de las capas de una red, esta vez basada en un proceso de decisión Markov entrenado a través del aprendizaje por refuerzo más que en la solución de un programa convexo utilizado por Net-trim.\nSVCCA: Análisis de componentes canónicos vectoriales singulares para una dinámica de aprendizaje profundo e interpretabilidad, un buen bocado, pero también una obra muy rica que fusiona las técnicas SVD y CCA para analizar la similitud de las capas de una red neuronal profunda (aquí interpretada como espacios vectoriales abarcados por las neuronas, interpretadas como vectores) y mostrar cómo la convergencia de las redes, eso es lo que podemos considerar la formación de los conceptos de clases en un problema de clasificación, por ejemplo, es lo que podemos considerar la formación de las clases. Este estudio también proporciona una base para desarrollar condiciones de parada para la formación de redes profundas.\n\nEn el mismo espíritu de la simplificación de los modelos que conservan su potencia, recomendamos encarecidamente otros trabajos para una lectura amena e instructiva:\n\nK-medoids for K-means seeding, un trabajo altamente impactante y fuertemente experimental donde el algoritmo simple de CLARANS se utiliza para mejorar la siembra del algoritmo Lloyd’s K-means.\nGeneralization properties of learning with random features, a learning-theoretic study of how a randomly sampling the weights and bias vectors for a ridge regression problem can actually yield reduced sample complexity while producing a good learnt model\nPropiedades de generalización del aprendizaje con características aleatorias, un estudio de aprendizaje teórico de cómo un muestreo aleatorio de las ponderaciones y los vectores de sesgo para un problema de regresión de la cresta puede realmente producir una menor complejidad de la muestra mientras se produce un buen modelo de aprendizaje.\nLa efectividad irrazonable de incrustaciones ortogonales aleatorias estructuradas, mostrando cómo construir fácilmente matrices aleatorias con filas ortogonales que ofrecen poderosas propiedades de reducción dimensional, entre otras.\n\nDe la alquimia a la electricidad\nUna de las charlas más sugerentes de toda la conferencia fue, sin duda, el discurso de Ali Rahimi como receptor del premio Test of Time Award, un premio que reconoce el impacto duradero del paper Características aleatorias para máquinas kernel a gran escala. Después de presentar brevemente el trabajo recogido en su publicación, Rahimi siguió abogando por un retorno al rigor en el escrutinio metodológico de la investigación del aprendizaje automático, argumentando que en lugar de considerar a la Inteligencia Artificial como la nueva electricidad, como sugirió Andrew Ng, en realidad podríamos estar tratando con una nueva alquimia: Una disciplina que lleva dentro de sí misma la semilla de una ciencia fundamental, pero que también promete convertir los metales en oro.\nEl debate sobre si estamos comercializando el rigor científico para el rendimiento de la investigación es animado y central, y apenas pasa un día sin que se publique una nueva investigación que demuestre que lo que pensamos que es verdad sobre el funcionamiento de algún modelo de Machine Learning podría ser en realidad el producto de una metodología débil. Es por eso que la poderosa intervención de Rahimi merece ser vista y meditada.\n\nLo suficientemente justo\nOtra tendencia que acogimos con gran satisfacción durante esta edición de NIPS fue un mayor impulso a la investigación sobre la imparcialidad, que generalmente pretende ser la sensibilidad injustificada de los algoritmos a algunas características. Un ejemplo clásico de un algoritmo injusto, y que nos concierne muy de cerca, es el de un sistema automatizado de aprobación hipotecaria que le ofrecería un mejor tratamiento si, todo lo demás sin cambios, tuviera un sexo, raza o código postal diferente.\n\nDe las diversas contribuciones en esta área, consideramos que los siguientes documentos son de lectura altamente recomendable:\n\nAgrupamiento justo a través de fairlets, un estudio que muestra cómo hacer cumplir la imparcialidad como una restricción dura en los problemas de agrupamiento a través de un paso de preprocesamiento que divide los datos en regiones donde la imparcialidad no puede ser fácilmente violada.\nSobre la imparcialidad y la calibración, mostrando una incompatibilidad inherente entre la exactitud de la tasa de error y la calibración. Esto quiere decir, por ejemplo, que un modelo predictivo que quiere alcanzar las mismas tasas de error falso-positivo y falso-negativo en dos subconjuntos de puntos no tendrá, en general, sus probabilidades de predicción que reflejen las probabilidades reales de que algo suceda.\nEn lugar de esto, se ofrece una visión un poco más brillante por De paridad a nociones basadas en preferencias de equidad en la clasificación, un trabajo interesante que muestra que la idea habitual de equidad basada en la paridad puede ser demasiado estricta para lograr una buena precisión de clasificación. Específicamente, el documento muestra que se puede lograr una precisión mucho mejor al requerir que ningún individuo prefiera estar en un grupo que no sea el actual, en lugar de exigir que todos los grupos reciban necesariamente el mismo tratamiento.\n\nEstos documentos sólo muestran una fracción de los problemas que surgen cuando algunos conceptos de justicia necesitan ser incluidos en un algoritmo de aprendizaje automático, y una parte aún más pequeña de las técnicas que se pueden utilizar para abordarlos. Aunque relativamente nueva, esta línea de investigación es prometedora y sin duda muy necesaria, no sólo para los modelos mejorados que pueden salir de ella, sino también porque, como señaló Kate Crawford en su presentación, la cual debe ver, a través de la lente de modelos injustos podemos darnos cuenta de lo que está mal en nuestra sociedad, y tratar de arreglarlo.\n\nCompraré esa Inteligencia Artificial\nSi bien el espíritu de la conferencia sobre NIPS sigue siendo principalmente académico, es innegable la presencia cada vez más fuerte de la industria, tanto a través de los muchos patrocinios como de las contribuciones a la investigación. Pasamos algún tiempo hablando con algunos de los patrocinadores del evento y nos dimos cuenta de que tanto las empresas como los fondos de capital riesgo están dispuestos a apostar por una mercantilización de la IA. La creencia común es que sólo hay una cantidad limitada de talento relacionado con la IA, y la mayoría de las empresas preferirían ser consumidores de soluciones de IA ya hechas que luchar con los detalles sangrientos de los algoritmos y el procesamiento de datos. Es entonces verosímil que en los próximos años seremos testigos de una multiplicación de proveedores de productos de IA o, más probablemente, de servicios.\nPor otra parte, para las empresas que desean crear soluciones de IA, ya sea para sí mismas o para otras, NIPS también está registrando una creciente presencia de proveedores de hardware que no se detienen con los sospechosos habituales Nvidia e Intel, sino que también incluyen ofertas como las máquinas preconfiguradas listas para usar de Lambda Lab, que funcionan a pleno rendimiento, así como nuevos e interesantes jugadores como Graphcore, una startup británica que promete una unidad de procesamiento de inteligencia de IPU, que es un procesador específicamente diseñado para aplicaciones masivamente paralelas de inteligencia artificial (esperando TPU, ¿alguien?).\nYa sea entre las universidades, los grandes internautas, las nuevas empresas visionarias, las plataformas de comercio algorítmico, las empresas automovilísticas o las empresas de consultoría, está más claro que nunca que la carrera para dar forma al futuro de lo que promete ser una enorme (r)evolución tecnológica está en pleno funcionamiento. Y sin embargo, como parece sugerir la investigación que discutimos aquí, es posible que sólo estemos empezando.\n\n"
    },
    {
      "title": "Primero, Miramos Hacia Atrás. Luego Hacia Adelante",
      "url": "https://www.bbvadata.com/es/first-look-back-look-forward/",
      "date": "26 diciembre, 2017",
      "content": "\nA medida que finaliza 2017, es el momento ideal para elaborar una lista de los logros alcanzados durante este período y, por supuesto, discutir los nuevos retos que se avecinan. Nuestros co-CEOs Fabien Girardin y Jon Ander Beracoechea acaban de cerrar la revisión de nuestros objetivos anuales destacando los esfuerzos y sus métricas para generar impacto empresarial, crear ventajas competitivas para BBVA y nutrir un entorno de trabajo sobresaliente para la ciencia de datos. Dejamos este año satisfechos, con innumerables horas de arduo trabajo y grandes momentos, algunos de ellos cuantificados en los hechos divertidos de abajo, como hicimos el año pasado.\n \n\n¡Nos vemos en 2018!\n\n"
    },
    {
      "title": "Lo que vimos en la Conferencia Open Data Science Europe 2017",
      "url": "https://www.bbvadata.com/es/saw-open-data-science-conference-europe-2017/",
      "date": "19 diciembre, 2017",
      "content": "\nODSC Londres 2017 nos mostró una asombrosa variedad de herramientas, librerías, notebooks y aplicaciones de ciencia de datos. Hubo 75 ponentes, más de 1.500 asistentes y literalmente no hubo espacio para más. No importa si prefieres Python, R o incluso Julia. Ya sea que estés interesado en visualización de datos, investigación académica o librerías Machine Learning de código abierto. Allí encontramos los temas calientes habituales, como el aprendizaje profundo (deep learning), y uno de los más difíciles de encontrar en una conferencia de código abierto: Finanzas Cuantitativas.\nHubo varios itinerarios, dependiendo del tipo de entrada que compraras: talleres, la llamada Accelerate AI Conference y las conferencias/charlas. Asistimos a los talleres y a la conferencia, y nos saltamos la conferencia de Accelerate AI.\nLos talleres fueron sesiones prácticas de larga duración, principalmente a nivel introductorio, sobre diferentes temas de ciencia de datos. Nos las arreglamos para asistir al Taller de Python Quants sobre comercio algorítmico con Python, usando Pandas, y otro taller sobre la ejecución de Tensorflow en Google Cloud.\nLa conferencia se centró en seis temas: Ciencia de Datos Abierta, Machine Learning, Quant Finance, Visualización, Investigación en Ciencia de Datos y Kickstarter. La frontera entre algunos de ellos era un poco borrosa como puedes imaginarte En relación con Machine Learning, el bombo en el aprendizaje profundo hizo que varios oradores comenzaran sus presentaciones presentando su elemento básico: el perceptrón. Fue agradable para los asistentes a nivel de entrada, pero creemos que la organización debe fomentar cierta coordinación para evitar repeticiones.\nEl resto de la conferencia versó sobre temas más avanzados y temas de actualidad en el mundo de la ciencia de datos.\nNotas clave\nLa ponencia de Neil Lawrence señaló la gran necesidad de lo que él llamó “profesionalismo de datos”. Su excelente reflexión nos reconfortó mostrando que estas cuestiones no han pasado desapercibidas para muchas empresas e instituciones. Pero sigue siendo un problema sin resolver.\n¿Qué sucede si uno de los científicos de datos de su organización se va o, en sus propias palabras, es atropellado por un autobús en la calle? La respuesta es que asumir su trabajo requiere un gran esfuerzo en términos de adquisición de talento, formación, etc.\nIncluso si se contrata a un nuevo gran científico de datos, el proceso de adaptación y las curvas de aprendizaje suponen un drenaje de recursos, debido a la falta de estandarización y prácticas comunes. Nosotros somos conscientes de este coste, y hemos puesto en marcha proyectos específicos que pretenden facilitar este camino lo antes posible.\nProgramación Probabilística con PyMC3\nNos alegramos de descubrir que estamos utilizando esta librería específica en uno de los proyectos que estamos desarrollando en BBVA Data & Analytics. Lo hemos utilizado en algunos análisis donde queremos inferir la distribución de probabilidad posterior completa a través de métodos de muestreo, como el MCMC.\nThomas Wiecki es uno de los principales colaboradores de Pymc3. En esta librería de Python solo tienes que especificar la formulación del modelo bayesiano y, bajo el capó, la librería construye un gráfico de cálculo en Theano antes de aplicar simulaciones MonteCarlo. Aquí hay algunos enlaces sobre esta charla:\n\nhttps://www.youtube.com/playlist?list=PL1Ma_1DBbE82OVW8Fz_6Ts1oOeyOAiovy \nhttps://twiecki.github.io\n\nDonde el Comercio Algorítmico se encuentra con el Código Abierto\nAntes de conocer Quantopian, Python Quants GmbH o a algunos ponentes de ODSC, el comercio algorítmico nos parecía como magia negra. Hablar sobre los recursos de código abierto en este campo fue algo inesperado y muy bienvenido.\nPor ejemplo, Quantopian es una plataforma basada en una comunidad comercial algorítmica donde la gente escribe código para estrategias de inversión. La plataforma comprueba el rendimiento de estas estrategias con una evaluación de backtesting. Además de eso, los mejores algoritmos en la comunidad son elegibles para recibir dinero de los fondos de cobertura. Como recompensa, el autor recibe una comisión por los beneficios generados por sus estrategias.\nEsta conferencia fue un buen punto de partida para las personas que quieren aprender a aplicar Ciencia de Datos a Quant Finance.\nDescubra más sobre esta charla y el comercio algorítmico en:\n\nhttps://github.com/AlexandruAgachi/ODSC2017\nhttps://gist.github.com/yhilpisch/35a18382cfdd850c55fe971ae54cf752\nhttps://www.quantopian.com/lectures\n\nCómo ganar competiciones Kaggle: Stacking Made Easy\nEl tema de esta charla fue un trabajo ordenado de uno de los grandes maestros de la comunidad de Kaggle. Básicamente, Stacking es un paso de meta-aprendizaje sobre varios modelos de aprendizaje automático para construir un conjunto de forma inteligente. En primer lugar, se utilizan clasificadores de base múltiple para predecir o clasificar en un problema supervisado. En segundo lugar, un nuevo modelo de aprendizaje combina sus predicciones utilizando el resultado de los modelos básicos como insumo para formar a un modelo de nivel superior creando un conjunto apilado.\nStackNet es una estructura de meta-modelado que se puede utilizar fácilmente para aumentar la precisión de su algoritmo encontrando los pesos óptimos para cada modelo de aprendizaje de base. Soporta algunos de los marcos de aprendizaje de máquinas más populares: scikit-learning, xgboost, h20, Keras, etc.\nMarios Michailidis explicó en detalle cómo funciona esta red de apilamiento generalizada:\n“La mayoría de los algoritmos se basan en ciertos parámetros o suposiciones para rendir mejor, por lo tanto cada uno tiene ventajas y desventajas. El apilamiento es un mecanismo que intenta aprovechar los beneficios de cada algoritmo sin tener en cuenta (en cierta medida) o corrigiendo sus desventajas. En su forma más abstracta, el apilamiento puede verse como un mecanismo que corrige los errores de sus algoritmos .”\nStacking Made Easy: An Introduction to StackNet by Competitions Grandmaster Marios Michailidis (KazAnova)\n\nEn otra charla, Piotr Migdał nos dio algunos trucos y consejos sobre la reproducibilidad y una serie de buenas prácticas sobre la cooperación en equipo y el despliegue de modelos. Es importante tener en cuenta que incluso un solo ajuste de parámetros sin seguimiento puede llevar a la frustración e ineficiencias en todo el equipo. No puedo resistirme a mencionar su buen humor con el paralelismo entre la pintura del Ecce Homo de Borja y los temas de reproducibilidad de modelos.\nBEHIND THE SCENES OF TRAINING, MANAGING AND DEPLOYING MODELS\n\nManténte al tanto del blog de ODSC para encontrar más:\nVolvimos a Madrid con la mochila llena de ideas, nuevas herramientas para probar y esperando la próxima edición de esta conferencia en Europa. Mientras tanto, saciamos nuestro apetito por más con el blog de ODSC, siempre lleno de artículos interesantes:\nhttps://opendatascience.com/blog/\n\n"
    },
    {
      "title": "Lo que vimos en Spark Summit Europe 2017: Más Librerías Spark, Sistemas Reales de Aprendizaje de Máquinas en Producción, y la Realidad del Aprendizaje Profundo en Spark",
      "url": "https://www.bbvadata.com/es/saw-spark-summit-europe-2017-spark-libraries-real-machine-learning-systems-production-reality-deep-learning-spark/",
      "date": "12 diciembre, 2017",
      "content": "\nHace un mes tuvo lugar el último Spark Summit Europe, el evento más grande del mundo para la Comunidad Apache Spark™, en el que participaron más de 1200 desarrolladores, ingenieros, científicos de datos, investigadores y profesionales de negocio.\nJosé A. Rodríguez, uno de nuestros científicos de datos, tuvo la oportunidad de participar como ponente con una charla en la que explicó un caso de uso de Spark en el sector bancario. La charla, que ha sido un trabajo conjunto con Luis Peinado bajo el apoyo e interacciones continuas con el equipo de Modelos Predictivos, destaca algunos de los productos BBVA existentes que se basan en Spark, y se centra en cómo hemos experimentado con un clasificador de texto para transferencias bancarias basado en Spark.\nEste clasificador ha hecho posible que, en BBVA, cada transferencia de dinero que realiza un cliente pase por un motor que deduce una categoría de su descripción textual. Este motor funciona en Spark, mezcla MLLib e implementaciones propias, y actualmente está en producción sirviendo a más de 5 millones de clientes diarios. A lo largo de la presentación, José compartió experiencias y lecciones aprendidas por el equipo desde el punto de vista de la ciencia de datos, incluyendo los desafíos que esto representó en términos de ciencia de datos, algunos bocetos de la solución actual y una prueba experimental usando las conocidas incrustaciones word2vec y una técnica llamada VLAD (vector de descriptores agregados localmente, inspirados en la visión por computadora).\nEl evento contó con más de 70 sesiones, con contenido para todos los niveles y roles, y a continuación destacamos lo que nos resultó más relevante:\nLibrerías\nA medida que aumenta la adopción de Spark y las API principales de Spark se vuelven estables, la tarea de la comunidad es empezar a poblar el ecosistema de librerías. A continuación se presentan algunos ejemplos de charlas que proponen y comparten librerías para las características que faltan.\n\nBuilding Custom Machine Learning PipelineStages for Feature Selection (BMW)\nSparkNLP: Natural Language Understanding at Scale with Spark-Native NLP, Spark ML, and TensorFlow (Indeed)\nMML-Spark (Microsoft): A machine learning library by Microsoft.\n\nProductización\nLa puesta en producción de los motores de aprendizaje de máquinas implica retos que van más allá del diseño del algoritmo o de su implementación: cómo garantizar la reproducibilidad, cómo programar de forma óptima los experimentos, cómo automatizar la selección de modelos. Incluso cómo organizar equipos de científicos e ingenieros de datos. Vimos muchos bocetos de esto en varias presentaciones. Aquí está nuestra selección:\n\nProductionizing Behavioural Features for Machine Learning with Apache Spark Streaming (Booking.com)\nSpline: Apache Spark Lineage, Not Only for the Banking Industry (Barclays) [github]\nNo More Cumbersomeness: Automatic Predictive Modeling on Apache Spark (NEC / 9Lives)\n\nBuenas prácticas en Machine Learning\nComo Spark es joven, y MLLib tiene un conjunto limitado de algoritmos, estos días no es inusual implementar sus propios algoritmos de Machine Learning. Si lo hace, las siguientes presentaciones ofrecen algunas buenas ideas a seguir.\n\nBuilding Machine Learning Algorithms on Apache Spark (Red Hat)\nLessons Learned while Implementing a Sparse Logistic Regression Algorithm in Apache Spark (Zalando) Code\nMatFast: In-Memory Distributed Matrix Computation Processing and Optimization Based on Spark SQL (Hortonworks) – Library\n\nLa primera presentación de William Benton, Científico de Datos en Red Hat, nos dio algunos consejos para construir algoritmos de Machine Learning en Apache Spark. Por ejemplo, un reto común es cómo distribuir el algoritmo de entrenamiento, especialmente cuando los algoritmos de aprendizaje de la máquina necesitan realizar varias iteraciones sobre el conjunto de datos. Para esos casos, una receta general con la que empezar es: mantener el bucle de iteración, vectorizar el código dentro de cada iteración, y utilizar las funciones aggregate y treeAggregate siempre que sea posible.\nDeep Learning\nComo destacamos en el post de la edición global de Spark Summit, Spark comienza a abrazar el aprendizaje profundo, con los Oleoductos de Aprendizaje Profundo, así como las herramientas de jugadores como Intel, Yahoo o Microsoft. La siguiente charla compara diferentes maneras de establecer Deep Learning and Spark.\n\nApache Spark-and-Tensorflow-as-a-Service (KTH)\n\n \nTe invitamos a que consultes nuestra presentación y leas la publicación sobre la versión internacional de esta conferencia en San Francisco, y los puntos destacados que vimos allí.\n \n \n \n\n"
    },
    {
      "title": "Lecturas Recomendadas sobre Ciencia de Datos",
      "url": "https://www.bbvadata.com/es/recommended-readings-data-science/",
      "date": "28 noviembre, 2017",
      "content": "\nEl viaje que ha llevado a los datos y sus aplicaciones desde la  periferia hasta el centro del debate actual tiene múltiples ramificaciones: cuestiones sociales, éticas y legales sobre las oportunidades y los riesgos de la “datificación” de nuestra sociedad, aspectos de negocio en torno al reto que supone transformar los datos en soluciones innovadoras percibidas como un nuevo valor por los clientes, y aspectos técnicos y metodológicos en torno a la profesión en boga: la ciencia de datos. Desde BBVA Data & Analytics hacemos esfuerzos divulgativos en todos esos frentes, y en esta ocasión compartimos una selección de referencias bibliográficas orientadas a facilitar objetivos diferentes en audiencias diversas. \nEn los bloques A y D el público objetivo de las publicaciones sería muy amplio: project managers, perfiles de diseño y de desarrollo de negocio, o simplemente todo el que esté interesado en aprender y tener unos fundamentos elementales sobre la disciplina, sus posibles aplicaciones y los dilemas implícitos.\nEn los bloques B y C sin embargo apuntamos a otro público: el científico de datos “practicante” que busca resolver dudas metodológicas y profundizar en las diversas especializaciones de la disciplina:\n \nA] Aproximación a la ciencia de datos y sus aplicaciones de negocio: introducción a la estadística, sin necesidad de conocer códigos de programación, pero sí ciertas nociones previas matemáticas para proponer aplicaciones con rigor: definiciones, catálogo de posibilidades, riesgos técnicos relativos al tratamiento de sesgos, calidad del dato, métricas de precisión y de exactitud.\n\nStatistics Done Wrong. Alex Reinhart\nHow to Lie with Statistics. Darrell Huff & Irving Geis\nData Science for Business: What you need to know about data mining and data-analytic thinking. Foster Provost\nThe Signal and the Noise: Why So Many Predictions Fail-but Some Don’t. Nate Silver\nEverything Is Obvious: How Common Sense Fails Us. Duncan J. Watts\nBig Data: A Revolution That Will Transform How We Live, Work, and Think. Viktor Mayer-Schönberger & Kenneth Cukier\n\n \nB] Profundización en ciencia de datos; analítica, arquitectura de datos y programación:\n\nToda la colección O’Reilly, destacando los siguientes:\n\nPractical Statistics for Data Scientists: 50 Essential Concepts. Peter Bruce & Andrew Bruce\nAdvanced Analytics with Spark: Patterns for Learning from Data at Scale. Sandy Ryza & Uri Laserson\nPython for Data Analysis. Wes McKinney\n\n\nDoing Bayesian Data Analysis. John K. Kruschke\nTransparent Data Mining for Big and Small Data. Tania Cerquitelli, Danielle Quercia & Frank Pasquale.\nThe R Book. Michael J. Crawley\nThe Data Warehouse Toolkit: The definitive guide to dimensional modeling. Ralph Kimball\nDisplaying Time Series, Spatial, and Space-Time Data with R. Oscar Perpinan Lamigueiro\nComputer Age Statistical Inference. Algorithms, Evidence and data Science. Bradley Efron & Trevor Hastie.\nIntroduction to Linear Algebra. Gilbert Strang\nApplied Predictive Modelling. Max Kuhn & Kjell Johnson\nMining Massive Datasets. Jeffrey Ullman\n\n \nC]  Profundización en ciencia de datos; estadística, machine learning e inteligencia artificial:\n\nThe Elements of Statistical Learning. Trevor Hastie, Robert Tibshirani and  JH Friedman\nStatistical Rethinking: A Bayesian Course with Examples in R and Stan. Richard McElreath\nNetwork Science. Albert-László Barabási & Márton Pósfai\nArtificial Intelligence. Peter Norvig & Stuart J. Russell\nMachine Learning. A Probabilistic Perspective. Kevin P. Murphy\nPattern Recognition and Machine Learning. Christopher Bishop\nNetworks. An Introduction. M.E.J. Newman\nUnsupervised Learning Algorithms. M. Emre Celebi & Kemal Aydin\nDeep Learning (Adaptive Computation and Machine Learning series). Ian Goodfellow and Yoshua Bengio and Aaron Courville\nReinforcement Learning: an introduction. Richard S. Sutton, Andrew Barto\nLearning From Data. Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin\n\n \nD] Dilemas éticos y visiones de futuro en torno a la inteligencia artificial: riesgos morales, discriminación injusta, control centralizado de la sociedad, reemplazo del hombre por la máquina, (discurso de Nick Bostrom, Elon Musk o Stephen Hawking), pero también su contrapunto más visionario y tecno-optimista (Ray Kurzweil, Grady Booch, etc)\n\nAlgorithms to Live By: The Computer Science of Human Decisions. Brian Christian & Tom Griffiths\nWeapons of Math Destruction. Cathy O’Neil\nData and Goliath: The Hidden Battles to Collect Your Data and Control Your World. Bruce Schneier\nHomo Deus. Yuval Noah Harari\nNaked Diplomacy. Understanding Power and Politics in the Digital Age. Tom Fletcher\nLife 3.0: Being Human in the Age of Artificial Intelligence. Max Tegmark\nSuperintelligence: Paths, Dangers, Strategies. Nick Bostrom\nWho Owns The Future? Lanier, Jaron\nMachines Of Loving Grace. John Markoff\nHow to Create a Mind: The Secret of Human Thought Revealed. Ray Kurzweil\n\n \n \n\n"
    },
    {
      "title": "Lo que vimos en ECML & PKDD 2017",
      "url": "https://www.bbvadata.com/es/saw-ecml-pkdd-2017/",
      "date": "22 noviembre, 2017",
      "content": "\nEl pasado septiembre tuve el placer de asistir a la European Conference on Machine Learning (ECML), que se ha venido celebrando simultáneamente junto con la conferencia Principles and Practice of Knowledge Discovery in Databases (PKDD) durante los últimos 13 años.\nEn esta edición, la conferencia se celebraba en Skopje, donde nos recibieron con una hospitalidad admirable, dícese típica de los Balcanes. La capital de Macedonia es una ciudad vívida, moderna y con una gran oferta cultural en su zona céntrica, mientras que es extremadamente tranquila y apacible en cuanto te alejas unas cuantas cuadras (como bien refleja la fotografía). Está situada en un paraje maravilloso, rodeada de verdes colinas en todo su perímetro.\n\nAunque la conferencia es europea (tiene otra edición internacional), muchos de los asistentes vienen desde todas partes del mundo. Por ello tuve la oportunidad de conocer gente de Francia, Reino Unido, Rusia, Estados Unidos, Corea.. y de compartir el conocimiento y el tiempo libre con ellos. ECML es una conferencia grande (400-500 asistentes), que tiene 5 sesiones simultáneas; algunas de ellas con un público de 30-40 personas, lo que en mi opinión diluye un poco el nivel de algunas de las charlas. Por ello, es bastante útil poder compartir las charlas más interesantes con personas que asisten a diferentes sesiones.\nLo más destacado de ECML-PKDD 2017\n \nJohn Quackenbush, que entre otros lugares trabaja en la Harvard T.H. Chan School of Public Health y es un importante colaborador del Proyecto del Genoma Humano, presentó su trabajo en el papel que la expresión genética tiene en las enfermedades. \n\nSi realmente te apetece emocionarte con la charla de este gran orador y excepcional físico, aquí puedes seguir la hora de charla que, más o menos en su totalidad, supuso su contribución como charla plenaria a la conferencia. En ella podrás viajar desde el pequeño mundo de una proteína al intrincado universo de los sistemas y redes complejas y de su aplicación al conocimiento de qué grupos de genes y de qué manera están relacionados con funciones biológicas y enfermedades. \n\nCordelia Schmid, una experta en visión artificial que trabaja en el INRIA, Francia, presentó “Automatic Understanding of the Visual World”, una charla plenaria donde mostró las mejoras a las que su grupo ha llegado en la vídeo-segmentación de objetos usando un módulo de memoria auxiliar para estudiar la evolución de los objetos a lo largo del tiempo a partir de redes neuronales. Cordelia impresionó visualmente a la audiencia cuando enseñó el conjunto de datos SURREAL (synthetic hUmans forR REAL tasks ) que ha construido para mejorar la segmentación de imágenes de vídeo.\n\nEn el tiempo de preguntas,  expuso que en su opinión, el mayor reto al que se enfrentan las redes neuronales es el del olvido catastrófico, esto es, la tendencia de las redes neurales a olvidar todo lo aprendido cuando se disponen a aprender nueva información.\nTambién Frank Hutter presentó una extraordinaria charla plenaria titulada   “Towards end-to-end lLarning & Optimization”, donde resumió su trabajo (centrado en el aprendizaje automático o AutoML) y remarcó la importancia de entender la influencia de los hiper-parámetros en los modelos. Primero introdujo su trabajo AutoWeka, para luego seguir con auto-sklearn y extensiones que usan optimización bayesiana para encontrar los hiper-parámetros. Podéis empezar a probarlo en cuanto queráis, porque, como enfatizó, “todo está en github”. Para finalizar presentó auto-Nets: “towards automatically-tuned neural networks” (aquí están los detalles de sus transparencias) y cómo acelerar la optimización de hiper-parámetros para diferentes arquitecturas de redes neuronales y diferentes optimizadores. Y por supuesto, todo eso, como dijo, ¡también está en github!  De hecho, hubo un tutorial y un workshop de un día completo dedicado a AutoML que reunió una audiencia considerable; más específicamente dedicada a meta-aprendizaje, selección del algoritmo más adecuado, y configuración del mismo. Si estáis interesados aquí podéis echar un vistazo a las transparencias tutorial y a los procedimientos del  workshop.\nAlex Graves , de DeepMind, presentó una charla plenaria técnica centrada en redes neuronales recurrentes. En particular, en el reto que resulta al intentar escalar las RNN en espacio y tiempo, esto es, a medida que la memoria crece. Presentó su artículo sobre SAM’s (sparse access memory) para solventar estos problemas, y un algoritmo de tiempo de cálculo adaptativo y su aplicación al conjunto de datos de la Wikipedia. Mostró cómo el algoritmo podía distinguir estructura de ruido cuando otras métricas no pueden. Finalizó hablando de gradientes sintéticos y otros algoritmos como curriculum learning que utilizan para acelerar el aprendizaje.\nEl premio al mejor artículo de estudiantes se otorgó a “Arbitrated Ensemble for Time Series Forecasting”, un método de ensamblaje adaptativo para tratar las diferentes dinámicas de los datos de las series temporales, descomponiendo la predicción en una predicción de base y varios meta-predictores. Si estáis interesados, tengo una buena noticia: hay un paquete R llamado tsensembler listo para usar.\nHubo una presentación que despertó gran interés en el público por la novedad y sencillez de la idea. La idea de FCNN, Fourier Convolutional Neural Networks,  consiste en entrenar por completo una CNN en el espacio de Fourier, lo que da lugar a mejoras en velocidad sin pérdida alguna de efectividad. Se implementó usando una NVidia K40c GPU de 12GB RAM + Keras. \nPero sin lugar a dudas, una de mis presentaciones favoritas de toda la semana fue la presentada en la sesión Learning and Optimization por Alon Zweig y titulada “Group Online Adaptive Learning (GOAL)”.  Aquí, el autor introdujo un método donde existen múltiples agentes de aprendizaje, cada uno de ellos en su propio entorno, que comparten información entre ellos para acelerar el aprendizaje global. Este método es válido para muchas aplicaciones: reconocimiento de objetos, finanzas y sistemas de recomendación, entre otros. Los autores señalaron que el uso compartido de todos los datos de todos los agentes de aprendizajes no escala; por lo tanto, su modelo se limita a compartir los datos de los expertos, mientras que los expertos se añaden o retiran según lo que han aportado, y otros agentes pueden utilizar expertos que han sido ya previamente congelados. El algoritmo se llama SOAL, y su funcionamiento fue mostrado en una tarea de navegación visual donde un robot aprende a navegar basado en 6 escenas de video al aire libre. SOAL demostró con creces mejorar notablemente el aprendizaje cuando el conocimiento de otros robots en escenas relacionadas estaba disponible y era compartido.\nEn el campo de las redes, pudimos ver una interesante charla sobre “Activity-Driven Influence Maximization in Social Networks” de Rohit Kumar aplicada a encontrar ubicaciones influentes. Para ello, en lugar de estudiar a los usuarios con el mayor conjunto de relaciones o el conjunto de lugares visitados por la mayoría de los usuarios, identificaban los patrones de propagación de la influencia, y lo hacían de manera escalable.\nOtros tutoriales y Workshops interesantes\n \nInteractive Adaptive Learning: Aquí podéis encontrar un interesante documento introductorio a la Inteligencia Artificial (“Challenges of Reliable, Realistic and Comparable Active Learning Evaluation”) y el resto de las ponencias presentadas en el workshop, donde pudimos encontrar métodos para optimizar todo el proceso de aprendizaje, desde la interacción con supervisores humanos hasta técnicas de aprendizaje adaptativo o de transfer learning.\nRedes: Hubo un workshop dedicado a las redes temporales y su desarrollo a gran escala y un tutorial en el que se hizo hincapié en el análisis y la descomposición del k-core de las redes para estudiar sus propiedades.\nScaling-Up Reinforcement Learning: Este workshop reunió a investigadores que están trabajando en diferentes métodos para mejorar el aprendizaje por refuerzo.\nMIDAS “MIning DAta for financial applicationS”: Los procedimientos del workshop muestran algunos de los retos y aplicaciones de la combinación de datos financieros y machine learning.\nLearning with Imbalanced Domains: Theory and Applications: El workshop se enfocó en el problema de los datos desbalanceados, problema muy común en la ciencia de datos.\n \n \nEsto es todo. Espero que disfrutéis de las charlas y del material de aprendizaje tanto como lo hice yo. ¡Nos vemos en Dublin en 2018!\n \n \n\n"
    },
    {
      "title": "Transformación Digital y Big Data en las Empresas",
      "url": "https://www.bbvadata.com/es/digital-transformation-big-data-companies/",
      "date": "14 noviembre, 2017",
      "content": "\nDurante el pasado Simposio Internacional de Innovación Aplicada IMAT, celebrado del 28 al 30 de junio en Valencia, nuestro compañero Josep Amorós, responsable de Commerce360, participó como ponente con una charla dedicada a la Transformación Digital y el Big Data en las empresas. Con motivo de su intervención le realizaron una entrevista en relación con este tema.\nPodéis escuchar la entrevista completa a continuación:\n \n\n\n"
    },
    {
      "title": "¿Puede la Analítica de Personas Transformar la Gestión?",
      "url": "https://www.bbvadata.com/es/can-people-analytics-transform-management/",
      "date": "7 noviembre, 2017",
      "content": "\nEste artículo es un breve resumen del contenido del curso “People Analytics: Transforming Management with Behavioral Data” (Analítica de Personas: Transformando la gestión con Datos de Comportamiento), impartido por Ben Waber y Sandy Pentland en el MIT Media Lab los días 24 y 25 de julio de 2017. El contenido del artículo se basa en su explicación y referencias bibliográficas.\nAntecedentes\nLa Análitica de Personas es una disciplina de la Física Social que tiene como objetivo entender el comportamiento humano a través de datos frecuentemente utilizados para el desarrollo organizacional. Es una disciplina centrada en el ser humano, diferente al Análisis de Recursos Humanos, cuyo objetivo final está más centrado en el desempeño del empleado.\nAprovechando una variedad de fuentes de datos, la Análitica de Personas se utiliza como un método multidisciplinario de gestión fundacional, basado en el método científico, para generar y probar hipótesis, y resolver problemas empresariales. Contrariamente al desarrollo organizacional tradicional, se basa en los datos para hacer observaciones, pensar en preguntas interesantes, formular hipótesis, desarrollar predicciones comprobables, recopilar datos para probar predicciones, refinar, alterar, expandir o rechazar hipótesis y desarrollar teorías generales sobre el futuro de la organización.\nMarco Analítico y Fuentes de Datos\nLa Analítica de Personas aprovecha una gran variedad de fuentes de datos, tales como los datos de las instalaciones (HVAC, uso de ascensores, consumo de energía, sensores de los asientos), datos de personas (insignias de sensores, placas RFID, prendas de vestir para el consumidor, teléfonos móviles, rastreo Wi-Fi); datos de comportamiento (demografía, equipo, tenencia, rendimiento, conocimiento contextual cualitativo); y datos digitales (correo electrónico, calendario, chat, software de gestión de proyectos, teléfonos, uso de ordenadores).\nConfiando en el análisis de las redes sociales, la Analítica de Personas tiene como objetivo descubrir cómo las personas están conectadas entre sí y cómo esa interacción influye en su productividad como empresa. Las métricas locales miden la cohesión (qué tan estrecha es la red), la visibilidad (número de participantes únicos en cada interacción) y la exploración (cuánto interactúan los miembros de un equipo con otros equipos). Las métricas globales incluyen la agrupación de clústeres (cohesión general de la red); centralidad (cómo se conectan entre sí los nodos de la red) y diámetro (distancia entre nodos).\nPara resolver los problemas de privacidad, la Analítica de Personas se puede aplicar mejor bajo marcos optativos que sean claros, concisos y fáciles de entender, y con métodos de agregación complejos, como hashing and salting, que aseguren que no se compartan datos individualizados.\nResultados\nUn estudio de Call Centers demostró que la cohesión entre los empleados predecía la productividad y que facilitar al empleado la interacción durante las pausas de descanso y las comidas podría reducir el estrés y mejorar el nivel de compromiso.\nUn estudio sobre los bancos minoristas que analizó cómo los empleados se comunicaban entre sí, demostró que las políticas activas de compromiso de los empleados mejoraban la interacción entre ellos y podían aumentar la productividad en un 11%.\nEl caso de la política de trabajo a distancia de Yahoo, demostró que los empleados con ubicaciones compartidas podrían incrementar la comunicación entre los empleados y aumentar en $150 millones al año los ingresos totales de la empresa.\nEl caso de EToro destaca el valor del aprendizaje social y cómo las comunidades cerradas y aisladas generan pérdidas y representan un riesgo.\nLecturas adicionales\n\nPentland, Sandy. Social Physics\nWaber, Ben. People Analytics\nWatts, Duncan. “Small Worlds: The Dynamics of Networks between Order and Randomness.” Princeton University Press, 2003.\n\n \n \n \n\n"
    },
    {
      "title": "Mejorar la Tasa de Respuesta de los No-Clientes a las Campañas con el Algoritmo SMOTE",
      "url": "https://www.bbvadata.com/es/improving-response-rate-non-customer-campaigns-smote-algorithm/",
      "date": "31 octubre, 2017",
      "content": "\nAntecedentes\nAtraer nuevos clientes es un reto para cualquier empresa, y la tasa de respuesta a las campañas de marketing es generalmente baja entre los no-clientes. Los algoritmos tradicionales de aprendizaje automático, como la Regresión Logística, pueden clasificar a los clientes potenciales y predecir el resultado de las campañas de marketing, pero su precisión predictiva es limitada cuando los datos están desbalanceados (Chawla, Bowyer, Hall & Kegelmeyer, 2002).\nLas respuestas a las campañas comerciales son conjuntos de datos desbalanceados que generalmente incluyen una pequeña fracción de respuestas positivas y una gran fracción de respuestas negativas. En este contexto, nuevos métodos, como SMOTE (Synthetic Minority Oversampling Technique), han demostrado un mejor desempeño en las muestras de prueba con Area Under the Curve (AUC) que otros métodos simples como la Regresión Logística.\nMarco Analítico y Fuentes de Datos\nSegún lo definido por Chawla et al. SMOTE es “un enfoque de sobremuestreo en el que la clase minoritaria es sobremuestreada creando ejemplos sintéticos en lugar de sobremuestrear con sustituciones[…] que ayuda al clasificador a crear regiones de decisión más grandes y menos específicas”.\nEl algoritmo SMOTE selecciona dos instancias similares utilizando vecinos más cercanos y bootstrapping, y genera muestras sintéticas a partir de instancias de las clases minoritarias. En este contexto, y siguiendo los pasos indicados a continuación, se probó el algoritmo para identificar perfiles de no-clientes con alta propensión a responder positivamente a una campaña de marketing.\n\nExtracción de datos externos para construir variables analíticas que describan el comportamiento del no-cliente. Esto puede hacerse aprovechando fuentes externas de datos, como la deuda en otros bancos y productos financieros. \nSelección de características para identificar variables relevantes a partir de un gran número de variables financieras y de comportamiento.\nDefinición del tamaño de la muestra utilizando SMOTE para probar el poder predictivo de algoritmos de clasificación como Regresión Logística.\n\n\nFuente: Dal Pozzo, Caelen, Waterschoot and Bontempi\n \n\nFinalmente, tras aplicar un modelo de Regresión Logística con y sin SMOTE, evaluamos el rendimiento en la muestra de prueba con indicador AUC.\n\n \nResultados\nDespués de comparar los resultados de la Regresión Logística con SMOTE y el proceso tradicional de Regresión Logística con muestreo desbalanceado, encontramos los siguientes resultados:\n\nAunque los dos diseños de los modelos obtuvieron la misma métrica AUC (0.78), la Regresión Logística SMOTE mejora el indicador de Especificidad de 0.71 a 0.73. La interpretación de eso es: la Regresión Logística SMOTE tiene más poder de predicción para identificar el rechazo de la oferta de tarjeta de crédito por parte de los no-clientes. (Figura 1)\nUna prueba comercial de la ventaja de la Regresión Logística SMOTE es la comparación de la capacidad de priorización entre dos diseños de modelos. Concluimos que la Regresión Logística SMOTE puede identificar casi 60 tarjetas de crédito adicionales vendidas en alta propensión a no-clientes que la Regresión Logística de manera tradicional. (Figura 2)\n\n \nFigura 1: Comparación del rendimiento de los modelos en datos de prueba.\n \n\n \nFigura 2: Comparación de la capacidad de priorización entre dos modelos de diseño\n \n\nSMOTE es una herramienta de código abierto en R con muchos tutoriales.\n\n"
    },
    {
      "title": "La Transformación de un Banco en una Empresa Digital",
      "url": "https://www.bbvadata.com/es/transformation-bank-digital-business/",
      "date": "24 octubre, 2017",
      "content": "\nLa semana pasada, The Economist, una de las publicaciones de referencia en el ámbito económico a nivel mundial, publicó un artículo sobre el Grupo BBVA, titulado: “BBVA, un banco español que se reinventa como empresa digital”.\nA lo largo del texto, se describen las iniciativas que BBVA, como banco global, está llevando a cabo para convertirse en el banco del futuro, y las razones por las que quiere llegar a serlo. En este sentido, Francisco González, presidente de BBVA, destaca que, tarde o temprano, los gigantes de internet (Google, Amazon, Facebook) serán los principales competidores de BBVA, debido a que “el mundo digital no permite muchos competidores, en unos 20 años, los bancos del mundo podrían quedar reducidos de unos miles a unas decenas, porque se necesitará escala para sobrevivir. La desconfianza de la regulación podría frenar a los e-gigantes, pero no para siempre. Si no se está preparado para este momento y no eres igual de eficiente que ellos, entonces estás muerto”.\nDesde BBVA Data & Analytics una de nuestras principales misiones es impulsar esta transformación hacia una empresa basada en datos, y empezamos a darnos cuenta de que lo que una vez fue una visión de futuro se ha convertido en realidad.\nCuando BBVA inició el camino hacia la digitalización, hace casi diez años, los datos se convirtieron en un activo de oro que muchos aspiraban a explotar, pero muy pocos sabían aprovechar. La interacción digital de BBVA con sus clientes a través de su presencia global, pero sobre todo de las aplicaciones multicanal, pone voz a las necesidades de los usuarios a los que damos servicio: supone una fuente de información rica y en actualización permanente que permite al Banco desarrollar productos y servicios adaptados a las necesidades específicas del cliente, aprovechando métodos y tecnologías de analítica avanzada.\nLa característica principal de todos los desarrollos que ha acometido la entidad recientemente es dotar al cliente de la capacidad de tomar mejores decisiones basadas en sus propios datos.\nUn buen ejemplo es Commerce360, lanzado hace algo más de un año, cuyo objetivo principal es, poner en manos de las PYMES, analíticas que les ayudan a entender qué pasa entorno a su comercio y cómo son sus clientes. Analíticas antes solo accesible para las grandes empresas.\nY en esta misma línea, recientemente ha visto la luz Bconomy, una herramienta de diagnóstico sobre salud financiera basada en un análisis de los gastos fijos y variables, y los ahorros del cliente, que, además de todo ello, engloba otras herramientas que BBVA pone a disposición de sus clientes como “Mi Día a Día”, “Mi Presupuesto”, “Mis Metas” y una nueva funcionalidad de benchmarking personal denominada “Compárate”.\nA partir de 2018 la nueva directiva europea PSD2 obligará a abrir los datos de aquellos clientes que así lo requieran y autoricen hacia terceras partes, con el objetivo de facilitar la interoperabilidad y la portabilidad. Esto desencadenará una competición entre bancos y también con agentes externos. Para BBVA, esto es más una oportunidad que una amenaza, como cita The Economist en su artículo.\nLa publicación también destaca los procesos que involucran estos desarrollos y la metodología agile que los sustentan, en palabras del propio Derek White, responsable global de Customer and Client Solutions, la idea es replicar la agilidad de las startups fintech, pero a gran escala.\n“Cuando se concibe un proyecto, se forma un pequeño grupo para trabajar en ello durante tres días. Se crea un prototipo en seis semanas. El producto terminado debería estar en las manos de los clientes en nueve meses. El ciclo trimestral se inicia con una sesión de planificación para discutir largo y tendido las prioridades. Concluye con una demostración, al estilo de las startups”.\nSi quieres leer el artículo completo en The Economist, puedes hacerlo a través de este enlace.\n\n"
    },
    {
      "title": "Personalización de Servicios Bancarios con Analítica Avanzada de Datos",
      "url": "https://www.bbvadata.com/es/personalizing-banking-services-advanced-data-analytics/",
      "date": "17 octubre, 2017",
      "content": "\nEn el modelo bancario tradicional, los datos eran producidos por el cliente y procesados por los analistas de datos. Los ingresos y gastos de los clientes se utilizaron para comprender las tendencias globales y diseñar productos y servicios atractivos para segmentos específicos.\nBBVA está aprovechando los métodos y tecnologías de analítica avanzada para dar a los usuarios un control total sobre sus datos y ayudarles a comprender su salud financiera para que puedan tomar mejores decisiones. La última novedad de BBVA, Bconomy, es una puntuación de salud financiera (0-100) basada en un análisis de los gastos fijos y variables y los ahorros del cliente.\nBconomy es más que una funcionalidad, y ha sido concebida para englobar otras herramientas existentes que BBVA pone a disposición del usuario, como “Mi Día a Día”, “Mi Presupuesto”, “Mis Objetivos” y una nueva herramienta de benchmarking personal denominada “Compárate”, brinda a los clientes las herramientas para tomar mejores decisiones financieras basadas en sus propios datos.\nTodas las herramientas existentes transforman los datos de las diferentes bases de datos de BBVA para dotar al usuario de mayor capacidad de control de sus finanzas. Gonzalo Rodríguez, director de Transformación Digital de BBVA, explica que “esta herramienta ayudará a los clientes a ahorrar dinero y afrontar gastos imprevistos”. Elena Alfaro, Head of Data and Open Innovation de BBVA, destacó que “el uso inteligente de los datos aporta personalización y relevancia y permite una mejor toma de decisiones que, en última instancia, ayudará a los clientes a gestionar sus recursos de forma más eficaz”.\n \nComponentes y Fuentes de Datos de BConomy\n \nProporcionamos brevemente una descripción general de los componentes analíticos detrás de Bconomy.\n\nCalificación Financiera: La calificación se basa en un análisis de las transacciones y utiliza una fórmula estándar para evaluar el desempeño de las finanzas de un cliente. La fórmula estima que un máximo del 50% de los gastos debe ser para gastos fijos, tales como alquiler de vivienda o electricidad; 30% para gastos variables y 20% para ahorros.\nCategorización de transacciones: Esta herramienta de análisis de texto, incluida en la funcionalidad “mi día a día”, está impulsada por el motor de categorización desarrollado en BBVA Data & Analytics, que utiliza un conjunto de grandes tecnologías informáticas para procesar diariamente millones de transacciones, clasificarlas y enriquecerlas con etiquetas de categorías. La taxonomía de categorización incluye 15 categorías principales con 72 subcategorías. Lea más aquí. El motor procesa un total de 7 millones de transacciones diarias, cuatro de movimientos de cuenta y tres de transacciones de tarjetas de crédito.\nBenchmark Sociodemográfico: Este componente permite al usuario comparar su desempeño financiero con el de otros clientes con perfiles sociodemográficos similares. El motor aprovecha los datos agregados y anónimos que se presentan al usuario como un dashboard personal.\n\n \nBConomy es parte de otros esfuerzos de BBVA para ayudar a los clientes a comprender mejor sus gastos y ser más eficientes en sus finanzas. Por ejemplo, durante el verano, BBVA lanzó “Movimientos Previstos”, el pronóstico de los movimientos financieros de los clientes para los próximos meses. Esta característica ahora está disponible como parte de Personal Finance Manager (PFM) de BBVA llamado “My Day to Day”, que actualmente es utilizado por 1 millón de clientes cada mes. El algoritmo principal, su motor de pronóstico, fue desarrollado por BBVA Data & Analytics usando herramientas Big Data como Apache Spark e implementando una combinación de algoritmos de aprendizaje automático para la predicción de series temporales. Es otro ejemplo de cómo BBVA Data & Analytics aplica la ciencia de datos para mejorar la vida financiera de los clientes de BBVA al proporcionar información útil para tomar decisiones.\n \nSi quieres saber más, te invitamos a ver el siguiente vídeo:\n\n\n"
    },
    {
      "title": "No se Trata de Aprendizaje Profundo, sino de Aprender a Representar",
      "url": "https://www.bbvadata.com/es/not-deep-learning-learning-represent/",
      "date": "10 octubre, 2017",
      "content": "\nRecientemente, hemos creado un grupo de trabajo dedicado a Deep Learning (Aprendizaje Profundo). Los grupos de trabajo ofrecen oportunidades para compartir internamente ideas, conceptos, recursos, código, etc. Además, se pretende promover el uso de Machine Learning (Aprendizaje Automático) en BBVA. Recuerdo vivamente cómo José Antonio Rodríguez, uno de los impulsores de este grupo de trabajo, nos dijo entonces:”Deberíamos llamarlo el grupo de trabajo sobre aprendizaje de la representación, más que sobre Deep Learning. Hoy en día, estoy convencido de que su perspectiva era correcta: un aspecto crucial del Deep Learning es que ayuda a hacer mejores representaciones (abstracciones) de la naturaleza -y de los negocios, para este caso.\nAsí que, dada toda la expectación que está rodeando al Deep Learning en estos días, pensé que sería oportuno reflexionar y pensar sobre lo que Deep Learning ha traído realmente a la comunidad de Machine Learning, y separar el trigo de la paja. En este post, intentaré resaltar la importancia del aprendizaje de la representación para el desarrollo de la Inteligencia Artificial, y describiré el papel del Deep Learning como un marco de aprendizaje de la representación.\nRepresentando Objetos y Conceptos\nUno de los problemas fundamentales en Machine Learning es cómo una máquina debe representar un objeto/concepto. Las máquinas sólo entienden los números, por lo que necesitamos encontrar una manera de condensar toda la información sobre un objeto en un conjunto de números (por ejemplo un vector). Hablando en términos generales, el vector sería la representación -seré más preciso más adelante-. Dicha representación puede ser utilizada más tarde para realizar diferentes tareas; tanto si queremos clasificar el objeto, generar nuevas tareas o hacer cualquier otra cosa, no es importante para nuestra discusión. En cualquier caso, necesitaremos una representación adecuada.\nPermítanme explicar lo que es una representación con un ejemplo sencillo: una manzana. La máquina necesita encontrar un conjunto de números (un vector) que represente adecuadamente lo que hace de la manzana una manzana: su textura, color, sabor, olor, forma, etc. Además, la representación debe reflejar los diferentes contextos donde aparece el concepto de manzana: no debe aparecer en el espacio (no muy a menudo, esperemos), aunque es común observar una manzana colgada de un árbol. Esta representación genérica de la manzana puede ser utilizada posteriormente para crear nuevas manzanas con diferente brillo, sombras y así sucesivamente -cada manzana en particular será una realización de la representación abstracta del concepto manzana-. O para pintar una naturaleza muerta que contiene una hermosa y brillante manzana en el centro. Como dijimos anteriormente, la tarea no importa para esta discusión; lo importante es que la máquina necesita una representación adecuada del concepto de manzana.\nComo algunos de ustedes se preguntarán en este punto, ¿pueden todas las manzanas ser representadas por un solo vector? Ciertamente no. Necesitamos un conjunto completo de vectores, que probablemente serán similares -para alguna definición de similitudes, que yo dejo para otro post. El espacio (Riemanniano) en el que viven estos vectores de manzana es una estructura matemática conocida como colector (véase la subsección 5.11.3 del Deep Learning book y el excelente post de Christopher Olah). Por lo tanto, la tarea de cualquier técnica de aprendizaje de la representación es aproximarse a tales colectores.\nRedes Neuronales Profundas\n\nEntonces, ¿qué hay de Deep Learning? Bueno, el Deep Learning no es más que una técnica (poderosa) para aprender representaciones de datos, de una manera jerárquica y composicional. Las redes Neuronales profundas -Deep Neural Networks (DNN)- aprenden representaciones de un objeto de entrada aprendiendo abstracciones de creciente complejidad, que pueden ser compuestas para generar conceptos de mayor nivel, y finalmente, una abstracción apropiada del objeto original. Por lo tanto, es jerárquica (de abstracciones más bajas a abstracciones más altas) y composicional (las abstracciones de bajo nivel dan lugar a las de mayor nivel). Esto se muestra esquemáticamente en la figura anterior: la complejidad de la representación aumenta con la profundidad (de izquierda a derecha en la figura anterior). Observe que las representaciones de capas más profundas (abstracciones de capa superior) se componen de capas anteriores (abstracciones de nivel inferior) a medida que la información pasa a través de la red (ver, por ejemplo, este gif animado que ejemplifica el flujo de información en la clasificación de imágenes con TensorFlow).\nAunque las DNNs (Redes Neuronales Profundas) por sí solas se han aplicado con éxito en varios campos como la clasificación de imágenes, el reconocimiento de voz, etc., es importante tener en cuenta que su principal contribución en esos casos fue la creación de una representación adecuada. Por ejemplo, en el caso de una DNN que realiza la clasificación de imágenes, la red aprende simultáneamente un clasificador y una representación. La representación aprendida puede ser utilizada entonces para encontrar imágenes similares. O podría ser usada para generar nuevas. En la imagen animada de la izquierda, del blog de Erik Bernhardsson, una DNN fue entrenada en un conjunto de datos que consta de 50.000 fuentes diferentes. Se crean fuentes completamente nuevas a partir de la representación obtenida, aquí se muestra una interpolación entre algunas de estas fuentes. Hay varios GIF animados en ese blog, que demuestran aún más el potencial del aprendizaje de la representación adecuada. Se podría ir aún más lejos con las representaciones de DNN y dejar que la red alucine un poco, ver la figura de abajo.\n\nOtro paradigmático ejemplo es el caso de DeepMind y el antiguo juego del Go. Para el dominio de este juego por una Inteligencia Artificial, se aplicaron técnicas de Aprendizaje por Refuerzo (Reinforcement Learning) para prevenir el uso de un Árbol de Búsqueda Monte-Carlo completo. Aquí, las DNNs proporcionaron al Aprendizaje por Refuerzo una representación adecuada del tablero de juego completo – técnicamente hablando, de las funciones de política y valor dadas a los estados del tablero y a las acciones de los jugadores. Usted puede encontrar una explicación fácil de seguir del algoritmo AlphaGo de DeepMind aquí.\nSiguientes pasos\nVolviendo a la definición de Deep Learning, ¿son los aspectos jerárquicos y composicionales la historia completa? Bueno, en realidad no. Una cosa que, en mi opinión, ha hecho que las DNNs sean tan exitosos es la incorporación de simetrías. Las simetrías han permitido a las DNN superar la maldición de la dimensionalidad y, a su vez, poder seleccionar una pequeña familia de funciones del infinito conjunto de posibles. En otras palabras, han permitido a las computadoras aprender representaciones de los datos de manera eficiente y efectiva. Pero ésta es otra historia fascinante. Dejaré la discusión sobre la maldición de la dimensionalidad, y la importancia de incorporar simetrías mientras se aprende, para otro post.\nMientras trabajamos en ello, tal vez quieras echar un vistazo al trabajo de Stephane Mallat y Joan Bruna sobre Scattering Convolutional Networks (Redes Convolucionales Profundas), que proporciona un marco matemático para entender las Redes Convolucionales Profundas, y el trabajo reciente sobre la conexión entre Deep Learning y Física de Lin, Tegmark y Rolnick.\nEsperemos que después de esta lectura, obtengas una visión más clara de lo que representa el Aprendizaje de la Representación, y cómo el Deep Learning encaja en esta disciplina.\nAgradecimientos\nMe gustaría agradecer sus útiles comentarios y la lectura cuidadosa de este artículo a Iskra Velitchkova, Leonardo Baldassini, Fabien Girardin, José Antonio Rodríguez, Roberto Maestre, Juan Ramón Duque, Marco Creatura y Antonio Moneo.\n\n"
    },
    {
      "title": "Presentación de Urban Discovery: Nuestra Contribución a una Mejor Visión de las Grandes Ciudades",
      "url": "https://www.bbvadata.com/es/presentation-urban-discovery-contribution-improved-vision-big-cities/",
      "date": "5 octubre, 2017",
      "content": "\nEl pasado 21 de septiembre, en el Centro de Innovación BBVA, presentamos los resultados de nuestra última investigación de Analítica Territorial: Urban Discovery, una nueva herramienta desarrollada conjuntamente con CARTO, que muestra cómo el análisis de datos anónimos de pagos efectuados con tarjetas de crédito puede servir para identificar áreas funcionales en Madrid, Barcelona y México DF, para describir las principales dinámicas de la ciudad y sus ámbitos de especialización, y para comparar zonas de diferentes ciudades.\nEsta herramienta no sólo está pensada para recoger propuestas de etiquetado de áreas por parte de los usuarios, sino que también permite la descarga de sus resultados, para fomentar su reutilización en futuras investigaciones. La audiencia la conformaron numerosos expertos en urbanismo de los sectores público y privado.\n\nPrograma\nEl evento fue dirigido por Fabien Girardin, co-CEO de BBVA Data & Analytics y experto en aplicaciones basadas en datos urbanos. Fueron tres los ponentes: por un lado Juan Murillo y Juan de Dios Romero, en representación del equipo que realizó la investigación y el análisis por parte de BBVA Data & Analytics, y por otro Jaime de Mora, representante de CARTO, empresa pionera en el desarrollo de la herramienta GIS sobre la que se construyó el proyecto.\nJuan Murillo habló sobre las iniciativas de Análisis Urbano y Datos para el Bien Social desarrolladas en BBVA Data & Analytics, destacando algunas de ellas, como por ejemplo, cómo medimos y caracterizamos el impacto de las decisiones de gestión urbana en las ciudades, a partir de la huella digital de la actividad comercial, un análisis que fue llevado a cabo en las pasadas Navidades para estudiar los efectos de las medidas de calmado de tráfico promovidas por el Ayuntamiento de Madrid en la Gran Vía; o los resultados de nuestro proyecto “Big Data y Turismo” desarrollado en colaboración con BBVA Bancomer y la Secretaría de Turismo en México (SECTUR), cuyo objetivo fue crear nuevas percepciones basadas en los patrones de gasto de los turistas nacionales y extranjeros para potenciar el turismo en diferentes corredores del país.\nJuan de Dios Romero se centró en el desarrollo metodológico y tecnológico de Urban Discovery. En este artículo puedes encontrar las principales ideas que compartió durante el evento sobre los retos analíticos y de implementación que se han resuelto en este proyecto, así como las principales características y experiencia de usuario de la herramienta. Al cerrar esta primera parte, Juan Murillo contribuyó a la interpretación de los resultados desde una perspectiva urbana.\nPosteriormente, Jaime de Mora habló sobre el poder de los datos geolocalizados y cómo esto había provocado de alguna manera el nacimiento de CARTO. Entre los proyectos que presentó, podemos destacar dos proyectos muy interesantes, primero un análisis visual de las transacciones de tarjetas de crédito antes y durante el Mobile World Congress de 2012 para mostrar el impacto económico sobre la ciudad, desarrollado en colaboración con nosotros. En segundo lugar, una herramienta que permite monitorizar la deforestación de los bosques a partir del análisis de imágenes de satélite. Sin embargo, como dato destacable, señaló que aunque el 80% de todos los datos generados están geolocalizados, sólo el 10% de los gobiernos más pioneros los están explotando.\n\nEl acto contó con la presencia de personas de diversos ámbitos como periodistas, investigadores académicos y representantes de los Ayuntamientos de Madrid y Barcelona, que aportaron interesantes preguntas al debate con los ponentes, en torno a los siguientes temas:\n\nSuperación de las delimitaciones oficiales estáticas según distritos y barrios\nEntender las ciudades a través de datos dinámicos: aplicaciones prácticas de estos conocimientos para los ciudadanos, las empresas y el sector público\nDatos abiertos y reutilización de los datos: ejemplos académicos y/o comerciales\nAccesibilidad a los datos en bruto a través de BBVA API Market\nLa necesaria fusión de las disciplinas de la estadística tradicional, que aporta un rigor metodológico imprescindible, y la ciencia de datos, que abre nuevas oportunidades a los institutos oficiales de estadística.\nLa necesidad de disponer de habilidades en Ciencia de Datos y equipos con capacidades tecnológicas dentro de las administraciones públicas\n\nLecturas adicionales\nTras su presentación pública, el proyecto ha tenido un interesante reflejo en medios de comunicación, aquí puedes encontrar algunos de los artículos destacados:\nNota de prensa, publicada por bbva.com\nAsí cambian las tarjetas de crédito los mapas de las ciudades, por El País Retina\nEstas son las semejanzas entre Madrid y Barcelona, por La Vanguardia\nBBVA pone el ‘big data’ al servicio del urbanismo, por Expansión\n¿Hacia la ciudad ‘black friday’?, por eldiario.es\n \n\n"
    },
    {
      "title": "Una Solución Rentable y Escalable: Sistema de Recomendación basado en Filtros Colaborativos",
      "url": "https://www.bbvadata.com/es/cost-effective-scalable-collaborative-filtering-based-recommender-system/",
      "date": "3 octubre, 2017",
      "content": "\nEl pasado jueves (26 de septiembre) tuvimos la oportunidad de presentar una colaboración entre BBVA Data & Analytics y BEEVA en el Theatre of Partners durante la AWS Summit Madrid 2017.\nEn la conferencia, presentamos un enfoque rentable para los sistemas de recomendación basados ​​en el filtro colaborativo (RS), que escalan a millones de usuarios y un millón de productos. Nuestra implementación hizo uso de AWS EMR con Spark, para producir un modelo de factorización de matriz implícita de la matriz de calificación original. Al principio del proyecto descubrimos que la implementación basada en la CPU resultó ser demasiado costosa y poco práctica para el cálculo de la propia recomendación -i.e. la multiplicación de vectores de característica de producto de usuario, que implica entre 1012 a 1014 operaciones. Como la multiplicación matricial es una operación embarazosa paralela, decidimos que la tarea podría hacerse en GPUs.\nLa charla fue bien recibida, por lo que pensamos en explicarlo en mayor detalle en un blog. Aquí, empezamos a motivar la necesidad de RS muy grande cuando atacamos la larga cola del catálogo de productos. A continuación, revisaremos brevemente algunas técnicas de RS y daremos los detalles de cómo las implementamos en los clústeres EMR de Spark y AWS. Después de mostrar por qué Spark falló en nuestro caso de uso, volvemos al origen del problema, es decir, la escala del número de recomendaciones que se deben hacer. Esta enorme escala nos obligó a pasar a las GPU, y por lo tanto, explicamos a continuación por qué las GPUs son más adecuadas para esta tarea, y el detalle de su implementación utilizando TensorFlow en las instancias AWS p2.8xlarge- lanzado para uso de producción sólo una semana antes empezamos a usarlos-. Posteriormente, ofrecemos algunos de los aspectos clave para entrar en producción en la nube AWS con este tipo de tecnologías. Finalmente, sacamos algunas conclusiones y líneas de trabajo futuro.\nSistemas de Recomendación y Larga-Cola\nLos sistemas de recomendación (RS) se están convirtiendo en ubicuos a las aplicaciones en línea más comunes: de sugerencias personalizadas de la siguiente canción para escuchar, la película que ver, o el próximo producto financiero que adquirir, pasando por el descubrimiento de un comercio cercano para la próxima compra. Este tipo de RS presentan a los clientes con sugerencias personalizadas, lo que permite a las empresas hacer conocer los productos menos populares, pero rentables en la larga cola de su catálogo.\n\nEl fenómeno de la larga cola es una propiedad estadística presente en las distribuciones después de un decaimiento de la ley de potencia (pero no limitado a): hay pocos elementos que ocurren con una frecuencia muy alta y un número enorme que aparece en un número relativamente pequeño de casos. Cuando se traza la frecuencia (aka, popularidad) de cada elemento ocurrencia, se ve una cola que parece no tener fin, de ahí el nombre de larga cola. Esto se representa esquemáticamente en la figura anterior. Hay muchos ejemplos de distribuciones que exhiben una larga cola. El número de seguidores que cada usuario tiene en Twitter podría ser un ejemplo, con Barack Obama siendo ciertamente considerado como un elemento de cola corta en dicha distribución (muy popular), y los autores de este post como elementos en la cola larga (mucho menos popular que Obama, calculamos). Otros ejemplos clásicos incluyen la distribución de ventas por producto en un minorista web, como Amazon, o la distribución de popularidad de canciones y películas en plataformas como Spotify o Netflix.\nEntonces, ¿por qué la cola larga es absolutamente relevante? Resulta ser muy importante para las economías de escala, como señaló Chris Anderson en su famoso libro The Long Tail: Why the Future of Business Is Selling Less of More. La idea básica es que si usted tiene un catálogo de productos muy grande, el volumen de ventas acumuladas de productos raros puede compensar o superar el volumen de ventas de los pocos productos bien conocidos. De hecho, este modelo de negocio es la esencia de varias empresas exitosas como Amazon, Spotify o Netflix. El problema es, ¿cómo mostrar el catálogo completo a los usuarios? Aquí es donde entran en la escena los sistemas de recomendación: Los sistemas de recomendación son herramientas diseñadas para filtrar información no relevante al mismo tiempo que ofrecen sugerencias personalizadas. Además, si desea proporcionar a los usuarios una experiencia verdaderamente personalizada, debe tener un catálogo de productos grande que satisfaga la variedad de gustos de cada posible usuario.\nMétodos de Filtrado Colaborativo\nLa siguiente pregunta es cómo construir un sistema de recomendación para productos de nicho, de una manera asequible y escalable. En el caso de tener la historia de las adopciones de los usuarios (es decir, escuchas, relojes, clicks, compras, etc.), una técnica bien establecida es el llamado Filtrado Colaborativo (CF) para la retroalimentación implícita. Los métodos implícitos de los CF intentan predecir la preferencia que un usuario tendrá para los productos no vistos en el catálogo, basado en las preferencias observadas que todos los usuarios han tenido para todos los productos. Un ejemplo bastante famoso es el de Amazon de “usuario que compró este producto también compró estos otros productos”.\n\nUn déficit de este tipo de métodos CF (usualmente conocido como K-Nearest Neighbors, o KNN), es que no proporciona a los usuarios una recomendación para todos los productos posibles (no a bajo coste, al menos), sino más bien para el K más similar a un usuario / producto dado. Sin embargo, nuestro modelo de negocio requiere tener una recomendación para cada producto posible, ya que sólo un subconjunto del catálogo de productos estará disponible a la vez. Esta restricción no es tan rara en el mundo de los negocios: considere, por ejemplo, productos descontinuados, que podrían contener valiosa información sobre las preferencias del usuario y, sin embargo, no pueden ofrecerse.\nTal requisito puede ser satisfecho por otro, muy popular, método de filtrado colaborativo: factorización de la matriz. Para esta técnica, las interacciones entre usuarios y productos se apilan formando una matriz grande (la matriz de calificación), que tiene tantas filas como usuarios, y tantas columnas como productos hay. Ver la matriz R en la siguiente figura. Observe por favor que tal matriz de la calificación estará llena de ceros, pues la mayoría de las interacciones del artículo del usuario son desconocidas, y tendrá pocos elementos con un 1 (que indica una interacción observada del artículo del usuario). La tarea de los modelos de Factorizaciones de Matriz Implícita es completar la matriz de clasificación R, y así proporcionar una predicción para la preferencia de un usuario por cada producto. Estas predicciones numéricas se pueden ordenar, produciendo una lista de recomendaciones.\n\nSin entrar en demasiados detalles técnicos, la factorización de matrices consiste en descomponer la matriz de clasificación original en dos matrices más pequeñas, U y P en la figura anterior. La recomendación se obtiene después de multiplicar los dos. Como estas matrices son forzadas a ser mucho más pequeñas que la original, su producto producirá una matriz de clasificación sin ceros (matemáticamente, las matrices de descomposición U y P tienen un rango k, con k típicamente alrededor de varias decenas o centenas, eso es mucho menor que el número de usuarios o elementos).\nEn este punto, es importante darse cuenta de la escala de la matriz de calificación: en nuestro caso, constaba de decenas de millones de usuarios y más de un millón de productos. Por lo tanto, necesitábamos tomar una solución escalable, probablemente asequible a este problema.\nEntramos en profundidad en el Sistema de Recomendación de Spark\nHay varias implementaciones eficaces y escalables de Factorización de Matrices Implícita en la industria. Entre ellos, uno destacado es el proporcionado por Apache Spark, un motor de procesamiento de datos distribuido que se puede ejecutar fácilmente en Amazon Web Services con un cluster de Mapreduce Elastic.\nApache Spark implementa una versión distribuida del método Alternating Least Square (ALS) method with Weight Regularization. En nuestro caso, utilizamos la API Scala, así como la versión implícita del modelo, diseñada para modelar preferencias en lugar de clasificaciones. Normalmente ejecutamos nuestros experimentos en instancias AWS r3 / r4, ya que están optimizadas para motores que consumen mucha memoria, como Spark. Los tamaños de los grupos variaban de unas pocas decenas a más de un centenar de ejecutores, dependiendo de cada caso.\nComo buena práctica de código limpio, implementamos una definición personalizada para cada clase, forzando una clara separación entre las fases de modelado y recomendación. Estas clases cubren el mapeo inicial de los archivos de origen, el cálculo de un modelo de factorización de matriz y la generación de recomendaciones. Nos gustaría destacar que una definición adecuada de campos y tipos de derechos era fundamental para mejorar el rendimiento general. Además, optimizamos todos los hiperparámetros de Spark, como el nivel de paralelismo, la memoria y la fracción de almacenamiento, el montón JVM, etc.\nDespués de todos estos esfuerzos, pudimos calcular el modelo de factorización de la matriz con recursos razonables (tanto en tiempo como en coste). Sin embargo, la generación de todas las recomendaciones requeridas falló sistemáticamente, no importa cuántos ejecutores se usaron. Curiosamente, el uso de la CPU durante la fase de recomendación siempre se mantuvo por debajo del 30%.\nEste uso relativamente bajo de la CPU, nos permitió darnos cuenta de que el rendimiento de ALS está fuertemente afectado por su tamaño de bloque. El tamaño de bloque determina cómo se dividen las matrices de usuario y de producto (U y P en la descripción anterior), de modo que la multiplicación matricial se puede realizar en bloques usando bibliotecas de álgebra estándar. Dado que Spark es un sistema distribuido, el tamaño de los bloques determina tanto el uso de la CPU como la cantidad de datos que debe transferirse entre los ejecutores. Desafortunadamente, el parámetro de tamaño de bloque fue codificado en el método de bloqueo dentro de la implementación de Spark que usamos (y permanece como tal, como en la versión 2.2.0 de Spark), vea la siguiente figura. Después de realizar varias pruebas de esfuerzo, un valor diferente para el tamaño de bloque predeterminado demostró un mejor rendimiento. En consecuencia, implementamos una versión personalizada del método bloquify, que nos permitió parametrizar el tamaño del bloque.\n\nGracias a esta parametrización, hemos logrado incrementar el uso de la CPU del 30% al 70%, ¡nada mal! Y sin embargo, no pudimos terminar la fase de recomendación con un clúster de 80 nodos funcionando durante 6 horas. ¿Qué podría estar saliendo mal?\nRepensando el problema: CPU vs GPU\nDado los esfuerzos que hicimos tratando de optimizar varios clusters de Spark -grandes y optimizados-, pensamos que sería oportuno detenernos y pensar en el problema que estábamos enfrentando. Estaba claro que estábamos cosechando todos los beneficios de Spark en la CPU, y sin embargo, no pudimos completar la tarea.\n¿Sería posible que el número de operaciones requeridas para la recomendación fuera simplemente demasiado? Bueno, podría ser. Teníamos entre 2 y 20 millones de usuarios, dependiendo del país al que se aplica la RS, y más de un millón de artículos por país. Por lo tanto, el número de recomendaciones era algo entre 2 y más de 20 trillones. Eso es sin duda mucho, pero era difícil entender el tamaño de la misma.\n\nPara visualizar cuánto era un trillón de operaciones, empezamos a buscar ejemplos en la naturaleza que pudieran servir como comparación. Por ejemplo, encontramos en la web que el número de dólares en efectivo en todo el mundo era de alrededor de 100 mil millones. Pero eso no es ni siquiera cerca de un billón! Así que pensamos en pasar a otro campo, donde casi todo se cuenta con grandes números (o muy pequeños, pero nada intermedio): la astrofísica. Allí, tenemos varios ejemplos que nos pueden ayudar a visualizar el tamaño de nuestro problema -ver la figura anterior, donde la superficie de cada bola es proporcional al número de operaciones-. Por ejemplo, el número de estrellas que constituyen la Vía Láctea -nuestra galaxia, que no es tan grande- se calcula en 400.000 millones a lo sumo; no lo suficientemente cerca. Pero el número de galaxias en el Universo observable se cree que es de alrededor de dos billones. ¡Bingo! Por lo tanto, para un país de tamaño mediano, tuvimos que hacer tantas recomendaciones como galaxias; y para el país más grande, teníamos diez veces eso!\nPor lo tanto, dada la asombrosa cantidad de recomendaciones que teníamos que calcular, necesitábamos paralelizar aún más el cálculo. Y esto es lo que se hacen las Unidades Gráficas de Computación (GPU). Tal vez, habías oído hablar de ellas en el contexto de los videojuegos, donde se utilizan para el procesamiento de fotogramas de vídeo lo más rápido posible. La cosa es que una imagen no es nada más que una matriz, por lo que fue el hardware que necesitábamos. De hecho, las GPU de propósito general se utilizan hoy en día para la optimización Deep Learning precisamente por la misma razón: Deep Learning consiste en un montón de multiplicaciones matriciales.\nEn cuanto al nivel de paralelismo alcanzado por los dos hardwares, Ricardo Guerrero, uno de los Científicos de Datos en BEEVA Labs, dio como una brillante imagen de dibujos animados que ejemplifica las diferencias entre CPU y GPU. Las CPU están optimizadas para realizar tareas complicadas una tras otra con baja latencia. Es como un coche de Fórmula Uno: puede correr muy rápido, pero transporta una sola persona cada vez. Por el contrario, las GPU están optimizadas para tareas paralelas y simples (aka, multiplicaciones matriciales). Así que usted podría pensar en ellos como camiones, que no son tan rápidos, pero pueden transportar muchas cosas a la vez, por lo tanto, proporcionar más ancho de banda dependiendo de la naturaleza de la carga de trabajo. El número típico de núcleos para una CPU varía de 2 a 64, mientras que un Nvidia Tesla K80 GPU proporciona 2,496. Para fines de referencia, el precio actual de una instancia de gran demanda de m4.16xlarge con 64 núcleos que ejecutan Linux es de 3.55 USD por hora, mientras que el precio de una instancia de gran demanda de p2.8xlarge con 19.968 núcleos que ejecutan Linux es de 7.78 USD por hora.\n\nPor lo tanto, lo único que quedaba era aprender a programar tales dispositivos GPU. En el nivel muy bajo, esto se hace en marcos como Nvidia CUDA y cuDNN. Pero queríamos un API de nivel superior más productivo, de código abierto, estable y con una gran comunidad detrás de él. Ingrese TensorFlow.\nEn noviembre de 2015 Google lanzó TensorFlow como una biblioteca de software de código abierto para realizar cálculos numéricos, con especial atención a las GPU. Desde entonces, se ha convertido en una herramienta popular para desarrollar e implementar cualquier modelo de aprendizaje automático. Así que decidimos aprovechar nuestras oportunidades con TensorFlow.\nCon el fin de integrar en la memoria las matrices resultantes de la factorización, desarrollamos una estrategia de partición simple para que la multiplicación matricial se realice en bloques. Además, hacemos uso de TF Queues, lo que resulta en un aumento del 33% en el rendimiento. Larga historia corta, nos las arreglamos para producir todas las recomendaciones (y ordenarlos) en menos de 2 horas! Compare este resultado con la ejecución de 6 horas en un clúster de 80 nodos usando Apache Spark, que de hecho, nunca terminó. Por ejemplo, la reducción del coste es superior a un factor de 20.\nEntrando en producción\nPara poder lanzar el sistema de recomendación definido en un entorno de producción, definimos una arquitectura escalable, confiable y flexible en Amazon Web Services. Esta solución se basa en los fundamentos de la arquitectura en nube, tanto la infraestructura como el código, la automatización, la escalabilidad y los servicios gestionados.\n\nHemos creado AWS Lambdas programados y activados para orquestar cada paso en el proceso de recomendación de una manera sin servidor, desde alimentar el proceso y generar el modelo a obtener acceso a los valores finales recomendados.\nEsta solución sigue dos enfoques diferentes para el procesamiento de datos. Primero, utilizamos EMR ejecutando Apache Spark para hacer cálculos más generales, es decir, generar nuestros propios modelos personalizados. Para procesos de computación específicos con mayor consumo de recursos, creamos nuestros propios clusters de procesamiento en instancias de EC2 P2 basadas en GPU Nvidia para ejecutar fragmentos de código TensorFlow. Estos grupos crecen y se reducen en función del tamaño de la carga de trabajo.\nHacemos un uso intensivo de los servicios como CloudFormation y EC2 Container Registry, tanto desde una infraestructura como un punto de vista de administración de código para automatizar la creación y administración de recursos de la nube. Es importante notar que debido a las características de nuestro sistema de recomendación, la escalabilidad fue uno de los principales puntos de dolor que necesitábamos abordar. Ser capaz de configurar y proporcionar nuestra infraestructura, especialmente las instancias de GPU, ya que el código nos permite encajar las cargas de trabajo de procesamiento de producción en una cuestión de configuración de propiedades.\nLecciones aprendidas y próximos pasos\nConsideramos que una de las estrategias clave para tener éxito con este caso de uso fue la participación de equipos de Ciencia de Datos e Ingeniería de Datos desde el principio. Con frecuencia encontramos proyectos en los que una de las dos partes es desestimada y tomada en consideración demasiado tarde en la cadena de distribución, lo que lleva a malas técnicas formales de verificación, algoritmos poco precisos, soluciones no escalables, infraestructura costosa, preocupaciones de seguridad o malos resultados. Teniendo en cuenta los aspectos matemáticos y de ingeniería al construir desde el principio es una garantía en el largo plazo.\nHablando de escalabilidad, la complejidad de Big Data se ha medido tradicionalmente en volumen; cuanto más grande sea tu fuente de datos, más elegante será tu sistema. Ese no es el caso aquí, ya que estábamos manejando un conjunto de datos relativamente pequeño (escala TByte), pero tuvimos que calcular miles de millones de operaciones vectoriales con ellos. Aquí es donde los frameworks basados ​​en CPU tradicionales fallan mal, ya que las GPUs pueden ejecutar miles de operaciones matemáticas en paralelo mientras que las CPUs actuales sólo pueden ejecutar menos de cien. Las GPU han interrumpido el panorama de Ciencia de Datos y Big Data. En nuestra opinión, los que brinden un mejor apoyo para ellos liderarán la carrera en los próximos años.\nEsta tendencia se ha vuelto cada vez más evidente en los últimos meses con el aumento exponencial de la popularidad de los marcos de GPU como TensorFlow y MxNet. Pero no fue tan claro hace un año cuando empezamos esta colaboración. Por ejemplo, la familia de instancias de GPU AWS P2 fue anunciada para su uso en producción apenas una semana antes de que comenzáramos. Ser capaz de desarrollar utilizando tecnologías de vanguardia y probar rápidamente a bajo coste utilizando un proveedor de la nube fue una gran ventaja para la reducción de time-to-market. Sin embargo, nuestra arquitectura de contenedores enfrentó algunos desafíos; por ejemplo, no era fácil, y aún no lo es, desplegar instancias de GPU personalizadas utilizando contenedores de Docker. Además, desde la perspectiva de una solución de extremo a extremo, tuvimos que implementar soluciones ad-hoc de monitoreo y escalado.\nEn cuanto a nuestros próximos pasos, aunque la solución presentada es barata y escalable, los sistemas de recomendación de factorización de matriz estándar, tales como ALS-WR, son conocidos por proporcionar recomendaciones de calidad moderada para matrices de clasificación muy escasas. Por lo tanto, nuestro trabajo actual se centra en la implementación de métodos de filtrado colaborativo basados ​​en GPU, que aprovechan la infraestructura de AWS y proporcionan recomendaciones personalizadas de larga cola. De hecho, nuestra arquitectura ha sido construida para soportar un esquema multi-modelo, de modo que los desarrollos personalizados puedan ser incluidos en el registro de imágenes y ser desplegados en nuevas tuberías de procesamiento.\nAgradecimientos\nQueremos agradecer a todos los equipos que hacen realidad este proyecto: Payments Business Unit de BBVA, los equipos de ingeniería y soporte de BEEVA, Data and Open Innovation Platform y el equipo de pagos de BBVA Data & Analytics.\nJessica Olmos, Cristian Galán, Valentín Quintas, Pedro García,\nRoberto Andradas, Iván Fernández and César Silgo from BEEVA\nJuan Ramón Duque, Marco Creatura, Javier García Santamaría\nand Juan Arévalo from BBVA Data & Analytics\n\n"
    },
    {
      "title": "Lo que vimos en Spark Summit 2017: Spark adopta Deep Learning",
      "url": "https://www.bbvadata.com/es/saw-spark-summit-2017-spark-embraces-deep-learning/",
      "date": "28 septiembre, 2017",
      "content": "\nLa conferencia Spark Summit 2017, que tuvo lugar en San Francisco el pasado mes de junio, atrajo a más de 3000 miembros de distintos sectores que participaron en más de 170 sesiones para discutir las tendencias sobre la que se construirán los productos basados en datos en el futuro.\nApache Spark se ha convertido en el estándar de facto en el procesamiento masivo de datos debido a su amplia y rápida adopción en diversos sectores de la industria y academia y a su activa comunidad de desarrolladores que ha permitido la rápida evolución de este software y del ecosistema alrededor de él.\nEn este artículo resumimos los principales hitos de esta convocatoria:\nSpark 2.2: Hola Deep Learning, adios micro batching\nDataBricks (compañía fundada por los creadores de Apache Spark) abrieron el evento en calidad de anfitriones, presentando la nueva versión de Spark y sus funcionalidades. Concretamente, se presentó la versión 2.2 que incluirá entre otras cosas dos aspectos de especial interés:\n\nDeep Learning Pipelines: Es la posibilidad usar redes neuronales de manera natural dentro de Spark, siendo un elemento más a ser incorporado en un proceso de análisis estándar (ML pipeline), a través de una integración con frameworks conocidos como Tensor Flow, Keras y BigDL (comentaremos sobre este último después), permitiendo, entre otras cosas, usar una red neuronal como si fuera una función definida por el usuario (UDF).\nStructured/Continuous Streaming: Structured streaming es la capacidad de combinar la potencia de Spark SQL en un flujo, lo que facilita enormemente el desarrollo de este tipo de aplicaciones. Por su parte Continuous Streaming es la nueva capacidad Spark de procesar flujos en tiempo real, sin micro batches. Esto es algo que la comunidad ha venido demandando desde los orígenes del Streaming en Spark, cerrando así una brecha ante su potencial competidor, Flink.\n\nTodo esto lo explican en una interesante demo relacionada con James Bond, he aquí el video de la presentación\n\nOptimización Spark SQL\nPor otra parte, a lo largo del evento se realizaron presentaciones que profundizaban sobre los diversos aspectos de la herramienta, por ejemplo:\n\nOptimización de Spark SQL: Catalyst es un optimizador de consultas Spark SQL que construye el plan de ejecución como un composición operaciones sobre datos estructurados, usando una serie de estadísticas sobre las fuentes consultadas (Cost Based Optimization – CBO) para determinar el orden en el que se aplican. Esto permite reducir el tiempo requerido para consultar datos.\n\nCatalyst\nCBO\n\n\n\nDeep Learning\nComo es de esperar, Deep Learning fue un tema predominante en toda la conferencia. Intel presentó extensamente su librería de redes neuronales BigDL, la cual permite integración con Spark y MKL (Librería matemática de Intel optimizada para sus procesadores), además permite implementar las principales arquitecturas de Redes neuronales (Perceptrones multicapa, Convolucionales, Recurrentes, etc). Parece que Intel está haciendo una apuesta grande por posicionarse en este mercado, su reto principal será ganar terreno en un espacio dominado en el Software por TensorFlow y en el Hardware por Nvidia.\nAlgunas charlas adicionales sobre Deep Learning:\n\nTensorFlow On Spark, propuesta de Yahoo! De llevar Tensor Flow al mundo Spark.\nNvidia mostró su hardware especializado para entrenar/servir modelos y su desempeño en diversos frameworks (DL4J / H2O)  y casos de uso.\nProcesamiento de Lenguaje Natural usando CNTK, la librería Deep Learning de Microsoft\nComparativa entre diferentes arquitecturas hardware/software durante el entrenamiento de redes neuronales (CPU, GPU, Multi-GPU, TensorFlow On Spark, etc)\n\nCasos de uso\nUno de los aspectos más destacables de este edición del Spark Summit es la cantidad y diversidad de casos de uso interesantes que involucran a Spark como herramienta principal, lo cual demuestra la amplia comunidad alrededor de esta herramienta. He aquí unos cuantos:\n\nDetección de jugadores tóxicos en League of Legends: A partir del análisis de los chat textuales y con el uso de Word2Vec se identifican jugadores con tendencia a insultar y deteriorar la experiencia de otros jugadores.\nHotels.com presentó su infraestructura analitica y un caso de uso concreto de análisis de imágenes a gran escala.\nLinkedin analiza la interacción entre sus usuarios usando GraphX\nColaboración entre Microsoft y la ONU para el análisis y toma de decisiones ante crisis humanitarias, involucra el uso de streaming y GraphQL.\nSETI e IBM comenta su experiencia tecnica en el uso de Spark para búsqueda de vida extraterrestre\nProductivización de un modelo para la detección de Botnets.\n\nBuenas prácticas\nLa experiencia que la comunidad ha ido adquiriendo a través del tiempo ha permitido generar un conjunto de buenas prácticas ante diversas problemáticas tales como la productivización, desempeño, monitorización de aplicaciones basadas en Spark, etc. En esta conferencia fueron mencionados varias de ellas:\n\nLos procesos ETL representan un porción importante de cualquier aplicación basada en datos. Databricks habló sobre como los Data Pipelines gestionan problemáticas comunes de este proceso\nOptimización en procesos Spark en Facebook.\nComo Netflix ha mejorado su proceso de extracción de características.\nInfraestructura de ciencia de datos en PayPal\nMonitorización y optimización de procesos Spark usando Dr Elephant.\nAspectos a tener en cuenta en la productivización de aplicaciones MLlib\nClipper: Una herramienta emergente para productivización de modelos de machine learning\nAlgunas consideraciones al desarrollar aplicaciones basadas en Grafos\nProductivización de scripts R usando Sparkling Water\n\nEstas fueron solo algunas las 179 charlas que se dictaron durante la conferencia, cada una de ellas aportaron diferentes perspectivas y aspiraciones de una comunidad que ha crecido de manera vertiginosa y exige cada vez más de un ecosistema que ha crecido a la par y en la dirección correcta. Este año se ha visto una clara tendencia por la productivización de modelos de machine learning. También está emergiendo una diversidad de entornos que buscan facilitar la interacción entre Científicos e Ingenieros de datos en durante todo el ciclo de vida de los productos basados en datos. Todo esto al tiempo que el ecosistema pretende integrarse con Deep Learning asumiendo el costo técnico que esto implica.\nEn definitiva, la edición de este año ha sido bastante productiva y marca el inicio de una nueva etapa para Spark, en la cual deja de ser una herramienta emergente para convertirse en un producto bien posicionado en el mercado. El cual se ha propuesto como misión la de llevar las capacidades analiticas de la academia al mundo industrial.\nLa próxima cita de la comunidad será el Spark Summit Europe 2017 que se realizará del 24 al 26 de Octubre de 2018 en Dublín. Allí vamos a contar nuestra experiencia de uso de Spark para la categorización de transferencias bancarias, esperamos poder verlos allí 😉\n\n"
    },
    {
      "title": "Una Nueva Interpretación Urbana a través del Análisis de la Actividad Comercial (Parte 2)",
      "url": "https://www.bbvadata.com/es/rediscovering-cities-credit-card-data-2/",
      "date": "25 septiembre, 2017",
      "content": "\n— Lectura de los resultados desde una perspectiva urbanística\nAntecedentes\nEn un artículo anterior hemos descrito la metodología, los modelos y herramientas analíticas y de representación empleados en el proyecto Urban Discovery, que hemos desarrollado junto con CARTO. En esta segunda parte ponemos el foco en la utilidad de los resultados para interpretar y mejorar la gestión del territorio a través nuevas fuentes de datos de alta resolución espacial y temporal. En concreto hemos trabajado con datos masivos de pagos con tarjeta, que permiten lecturas novedosas de lo que acontece en el plano comercial, algo que, como veremos, tiene un reflejo urbanístico muy marcado.\nSobre la organización territorial de las grandes ciudades: ¿qué son los distritos y barrios, cómo se delimitan y para qué se utilizan?\nCon el objetivo de descentralizar el gobierno de la ciudad y de optimizar la prestación de servicios públicos las grandes ciudades subdividen su término municipal en unidades territoriales menores. Así, en Ciudad de México se delimitaron 16 delegaciones en 1970; Madrid se organiza en 21 distritos desde 1988; y en Barcelona se definieron 10 distritos en 1984. Estas subdivisiones se delimitan según criterios históricos (los municipios absorbidos suelen pervivir como distritos una vez integrados) y prácticos: se pretende lograr una mínima homogeneidad demográfica y territorial, dentro de unos rangos. Hay que tener en cuenta que las unidades resultantes de esta primera división administrativa tienen a menudo una población comparable a la de muchas ciudades de tamaño medio (en México las delegaciones -que pasarán a denominarse alcaldías en 2018- abarcan unos 500.000 habitantes, mientras que los distritos de Madrid y Barcelona tienen una población promedio de unas 150.000 personas). Por lo tanto, se hace preciso un nivel aún más granular: en Madrid y Barcelona surgen así los barrios (de unos 20.000 habitantes), y en Ciudad de México las colonias (de unos 4.000 habitantes). La redefinición de estos ámbitos de gestión de segundo nivel es mucho más viva y frecuente, (aquí un ejemplo del proceso seguido y de los criterios aplicados), pero sin embargo siguen basándose en datos estáticos.\nHoy en día existen nuevas fuentes de información que describen el uso real que sus habitantes y visitantes hacen de las ciudades, y que deberían ser consideradas a la hora de delimitar distritos y barrios, algo que posteriormente condiciona múltiples aspectos de la vida de los ciudadanos, como por ejemplo la movilidad (en Madrid los barrios definen las zonas de aparcamiento regulado) o la escolarización (en Barcelona la asignación de colegios de educación infantil y primaria se basa en los distritos oficiales).\nEmplear nuevas fuentes de datos dinámicos proporciona una visión del territorio mejorada\nTodo el que vive o visita una gran ciudad difícilmente la abarca por completo: acaba reduciéndola a un entorno acotado conformado por las zonas en las que reside, trabaja, estudia, o acude en su tiempo libre por diversión. Estas actividades dejan una huella digital que teje relaciones entre áreas. En nuestro equipo llevamos trabajando más de 5 años con una de dichas fuentes de datos: los pagos con tarjeta bancaria, capaces de describir el uso que los ciudadanos hacen de gran parte de los servicios que la ciudad les presta a través del tejido comercial y de ocio,(ver Big Data to Enhance the Tourism in Mexico (en inglés) o Análisis de Influencia sobre el Comercio de las Medidas de Calmado de Tráfico, por ejemplo).\nSi bien se trata de una fuente de datos parcial, que describe tan solo uno de los muchos sistemas que componen la realidad urbana -la relación comercio y territorio- ésta conforma una de las dinámicas más importantes a la hora de entender una ciudad y su evolución, como veremos.\nEn esta ocasión hemos trabajado con un doble propósito:\n\nMedir el grado de cohesión territorial de tres grandes ciudades para obtener nuevas delimitaciones internas: áreas funcionales basadas en la actividad real de la gente que se desenvuelve por los ámbitos estudiados.\nCualificar las áreas funcionales identificadas.\n\nSobre el primero de los objetivos no me extenderé: como se describió en el anterior artículo, se ha logrado aplicando la ciencia de datos a una cantidad masiva de transacciones (más de 413 millones) para delimitar lo que hemos venido a denominar macrocomunidades -divisiones de primer nivel, comparables a los distritos o delegaciones- y comunidades, divisiones de segundo nivel, comparables a los barrios, y que sirven de unidad mínima a la hora de acometer el segundo de los objetivos: la cualificación.\nLa ciudad ideal frente a la ciudad real: métricas y descriptores\nLa ciudad ideal estaría conformada por barrios equilibrados desde todos los puntos de vista, lo que redundaría en una mayor autosuficiencia y en un decremento de las necesidades de movilidad y de logística de suministro:\n\nequilibrio espacial: la ciudad policéntrica, o que complementa las funciones de su núcleo principal con centralidades periféricas muestra flujos de movilidad mejor distribuidos que los puramente radiales.\nequilibrio en usos, por combinar en zonas más autosuficientes todas las funciones: la residencial, la de generación y mantenimiento de puestos de empleo, o la de provisión de servicios comerciales.\nequilibrio sociodemográfico: logrando diversidad suficiente por edades y género, por niveles educativos, y por renta entre su población.\nequilibrio en su configuración urbanística, en las dotaciones y equipamientos existentes, y en el nivel de atractivo, para que los visitantes se distribuyan de manera homogénea y se eviten congestiones en torno a los hitos turísticos, culturales y de ocio, y se mitiguen las presiones inmobiliarias en las zonas mejor dotadas o de una calidad estética o medioambiental diferencial.\n\nAunque en investigaciones previas se ha demostrado que ligeras modificaciones en los patrones de consumo harían alcanzable alguno de estos objetivos parciales en favor del reequilibrio urbano, en la ciudad real esta situación ideal antijerárquica, e igualitaria no se da. Las ciudades reales tienden a la segregación, y a la especialización de sus barrios, y ello deja una señal en los datos de actividad comercial que nos hemos esforzado en leer. Sin apriorismos, la información manejada ha resultado ser elocuente.\nPara ello hemos trabajado en tres niveles: en primer lugar hemos identificado variables o métricas básicas en la información tratada; en segundo lugar dichas variables nos han servido para conformar atributos de consumo como combinación de variables. Empleamos estos atributos a modo de descriptores de zona, y pueden agruparse según tres tipos:\n\nAtributos del tejido comercial y de la actividad registrada: densidad comercial global o densidad de un tipo de comercio específico (por ejemplo, comercio de proximidad, que cubre necesidades básicas), importe de la transacción media registrada en un área, etc.\nAtributos de perfil sociodemográfico de los consumidores: presencia de un determinado perfil de consumidor de forma predominante\nAtributos de huella temporal: identificación de patrones de concentración del gasto en determinados momentos.\n\nEn tercer y último lugar hemos combinado estos atributos para conformar etiquetas de zona que delimitan ámbitos homogéneos como sumatorio de comunidades -no necesariamente adyacentes- con características compartidas. Es en este punto en el que afloran las fuertes relaciones entre el pulso vital en el plano comercial, y las distintas tipologías que sirven para describir un área urbana: esta lectura nos ha llevado a unos resultados que tienen coherencia cronológica, espacial y funcional. Los nombres elegidos, sin embargo, se derivan ante todo de esta tercera característica -la función urbana- aunque es evidente la interrelación entre todas ellas: la disposición geográfica de un área está a menudo vinculada a su momento de desarrollo, distintas etapas conllevan diferentes tipologías constructivas y de uso del suelo, diferentes tipos de especialización, y ésta a su vez influye sobre el perfil de sus residentes sobre y sobre la conformación y actividad del tejido comercial.\nResultados: las tipologías urbanas identificadas\nBajo la perspectiva de sus patrones de consumo característicos hemos distinguido seis tipos básicos de ámbitos urbanos que estructuran la ciudad. Esta taxonomía ha resultado válida para zonificar las tres grandes urbes analizadas, y sería aplicable a los entornos urbanos (de una mínima escala y actividad transaccional) que se puedan estudiar en el futuro bajo este mismo prisma:\n\nZona 1 – Centro: se trata de áreas con alta concentración comercial, como resultado de un tejido urbano denso y con locales en planta baja que mantienen una alta ocupación y actividad. Existen hitos culturales (museos, librerías) La presencia de turistas (tanto nacionales como extranjeros) y de usuarios de tarjeta propensos a realizar gastos vinculados al ocio cultural es superior a la de otras zonas de la ciudad. Bajo esta etiqueta en Madrid se identifica la ciudad en la extensión que tenía aproximadamente en el año 1900, abarcando toda la zona interior a la primera ronda (cerca de Felipe IV), más Chamberí por el norte y Arganzuela por el sur, pero con el ensanche de Castro aún sin completar hacia el este. En Barcelona esta zona incluye el Barrio Gótico, pero lo desborda y abarca la primera parte del ensanche de Cerdá, a ambos lados del eje de Grácia. En todo caso ambas zonas centro tienen una importancia relativa alta respecto al conjunto de ambas ciudades, efecto que no se da en Ciudad de México: lo que allí destaca como centro -coincidente con la extensión de la ciudad hace cien años- ha quedado totalmente empequeñecido por los crecimientos posteriores, que tienen otras características y otra huella digital, como veremos.\nZona 2 – Barrio Acomodado: se trata de áreas con alta densidad comercial, al igual que las zonas centro -a las que suelen ser contiguas en el espacio y en el momento histórico de su desarrollo, pues las conforman áreas edificadas en la siguiente etapa, que abarca hasta 1940- pero se diferencian de la zona centro por una mayor presencia relativa de clientes con alto poder adquisitivo, y por acoger menos hitos de ocio cultural y menos comercios de moda activos (esto último ante todo en Ciudad de México), aunque se mantiene una presencia comparativamente alta de foráneos.\nZona 3 – Barrio Popular: tras debatirlo mucho hemos denominado así a las zonas desarrolladas en la etapa de mayor crecimiento de las ciudades estudiadas: la segunda mitad del siglo XX, momento en el que se produjo la llegada de una gran cantidad de población del medio rural a las grandes ciudades. Para hacer frente a esta demanda se llevaron a cabo nuevos crecimientos bajo parámetros racionalistas: se reemplaza la manzana cerrada con zócalo comercial por manzanas abiertas, a menudo con viviendas en planta baja. El resultado son áreas con baja densidad comercial y especial importancia del comercio de proximidad o de conveniencia (panaderías, ferreterías, farmacias, alimentación), con bajos ticket medios y poco gasto en fin de semana. En Madrid esta tipología urbana se ubica en la periferia intermedia (el arco entre M30 y M40), pero también dentro de la almendra central hay zonas con esta huella digital como Tetuán. En Barcelona estas zonas se concentran en el norte: Horta-Guinardó, Nou Barris, y en Ciudad de México es la demarcación territorial de Iztapalapa, la más poblada de todas, la que destaca con estos atributos.\nZona 4 – Centros de Trabajo: las zonas de oficinas o industriales se caracterizan por registrar una actividad extremadamente baja durante el fin de semana, y por tener un tejido comercial poco denso, conformado ante todo por bares y restaurantes que dan servicio a los empleados de los centros de trabajo. En Madrid la zona norte concentra la mayor parte de las oficinas -y los resultados así lo confirman- lo que provoca unos ciclos muy claros de movilidad, de sur a norte por las mañanas y de norte a sur por las tardes; queda como posible posterior análisis el estudiar más en detalle estos flujos, considerando como nodo de origen de las cadenas de transacciones el hexágono de residencia del usuario de tarjeta. en Ciudad de México destacan bajo esta etiqueta Miguel Hidalgo, Azcapotzalco, y Cuajimalpa de Morelos. En Barcelona sin embargo no destaca ninguna zona bajo esta combinación de atributos, lo cual en sí es positivo, pues denota una buena integración de los centros de trabajo en el entorno urbano, al que no logran imponer su ritmo horario; en Madrid ocurre esto mismo en AZCA y Castellana, donde hay múltiples oficinas, pero también una componente residencial y de servicios y ocio que mantiene activa la zona fuera del horario de oficinas.\nZona 5: Nuevos desarrollos: hemos denominado así a las áreas con las características de las zonas residenciales (baja densidad comercial, sin presencia de turistas) pero con una edad media menor, y gasto concentrado en el fin de semana. Urbanísticamente coincide con los últimos crecimientos de las ciudades, en los que residen familias que entre semana no realizan demasiada vida de barrio, concentrándose el consumo en el fin de semana. En Madrid destacan los crecimientos de periferia, la mayoría más allá del anillo de la M40, pero en ocasiones (Las Tabas, Sanchinarro) esta etiqueta queda eclipsada por la de centros de trabajo, característica en realidad no excluyente sino complementaria. En Barcelona esta etiqueta destaca la zona de la Villa Olímpica, pero también la parte alta de la ciudad. En Ciudad de México este tipo de áreas ocupan extensos ámbitos en los cuatro puntos cardinales.\nZona 6: Centros comerciales: áreas con muy baja densidad comercial y un tipo de consumo concentrado en grandes superficies (hipermercados, tiendas multiproducto, grandes almacenes…). La tipología urbanística ligada a esta huella digital es muy heterogénea: en Madrid encontramos áreas de dispersión como Aravaca, pero también otras de muy alta densidad residencial, como el entorno más próximo a La Vaguada, junto con zonas de densidad intermedia o en consolidación, como La Gavia. En Barcelona destaca el barrio de 22@ (donde el centro comercial Diagonal Mar marca el ritmo, prevaleciendo su señal por encima del ritmo que marcan las oficinas de la zona). Finalmente, en Ciudad de México los centros comerciales a lo largo de la Avenida de Miguel Ángel Quevedo y Calzada del Hueso imprimen esta etiqueta a dichos entornos, recordemos en todo caso que hemos manejado atributos relativos, y que existirán centros comerciales con mayor actividad en otras áreas, coexistiendo con otro tipo de actividad y sin llegar a marcar el patrón de una zona.\n\n\nPor último destacamos que toda la información generada es descargable a través de la propia visualización, y reutilizable para posteriores estudios. Contribuimos de este modo con una base inicial sobre la que seguir edificando y generando estudios derivados. En el futuro será interesante observar las evoluciones temporales de los atributos descritos, lo que puede servir para arrojar luz sobre fenómenos tales como la gentrificación, elitización, turistificación, en todo caso será algo para lo que será necesario trabajar sobre una taxonomía de categorías más detallada.\nDe momento consideramos logrado el primer objetivo: probar que la fuente sobre la que hemos trabajado y las metodologías aplicadas pueden ayudar a leer dinámicas en nuestras ciudades bajo un nuevo prisma, pero la capacidad descriptiva de los datos de consumo va mucho más allá, y ganará una dimensión más transversal al ser combinados con otras fuentes.\nMás allá del valor académico o de investigación que este trabajo alberga, estos resultados pueden contribuir a sentar las bases de herramientas prácticas que faciliten decisiones de toda índole a ciudadanos, empresas y administraciones, como por ejemplo ayudar a la identificación de áreas gemelas en ciudades diferentes, algo que puede guiar la búsqueda de vivienda o la implantación de nuevos negocios en áreas favorables que maximicen las posibilidades de éxito. Los ejemplos son numerosos, y confiamos en la creatividad de la sociedad para proponer soluciones sobre esta base, contando con la disponibilidad de la fuente de datos de actividad comercial a través de los servicios Paystats expuestos en el portal BBVA API Market.\n\n"
    },
    {
      "title": "Una Nueva Interpretación Urbana a través del Análisis de la Actividad Comercial (Parte 1)",
      "url": "https://www.bbvadata.com/es/rediscovering-cities-credit-card-data/",
      "date": "25 septiembre, 2017",
      "content": "\n— Metodología y Proceso Analítico\nLas divisiones oficiales de las ciudades, como distritos y barrios, están siendo utilizadas no sólo para gobernar las ciudades sino también para describir el comportamiento de los ciudadanos. De hecho, las estadísticas oficiales y el open data utilizan estas divisiones para agregar la información que publican. Sin embargo, estas divisiones podrían no ser la mejor opción para estos propósitos ya que en la actualidad los ciudadanos no las tienen en cuenta a la hora de moverse por la ciudad. \nLa iniciativa Urban Analytics de BBVA Data & Analytics, en colaboración con CARTO, presenta una nueva data story que muestra cómo se puede aplicar la ciencia de datos, sobre dato anonimizado y agregado, para identificar áreas funcionales, describir dinámicas y establecer comparaciones entre tres grandes ciudades como Madrid, Barcelona y Ciudad de México.\nContexto\nLos gobiernos organizan las ciudades en distritos y barrios. Los servicios de correos usan códigos postales. Pero, ¿de verdad estas divisiones sirven para entender la actividad de sus habitantes y visitantes? Al fin y al cabo fueron creadas hace tiempo para múltiples propósitos y raramente se han actualizado aunque los hábitos de los habitantes sí que hayan cambiado. Teniendo en cuenta todo esto, en este proyecto hemos comprobado cómo quedarían divididas las ciudades si se siguiese un criterio de movilidad. ¿Permanecería alguna división oficial sin cambio? ¿Cuál sería el número óptimo de divisiones? Además, también hemos caracterizado las divisiones obtenidas basándonos en patrones de consumo observados en los datos transaccionales.\nFuentes de datos y modelo analítico\nRed de conexiones: El primer paso de este proyecto ha sido establecer relaciones entre distintas zonas de las ciudades analizadas a partir de transacciones con tarjeta. En este caso, se optó por describir patrones de movilidad económica identificando consumidores compartidos entre distintas partes de la ciudad. Para ello, cada ciudad fue completamente dividida en celdas hexagonales de 200 metros de lado. Una vez hecho esto, cada vez que una misma tarjeta es utilizada en dos comercios pertenecientes a distintas celdas en un periodo menor a 3 horas se crea una conexión entre estas dos celdas. Esta operación se aplica a todas las transacciones realizadas en cada ciudad durante un año para asegurar que los patrones de consumo estacionales son incluidos. Cada una de estas conexiones recibe el nombre de copago; aquí puedes ver las redes de copagos correspondientes a las ciudades de Madrid, Barcelona y CDMX.\n\n \nIdentificación de áreas funcionales: Cada una de estas redes pesadas y no dirigidas contiene una gran cantidad de información sobre la manera en la que los consumidores se desenvuelven por la ciudad. Por eso, analizar su estructura permite encontrar una nueva manera de dividir la ciudad. En particular, al ejecutar distintos algoritmos de detección de comunidades se obtienen conjuntos de hexágonos fuertemente conectados. En este caso se han aplicado algoritmos basados en distintos enfoques como caminos aleatorios,  autovectores principales, optimización de modularidad o propagación de etiquetas para comprobar qué resultados se obtenían.\nTodos los algoritmos utilizados comparten la característica de no necesitar como parámetro el número de comunidades que deben devolver ya que la comparativa con el número de divisiones oficiales resulta interesante. Bajo estas condiciones la obtención de clusters similares con distintos algoritmos es una prueba de consistencia que ha servido para seleccionar un algoritmo voraz. Dado que sus resultados son similares a los de otros algoritmos, su elección se debe al menor coste computacional respecto a estos. Este aspecto es especialmente importante para hacer posible su ejecución sobre la red de de Ciudad de México ya que ésta contiene más de 94 millones de copagos. Además, la selección de un mismo algoritmo para todas las ciudades hace posible las comparaciones entre ellas, incluyendo la identificación de áreas gemelas entre ciudades.\nLos clusters resultantes, denominados macrocomunidades, tienen un tamaño similar al de los distritos oficiales pero, como cabía esperar, aparecen interesantes diferencias entre ellos. Es este tamaño el que hace que las macrocomunidades sean demasiado heterogéneas para ser descritas mediante atributos de consumo. Por esa razón, el mismo algoritmo de detección de comunidades se aplica a cada macrocomunidad (a la parte de la red de copagos correspondiente) para obtener una división más detallada del territorio. Estas nuevas particiones se denominan comunidades y, esta vez sí, permiten ser caracterizadas mediante atributos de consumo. Vale la pena comentar que los shapefiles correspondientes tanto a macrocomunidades como a comunidades están disponibles para su descarga en la página web del proyecto.\nCaracterización de comunidades: Para definir los atributos de consumo que permiten caracterizar cada comunidad se han utilizado todas las transacciones realizadas en la ciudad durante un año. A partir de los datos asociados a las transacciones se construyen más de 25 variables relacionadas con los comercios, los consumidores y el momento de la transacción. Para cada variable, los hexágonos reciben un valor discreto (bajo, medio, alto) que hace referencia a su desempeño comparado con el resto de la ciudad. Por ejemplo, la variable “Turismo nacional” mide el gasto realizado por turistas nacionales en cada hexágono. Una vez calculada, los hexágonos se ordenan y los valores discretos se asignan: alto para los hexágonos en el cuartil superior, bajo para los del cuartil inferior y medio para el resto.\nEn vez de utilizar directamente estos valores como caracterización de cada hexágono, el objetivo perseguido era encontrar qué características diferencian cada comunidad del resto de la ciudad. Para ello, se ha construido un modelo de clasificación para cada comunidad. Supongamos que queremos saber qué variables distinguen a los hexágonos de la  comunidad C de los del resto de la ciudad. El conjunto de entrenamiento estará compuesto por todos los datasets de la ciudad y las variables asignadas anteriormente. La variable objetivo y del modelo de clasificación tendrá un valor binario que informará de si el hexágono pertenece a la comunidad C o no.\nComo el objetivo de construir los modelos de clasificación es identificar qué variables son significativas en cada caso, no solo no se elimina el overfitting sino que se fomenta. Por esa razón, y dado que todos los conjuntos de datos están desbalanceados (el número de hexágonos que pertenece a la comunidad es mucho menor que el de los que no pertenecen) se aplica una técnica de over-sampling aleatorio básica para balancear la muestra.\nUna vez que los conjuntos de entrenamiento están preparados se entrena un modelo de regresión logística y un árbol de decisión para cada comunidad cuyos valores de significatividad de cada variable nos sirven para identificar aquellas que distinguen cada comunidad del resto de la ciudad.\nEl último paso del proceso de caracterización consiste en combinar variables para crear los 17 atributos utilizados en la aplicación. Al combinar variables se obtienen atributos que contienen mucha más información. Por ejemplo, el atributo “Ocio cultural” no sólo hace referencia a la existencia de museos y galerías de arte en una determinada zona sino que también informa de que la zona atrae a consumidores que gastan más que la media en cultura. Otro buen ejemplo es el atributo de “Área residencial”. Aunque no se ha utilizado ningún tipo de información adicional, a través de las transacciones se pueden identificar zonas sin actividad en horario laboral, cuyo tejido comercial es limitado y sin presencia de turistas y asignarles este atributo. Así, el atributo “Área residencial” nace de combinar estas tres variables.\n \nLa herramienta\nAdemás de consultar las divisiones resultantes del proceso de detección de comunidades, los usuarios pueden manejar una herramienta interactiva que les permitirá seleccionar aquellos atributos en los que están más interesados, combinarlos y comprobar qué comunidades cumplen los requisitos. Por ejemplo, alguien que esté buscando alojamiento podrá buscar zonas donde se desenvuelvan personas de su rango de edad y donde pueda encontrar comercios de proximidad para centrar su búsqueda en una determinada zona.\nPor cada atributo, la herramienta muestra dos valores: SI y NO. Los usuarios pueden seleccionar uno de ellos para filtrar el mapa y dejar iluminadas sólo aquellas comunidades que tienen ese valor en el atributo seleccionado. El botón de autostyle (gota de agua sobre cada atributo) permite previsualizar qué comunidades tienen asignado cada valor del atributo sin necesidad de realizar el filtrado. Además, más de un atributo puede ser filtrado a la vez, lo que permite que los usuarios los combinen y generen búsquedas más complejas.\n \n\nPor último, los usuarios pueden crear sus propias etiquetas para describir la ciudad. Una vez terminado el proceso de filtrado, pueden asignar un nombre al área que permanece iluminada utilizando el botón de la bandera situado en la esquina superior izquierda de la pantalla. Este feedback servirá para comprobar cómo es vista cada ciudad por sus habitantes. Además, aquellas usuarios que utilicen la herramienta de etiquetado podrán saltar a otras ciudades comprobar qué áreas se corresponden con su filtrado, encontrando así áreas gemelas entre ciudades.\nComo conclusión, este proyecto muestra cómo se pueden aplicar técnicas de ciencia de datos (análisis de redes y algoritmos de clasificación en este caso) a datos transaccionales geoposicionados para tomar el pulso a grandes ciudades. Además, en este proyecto se ha puesto énfasis en desarrollar una web abierta que no sólo permita a los usuarios (individuos, empresas o la propia administración) navegar por los resultados, sino que además puedan descargar los mapas de divisiones y atributos generados. Desde BBVA Data & Analytics esperamos que estos resultados puedan ser útiles para múltiples aplicaciones.\n\n"
    },
    {
      "title": "Mejorando la Experiencia del Cliente con Modelos de Previsión",
      "url": "https://www.bbvadata.com/es/improving-customer-experience-forecasting-models/",
      "date": "19 septiembre, 2017",
      "content": "\nLas tarjetas de débito y crédito han facilitado la vida de muchas personas. Nos han permitido ahorrar tiempo y dolores de cabeza, y nos han ayudado a financiar bienes que superan nuestro poder adquisitivo mensual. A veces, también han creado situaciones en las que los pagos imprevistos se han producido en un momento inesperado con un saldo bajo y el pago pendiente implica una penalización.\nPara resolver este problema, BBVA ha lanzado recientemente un Calendario Financiero que forma parte de sus aplicaciones web y móviles. Llamado “Expense and Income Forecast”, ayuda a los clientes a ganar control sobre su cuenta al proporcionar estimaciones de futuros ingresos y gastos recurrentes. Esta función ya está disponible como parte de la gestión de Personal Finance Manager (PFM) de BBVA denominada “My Day to Day”, que actualmente utilizan un millón de clientes al mes.\n \n\n \nEl algoritmo central, su motor de predicción, fue desarrollado por BBVA Data & Analytics utilizando herramientas Big Data como Apache Spark e implementando una combinación de algoritmos de Aprendizaje Automático para la predicción de series cronológicas. Es otro ejemplo más de cómo BBVA Data & Analytics aplica la Ciencia de Datos para mejorar la vida financiera de los clientes de BBVA, proporcionando información útil para la toma de decisiones.\nLos clientes pueden ver dos tipos de previsiones. Por un lado, la cantidad total de gastos e ingresos esperados en un conjunto de categorías financieras (como comestibles, educación, o tiempo libre) para el mes en curso. Por otra parte, para las operaciones que presentan un fuerte patrón temporal recurrente, éstas se asocian con datos tentativos y se muestran en un calendario. Por lo tanto, los clientes pueden hacerse una idea de cuándo recibirán en el futuro un pago o un débito directo de la compañía eléctrica o una hipoteca, y pueden prepararse para otros pagos extraordinarios que pueden afectar el saldo de su cuenta.\n \n\n\n \nMarco Analítico y Fuentes de Datos\nLa nueva función lee los datos históricos de las transacciones de la cuenta para identificar categorías de ingresos y pagos recurrentes y determinar una cantidad y fecha aproximada para los próximos 2 meses, combinando métodos de análisis de la serie temporal y un conjunto de modelos de aprendizaje automático.\nEste modelo genera alrededor de 30 millones de predicciones al mes, con una precisión del 85%. Los importes previstos difieren en un 12% o menos con el valor real para la mitad de los casos. Para cada predicción, el modelo proporciona una “etiqueta de confianza” que permite a los clientes saber qué cantidad de predicciones se espera que sean más precisas que otras.\nEsta función es solo un paso más en un proceso de investigación continuo, algunos resultados previos de esta investigación se mostraron en un poster titulado Evaluating uncertainty scores for deep regression networks in financial short time series forecasting que fue presentado en el Workshop on Machine Learning for Spatiotemporal Forecasting como parte de NIPS 2016\n \nResultados\nTras algunos meses de pruebas con los empleados de BBVA, el motor está abierto a todos los clientes de BBVA en España. Este es un ejemplo del aprendizaje automático en la etapa de producción, también es un caso aplicado a la Tecnología Financiera (Fintech) para mejorar la experiencia entre personas y bancos.\n \n\n"
    },
    {
      "title": "Lo que vimos en la Conferencia RecSys 2017",
      "url": "https://www.bbvadata.com/es/saw-recsys-2017-conference/",
      "date": "14 septiembre, 2017",
      "content": "\nRecSys, que tuvo lugar en Como el pasado 27 de agosto de 2017, es una de las mayores jornadas académicas sobre los Sistemas de Recomendación (RecSys) y ha alcanzado este año su undécima edición con un récord histórico de asistentes (627), lo que demuestra la creciente importancia de los Sistemas de Recomendación en la actual agenda digital.\nEn BBVA Data & Analytics, creemos que los Sistemas de Recomendación pueden facilitar la vida de nuestros clientes y hemos estado explorando cómo los Sistemas de Recomendación podrían realizar recomendaciones más adecuadas para los planes de pensiones a los clientes de BBVA.\nRecSys 2017: Un breve resumen\nLas undécimas conferencias de RecSys tenían como objetivo describir los recientes desarrollos en el campo, tendencias y desafíos en la provisión de componentes de recomendación en una variedad de contextos de aplicación innovadores. Además de la vía técnica principal, RecSys 2017 contó con ponentes de otras disciplinas, como George Loewenstein (Universidad Carnegie Mellon) que subrayó que la curiosidad está enormemente infrautilizada en los sistemas de recomendación.\nLa conferencia también contó con tutoriales sobre los últimos adelantos en este campo, talleres, sesiones especiales para socios industriales de diferentes sectores como viajes, juegos de azar e industrias de la moda y un simposio de doctorado. Cabe destacar que, por primera vez en su historia, la conferencia estuvo precedida por una Escuela de Verano de cinco días que reunió a expertos en la materia, cubriendo una amplia gama de temas como “Herramientas y Replicación de la Investigación RS” (Alan Said & Alejandro Bellogín),”Escalable RS en la Industria” (Xavier Amatriain) o “Evaluación de la investigación RS” (Guy Shani).\nEn este artículo se resumen los principales aprendizajes de RecSys 2017:\n“La exactitud no importa, lo que importa es el impacto” (J. Kolen)\n \nLa evaluación de los experimentos offline está aún lejos del objetivo de la aplicación final. Como han afirmado varios ponentes de la industria, todavía existe una brecha entre el mundo académico y la industria.\n“Sigue existiendo una brecha entre la mayoría de los modelos de filtrado colaborativo y el objetivo real del sistema de recomendación.”\n — Noam Koenigstein, Microsoft\nSiguiendo este tema, he recordado que un asunto similar fue discutido en la RecSys 2016.\nAnalizando la métrica más común de evaluación utilizada en RecSys 2017 en los principales trabajos de la conferencia, es posible ver que todavía hay publicaciones que optimizan su modelo para la predicción de la tarea de clasificación (es decir, RMSE, MAE y MSE), que está lejos de la mayoría de los escenarios del mundo real.\n\nEn la gráfica anterior, las métricas se han agrupado en “Otros” si se utilizan en menos de 3 publicaciones, por ejemplo tasa de aciertos, cobertura de artículos, puntuación G, etc.\nDeep Learning para Sistemas de Recomendación\n\nKaratzoglou y B. Hidasi proporcionaron una visión general completa de Deep Learning (Aprendizaje Profundo) y su aplicación en el dominio de los Sistemas de Recomendación. Su presentación mostró las técnicas más relevantes que se han utilizado hasta ahora para la recomendación, y proporcionó un interesante paralelismo entre la historia de la aviación y el aprendizaje profundo, ambos inspirados en la biología.\n\nLos panelistas destacaron algunas de las mejores prácticas, tales como: favorecer el código abierto, experimentar con conjuntos de datos públicos, no utilizar pequeños conjuntos de datos (no usar MovieLens 100k) ni trabajar en tareas irrelevantes como la predicción de la calificación. Su tutorial está disponible aquí.\nQué es lo siguiente a la Conferencia RecSys\nEl panel plenario, compuesto por J. Konstan, X. Amatriain, P. Brusilovsky y G. Karypis, sugirió varios puntos sobre la futura dirección de la conferencia #RecSys:\n\nEl conocimiento debe estar abierto a todos: bases de datos abiertas, código abierto/iniciativas y publicación abierta.\n\nSiguiendo el punto de conocimiento abierto comprobé cuántos papers comparten su código en RecSys 2017, obteniendo 8 de 46 (aproximadamente 18%). No está mal, pero todavía tenemos que empujar más en esta dirección!\n\n\nmantener la innovación dentro de la conferencia juzgando un artículo no sólo por su novedad, diversidad y serendipia\ndiversidad de género\nética\n\nPapers destacados\n\nGetting Deep Recommenders Fit: Bloom Embeddings for Sparse Binary Input/Output Networks [J.Serra et al.]\nPersonalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks [M.Quadrana et al.]\nA Gradient-based Adaptive Learning Framework for Efficient Personal Recommendation [Y.Ning et al.]\n\nNos vemos el año que viene en Vancouver!\nNotas\nPor favor, póngase en contacto conmigo sobre cualquier error en el resumen o la atribución.\n \n \n\n"
    },
    {
      "title": "Aprendiendo de las Arritmias Cardíacas para Mejorar la Satisfacción del Cliente — Trabajando con Datos Desbalanceados",
      "url": "https://www.bbvadata.com/es/learning-cardiac-arrhythmias-improve-customer-satisfaction-working-imbalanced-data/",
      "date": "12 septiembre, 2017",
      "content": "\nAntecedentes\nLa satisfacción del cliente es un indicador complejo. Puede cambiar después de cada interacción, puede ser influenciada por factores externos y puede conducir a múltiples resultados. La insatisfacción acumulativa puede llevar al cliente a terminar la relación con el proveedor de servicios y, sin embargo, es difícil de medir con métodos tradicionales como las encuestas de satisfacción del cliente. Los algoritmos de Machine Learning ofrecen hoy en día la posibilidad de utilizar datos de diferentes fuentes para estimar la satisfacción del cliente.\nLos avances recientes en el tratamiento de las arritmias potencialmente mortales ofrecen soluciones interesantes para estimar la satisfacción del cliente. Las arritmias cardíacas son cambios en la secuencia normal de impulsos eléctricos en el corazón. Las arritmias graves pueden causar un paro cardíaco y ser causa de muerte. La única manera efectiva de tratar estas arritmias letales es la aplicación de un choque de desfibrilación eléctrica de alta energía utilizando un desfibrilador externo automatizado (AED, por sus siglas en inglés), que incluye un algoritmo de asesoramiento de choque que analiza el electrocardiograma (ECG, por sus siglas en inglés) y emite un choque eléctrico si se detectan arritmias letales (llamadas “impactantes”). Este algoritmo procesa datos en tiempo real del corazón y calcula la intensidad del electroshock necesario para revertir la arritmia.\nLa metodología de Machine Learning aplicada en la detección de arritmias potencialmente mortales también puede ser utilizada para identificar clientes insatisfechos, utilizando diferentes tipos de datos.\nMarco Analítico y Fuentes de Datos\nEl desarrollo de un detector de arritmias impactantes en los AED mediante el uso de técnicas de Machine Learning requiere trabajar con conjuntos de datos desequilibrados en los que los algoritmos de clasificación estándar están sesgados a la clase mayoritaria, lo que compromete su rendimiento.\nHay varias maneras de tratar los datos desbalanceados en la clasificación:\n\nMuestreo de datos, cuyo objetivo es reducir (submuestreo) la clase mayoritaria, o aumentar la clase minoritaria (sobremuestreo o sintetizando nuevas muestras).\nAprendizaje sensible a los costes, introduciendo un mayor coste de penalización para los errores de clasificación errónea de la clase minoritaria.\nUtilizar métricas de evaluación singulares. La precisión no debe ser la métrica que guíe el proceso de aprendizaje de los algoritmos, sino la tasa de error equilibrada (BER) o la puntuación F1.\n\nEn el caso de las arritmias, la métrica BER se puede utilizar para establecer parámetros libres de algoritmos de Machine Learning, por varias razones: i) se define como un equilibrio entre la sensibilidad (Se) y la especialidad (Sp).\nBER = 1 – 0.5*(Se + Sp),\nque son métricas clave en entornos de diagnóstico médico; ii) es fácil de calcular; y iii) proporciona un buen rendimiento resultante. Además, será necesario introducir un mayor coste de penalización para la clase minoritaria.\nResultados\nMediante el uso de las métricas BER para evaluar el rendimiento de los algoritmos de Machine Learning, es posible i) proporcionar un detector de arritmia potencialmente mortal robusto; y ii) identificar qué parámetros del ECG eran más importantes para la detección de ritmos de choque en los EADs.\nEn BBVA, esta metodología se está probando para otro tipo de escenarios, como el de Quality by Behavioral Analytics (Calidad por Análisis de Comportamiento). El objetivo es obtener una puntuación de satisfacción del cliente basada en su comportamiento. Los atributos de entrada incluyen el perfil del cliente, la sociodemografía, el comportamiento digital, las transacciones realizadas, los productos de propiedad o las reclamaciones.\nPara probar la validez del modelo, entrenamos un modelo de desgaste asumiendo que los clientes insatisfechos son los que abandonan el banco. Esto constituye un conjunto de datos desbalanceado (p+1 ≈ 3%), y la metodología utilizada para el estudio de las arritmias demostró resultados prometedores. Alcanzamos valores Se y Sp superiores al 80% y un área bajo la curva ROC de 0,91.\n\n"
    },
    {
      "title": "Nuestra Lista de Lectura sobre Machine Learning para Diseñadores",
      "url": "https://www.bbvadata.com/es/reading-list-machine-learning-designers/",
      "date": "6 septiembre, 2017",
      "content": "\nEn BBVA Data & Analytics continuamente colaboramos con los equipos de diseño de BBVA para crear experiencias de usuario basadas en técnicas de aprendizaje automático (por ejemplo, modelos predictivos, sistemas de recomendación). Documentamos esa práctica interdisciplinaria en el artículo diseño de experiencias en la era del machine learning, que luego condujo a una contribución en colaboración con Neal Lathia de Skyscanner en el Simposio de Primavera de la AAAI 2017 sobre el diseño de la experiencia del usuario de sistemas de aprendizaje automático. Nuestro informe técnico titulado Cuando los diseñadores de experiencia de usuario se asocia con los científicos de datos se centra tanto en la creación de experiencias con algoritmos de aprendizaje como en describir cómo los métodos que los diseñadores y los científicos de datos emplean desafían sus colaboraciones.\nComo el interés en el tema ha crecido internamente en BBVA, soy parte de un pequeño grupo de científicos de datos y diseñadores que se han unido durante el verano para realizar un curso intensivo en Machine Learning para diseñadores. El material se basa en nuestras experiencias comunes mezcladas con la literatura que surgió recientemente. He aquí una breve lista de lecturas – a partir de septiembre de 2017 – de textos que nos inspiran:\n \nMachine Learning para Diseñadores, de Patrick Hebron\nUn libro con descripciones técnicas de cómo las técnicas de aprendizaje automático (por ejemplo, el procesamiento del lenguaje natural, el reconocimiento de imágenes, la personalización del contenido y la predicción del comportamiento) pueden afectar el diseño de la aplicación.\nDiseño de Experiencias en la Era del Machine Learning, de Fabien Girardin\nNuestra experiencia práctica y visión en BBVA Data & Analytics de crear experiencias con algoritmos de aprendizaje.\nCuando los Diseñadores de Experiencia de Usuario se asocian con los Científicos de Datos, de Fabien Girardin y Neal Lathia\nUn artículo científico que presenta una serie de puntos de contacto y principios que las asociaciones entre diseñadores y científicos de datos pueden considerar para las relaciones productivas.\nMachine Learning para Gerentes de Producto, de Neal Lathia\nUn resumen enfocado en productos de machine learning, con una lista particularmente útil de términos técnicos de lo que los productos están tratando de hacer: categorización, recomendación, clasificación, regresión, agrupación, detección de anomalías.\nLa Guía PM Paso a Paso para Construir Productos Basados en Machine Learning, de Yael Gavish\nUna guía sobre Machine Learning para gerentes de producto que contiene en particular un capítulo titulado El Aprendizaje Automático es un problema de UX (Experiencia de Usuario) muy importante.\nMachine Learning Centrado en Humanos, de Josh Lovejoy y Jess Holbrook\nEn lugar de ver el aprendizaje automático puramente como una tecnología, ¿y si lo imaginamos como un material con el que diseñar? La respuesta con una lista de 7 pasos para mantenerse enfocado en el usuario al diseñar con Machine Learning.\nAplicaciones de Machine Learning para Diseñadores, de Lassi Liikkanen\nEste artículo ilustra el poder del aprendizaje automático a través de las aplicaciones de detección, predicción y generación. Da seis razones por las que el aprendizaje automático mejora los productos y servicios e introduce cuatro patrones de diseño relevantes para tales aplicaciones.\nData Jujitsu, de DJ Patil\nUn libro electrónico con – entre otras cosas – trucos de diseño que alista la ayuda de sus usuarios para tomar atajos alrededor de problemas resistentes.\nEl Trabajo de Diseño más Crucial del Futuro, de Caroline Sinders\n¿Etnógrafo de datos? Una posible práctica futura en la encrucijada del diseño y la ciencia de datos para determinar el sesgo en los algoritmos y ayudar a revelar sus ingredientes para que la gente pueda interpretar los resultados que producen.\nInnovación en el Diseño de UX: Desafíos para trabajar con el Aprendizaje Automático como Material de Diseño, de Graham Dove et al.\nUn artículo científico que concluye sobre áreas en las que una nueva investigación y un nuevo plan de estudios podrían ayudar a nuestra comunidad a desbloquear el poder del pensamiento de diseño para re-imaginar lo que el Machine Learning podría ser y podría hacer.\nQué hará el Machine Learning por el Diseño, de Eve Weinberg\nUna mirada sobre cómo el aprendizaje automático se convirtió en una herramienta de diseño.\nRepensar las Herramientas de Diseño en la Era del Machine Learning, de Patrick Hebron\nUna visión general detallada de cómo el aprendizaje automático puede ayudar a simplificar las herramientas de diseño sin limitar su expresividad, sin apartar el control creativo del diseñador.\nSi deseas profundizar en trabajos académicos en el dominio, echa un vistazo a los Procedimientos del Simposio de Primavera de la AAAI 2017 Diseñando la Experiencia del Usuario de Sistemas de Aprendizaje Automático. Para estudiantes de diseño, podemos sugerir la Lista de Lectura de Machine Learning de Kars Alfrink.\n \n \n\n"
    },
    {
      "title": "Limpieza y análisis exploratorio de datos con Apache Spark y Optimus",
      "url": "https://www.bbvadata.com/es/cleansing-exploratory-data-analysis-apache-spark-optimus/",
      "date": "5 septiembre, 2017",
      "content": "\nLos datos obsoletos, inexactos o duplicados no impulsarán soluciones optimizadas de datos. Cuando los datos son inexactos, las necesidades son más difíciles de rastrear y nutrir, y las ideas pueden ser defectuosas. Los datos en los que basamos una estrategia Big Data deben ser precisos, actualizados, lo más completos posible y no deben contener entradas duplicadas. Los datos limpios resultan en mejores decisiones.\nLimpiar datos es la tarea de ciencia de datos más lenta y menos divertida (hasta Optimus), pero una de las más importantes. Nadie puede iniciar un proyecto de ciencia de datos, un aprendizaje automático o una solución basada en datos sin estar seguro de que los datos que van a consumir están en su estado óptimo. Aunque existen varias soluciones de limpieza de datos, ninguna de ellas puede mantenerse al día con la aparición del Big Data, o son realmente difíciles de usar.\n\nEn este momento, más y más empresas están entrando (o al menos tratando de entrar) en la Gran Revolución de Datos y Aprendizaje Automático. Todos los enfoques basados en datos deben limpiar, resolver, normalizar y corregir los datos que se introducirán en los modelos que se quieren crear, y con Optimus estamos lanzando una aplicación fácil de usar, fácil de implementar en producción y de código abierto para limpiar y analizar los datos de forma paralela utilizando tecnologías de vanguardia, que pueden ser utilizados por pequeñas, medianas, grandes industrias o incluso las empresas que quieren crear soluciones de ciencia de datos y no tienen el dinero para pagar un montón de científicos de datos y crear su propio grupo para limpiar los datos que van a utilizar.\n\nOptimus es un framework  para la limpieza (y mucho más), el pre-procesamiento y el análisis exploratorio de datos de forma distribuida. Utiliza todo el poder de Apache Spark para hacerlo. Implementa varias herramientas útiles para el manejo, corrección y análisis de datos que harán tu vida mucho más fácil. La primera ventaja obvia sobre cualquier otra biblioteca pública de limpieza de datos es que funcionará en tu computadora portátil o en un gran cluster, y segundo, es asombrosamente fácil de instalar, usar y entender.\nEl grupo de BBVA Data & Analytics en México ha estado utilizando Optimus durante los últimos meses y hemos impulsado nuestro desempeño para la limpieza, exploración y análisis de nuestros datos por factor 10x.\nRequerimientos\n\nApache Spark 2.2.0\nPython 3.5\n\nInstalación (Windows, Mac & Linux)\nEn tu terminal solo escribe:\n \n\r\npip install optimuspyspark\r\n\n \nPara obtener una documentación completa sobre cómo usarla, visite nuestro repositorio GitHub:\nhttps://github.com/ironmussa/Optimus\nSi desea una demostración de lo que puede hacer Optimus para usted echa un vistazo a esta notebook de ejemplo:\nhttps://nbviewer.jupyter.org/github/ironmussa/Optimus/blob/master/examples/Optimus_Example.ipynb\nContribuidores\n\nProject Manager: Argenis León.\nDesarrolladores originales del proyecto: Andrea Rosales, Hugo Reyes, Alberto Bonsanto.\nDesarrollador principal: Favio Vázquez.\n\nLicencia\nApache 2.0 © Iron.\n\n\n"
    },
    {
      "title": "BBVA Data & Analytics Galardonada con el Premio al Proyecto Tecnológico del Año por “The Banker”",
      "url": "https://www.bbvadata.com/es/bbva-data-analytics-awarded-by-the-banker-technology-projects-of-the-year-award/",
      "date": "2 agosto, 2017",
      "content": "\nEntrevista con Francisco Javier López Peñalver y Jordi Nin\n \n\nJordi Nin — Científico de Datos en BBVA Data & Analytics\nJordi es Doctor en Ciencias de la Computación, con más de 75 publicaciones de investigación en revistas, conferencias y capítulos de libros. Le concedieron el doctorado excepcional. Premio del departamento de informática de la UAB. La tesis de Jordi se enfocó en utilizar varios métodos de aprendizaje automático para calcular el riesgo de divulgación de las encuestas estadísticas oficiales publicadas por los organismos nacionales.\n\n\n  \n\nFrancisco Javier López — Gerente de Proyecto de Datos en BBVA Data & Analytics\nJavier es licenciado en Ciencias de la Computación e Innovación. Ahora dirige los proyectos de análisis de riesgo y fraude en BBVA D&A para impulsar la transformación digital de BBVA. Tiene experiencia como ingeniero de I+D, consultor de innovación y gestor de fondos públicos internacionales. Entusiasta por explorar las tecnologías para crear nuevas oportunidades.\n \n \n \nFinancial Times ha premiado a BBVA Data & Analytics con el premio al Proyecto tecnológico del año por “The Banker“, en la categoría de Gestión del Riesgo. Francisco Javier López Peñalver (Gerente de Proyecto de Datos) y Jordi Nin (Científico de Datos) en BBVA Data & Analytics explican el inicio del proyecto RedeX, desarrollado en colaboración con BBVA y BEEVA entre junio de 2015 y mayo de 2017.\n \n¿Qué es RedeX y por qué fue premiado?\nEste proyecto tiene como objetivo mejorar el método de evaluación de riesgo para clientes nuevos y actuales durante el proceso de admisión de préstamos, y representa una solución significativa para los clientes que tienen un archivo de crédito delgado. BBVA recibe miles de solicitudes de préstamos cada mes y muchos se niegan porque no hay información sobre el historial de crédito del cliente; un problema frecuente especialmente entre las empresas jóvenes, las nuevas empresas y los extranjeros que han accedido recientemente al sistema financiero.\nBasándose en diferentes tipos de datos, tales como transferencias bancarias, boletines oficiales e interacciones con otras entidades, y sin tener que solicitar ninguna información adicional, RedeX estableció una fuente muy rica de información sobre el ecosistema de los clientes complementando la información proporcionada por los clientes cuando solicitan un nuevo producto de crédito.\n¿Cuáles fueron los objetivos de este proyecto?\nEl objetivo de este proyecto fue triple: obtener todos los datos necesarios para construir una red de relaciones; realizar analíticas relacionales distribuidas para extraer los atributos de los clientes más relevantes y las métricas más adecuadas para analizar el contexto del cliente; y construir una herramienta de autoservicio para permitir que los analistas de riesgo exploren los datos, lo visualicen y obtengan nuevas métricas de clientes.\n¿Cómo fue relevante la Ciencia de Datos para este proyecto?\nDebido a la naturaleza compleja y adaptativa del sistema financiero, la Teoría de Sistemas Complejos proporcionó una caja de herramientas matemáticas para analizar Sistemas Complejos a escala micro, meso y macro. Una red es una abstracción matemática que representa sistemas de entidades que interactúan. Naturalmente, las entidades son simbolizadas por los nodos y las interacciones por los bordes o conexiones entre ellos.\nLa teoría de la red y la simulación basada en agentes fueron útiles para entender el riesgo sistémico en el sistema bancario, lo que sugiere que el riesgo por defecto se propaga a través de una red siguiendo patrones muy complejos a escala macro pero también menciona que tales patrones pueden ser descompuestos a otros más simples cuando la red es analizada a nivel microscópico.\n¿Cómo es útil RedeX para los analistas de riesgo?\nRedeX ofrece una herramienta de autoservicio con una interfaz interactiva para ayudar a los analistas de riesgo a explorar el contexto de los clientes a través de gráficos y análisis de redes, evaluar el nivel de riesgo de un cliente y tomar decisiones sobre las solicitudes de préstamo. También puede aplicarse a otras áreas como la gestión del fraude, los sistemas de referencia o la experiencia del usuario en el análisis de canales digitales. La captura de pantalla siguiente demuestra lo fácil que es para cualquier analista de riesgo acceder a los datos de los ecosistemas de los clientes y utilizar esta información para evaluar la salud financiera de un cliente que tenía un delgado archivo de crédito (datos y nombres se han anonimizado):\n\n \n\n"
    },
    {
      "title": "Comprendiendo la redistribución económica a través de pagos con tarjeta de crédito",
      "url": "https://www.bbvadata.com/es/understanding-economic-redistribution-with-credit-card-payments/",
      "date": "25 julio, 2017",
      "content": "\nAntecedentes\nLas ciudades contienen barrios que se desempeñan económicamente mejor que otros. Pero en algunos casos las diferencias son extremas, como ocurre en Río de Janeiro, donde el ingreso promedio de los barrios ricos como Copacabana e Ipanema puede ser hasta 100% más alto que en las favelas, como lo indican las Naciones Unidas.\nLa desigualdad es un problema común para muchas ciudades de Europa, pero a menudo es difícil de medir y complejo de resolver. En un estudio reciente, el CSIC, IRSTEA y BBVA utilizaron datos de los pagos con tarjeta de crédito en Barcelona y Madrid para analizar la desigualdad y simular una posible solución basada en la modificación de los hábitos de los clientes.\nLa hipótesis de este estudio es que la desigualdad puede ser mitigada si los ciudadanos compran bienes en tiendas similares ubicadas en áreas más pobres de la ciudad. Para probar esta idea, el objetivo de este estudio es doble: analizar la desigualdad en una ciudad y simular cómo se puede mitigar.\nMarco Analítico y Fuentes de Datos\nCada vez que un titular de una tarjeta BBVA paga en una tienda, hay un registro que conecta a una persona con un negocio. Cada registro contiene información sobre la ubicación de la tienda, el origen del cliente, la categoría de la empresa, el tiempo y el importe de la compra. La muestra para este estudio incluye 150.000 transacciones de tarjetas de crédito anonimizadas en 95.000 negocios en Barcelona y Madrid por un período de dos años.\nEstas transacciones definen una red de enlaces cuyos nodos son, en un extremo, los centros de los códigos postales de residencia de los titulares y, por otro, los establecimientos donde se realizaron los pagos. Para cambiar el destino de un número mínimo de viajes de compras necesarios para reequilibrar los ingresos de los barrios, se usó un algoritmo estocástico de recableado. Este modelo iterativo mantuvo inalterado el dinero gastado en cada transacción y la categoría comercial en la que se realizó (para ello se clasifican las tiendas en 17 categorías diferentes), optimizando al mismo tiempo el tiempo de viaje, reduciéndolo.\nResultados\nEl diagnóstico inicial de la situación demuestra que el ingreso por negocio puede ser cinco veces mayor entre algunos barrios de Barcelona y Madrid, por ejemplo Sárria comparado con El Besós.\n\nEl análisis de este gráfico también demuestra cómo la gente viaja por la ciudad para ir de compras y sirve para probar lo que sucedería si los ciudadanos compraran en diferentes tiendas. Al encontrar empresas con la misma categoría que están más cerca del cliente, el algoritmo demostró que una reducción de la desigualdad del ingreso del 80% es factible solo modificando el 5% de todas las transacciones. Además, los resultados también demuestran que este cambio reduciría el tiempo de viaje en un 25%.\nMuchos factores coinciden cuando una persona decide dónde comprar un producto: La prosperidad de la búsqueda, el gusto por el lugar, el coste (principalmente) y el trato recibido del personal, por mencionar sólo unos pocos. Cambiar los hábitos de un cliente es un objetivo complejo, y este documento no podría cubrir cómo se puede lograr esto. Sin embargo, este estudio muestra que, aunque los cambios sean muy pequeños en nuestros patrones de movilidad y las opciones de consumo, podría tener un gran impacto en el estado actual de las ciudades. Al igual que las abejas que viven en una colmena, tenemos una visión muy parcial de nuestro entorno urbano, pero los dispositivos de TIC y los datos de tarjetas de crédito pueden ayudarnos a enfrentar los desafíos globales como la pobreza y la desigualdad.\nAutor: José Javier Ramasco\n\n"
    },
    {
      "title": "Big Data: “El volumen no es lo problemático sino la generación de valor en un entorno con tanta diversidad”",
      "url": "https://www.bbvadata.com/es/big-data-volume-is-not-the-problem-but-the-generation-of-value-in-an-environment-with-such-diversity/",
      "date": "18 julio, 2017",
      "content": "\n\nJon Ander Beracoechea — Co-CEO en BBVA Data & Analytics\nJon tiene un Doctorado en Ingeniería Eléctrica y ha publicado múltiples trabajos en el campo del procesamiento adaptativo de señales. Desde hace 9 años aplica técnicas avanzadas de análisis y machine learning en el sector bancario, como la prueba de estrés CCAR (Moody’s) o el precio de derivados (Banco Santander).\n\nEsta es la entrevista completa que le fue realizada a Jon Ander por el diario Expansión en Abril de 2017.\n\n¿Qué representan los datos para BBVA?\nLos datos son parte fundamental del ADN de BBVA. Los bancos y el mundo financiero en general está poco a poco entendiendo la importancia que los datos (y su tratamiento) tienen a la hora de proporcionar el mejor servicio posible a nuestros clientes. BBVA es pionero en este camino. Los datos y su explotación están presentes en muchísimas facetas del día a día del banco: en la creación de nuevos productos y servicios, automatizando procesos, ayudando a tomar mejores decisiones y, en general, haciendo la vida más sencilla a nuestros clientes. Este es un camino que no tiene vuelta atrás.\n¿Cuándo se creó la división BBVA Data & Analytics? ¿Cuáles son sus objetivos? ¿Cuántas personas trabajan en ella?\nBBVA Data & Analytics se crea en Mayo de 2014 (aunque se venía trabajando desde 2012) precisamente para acelerar este proceso. Su objetivo es transformar el banco en una organización basada en datos. Para ello colaboramos tanto con las Áreas de Negocio como con los equipos de Ingeniería desarrollando proyectos y soluciones de una casuística muy variada. Aunque nuestro foco está en aportar valor en el futuro cercano, tenemos también una vocación de exploración a más largo plazo donde tratamos de hacer “ciencia” y transformarla en Negocio. Tratamos de imaginarnos cómo será la banca en el futuro y cómo podemos adelantarnos a estos cambios. El equipo está formado por unas 50 personas y tenemos oficinas en Madrid, Barcelona y Ciudad de México.\n\n¿Qué volumen de datos gestiona el banco? ¿Cómo se puede extraer valor de tanta información?\nEl volumen de datos que manejamos fácilmente se sitúa en la banda de los petabytes. Ahora bien, el problema real no es tanto el volumen como la variedad. BBVA recoge información de muchos canales (oficinas, web, aplicaciones móviles…) y para muchos verticales (clientes particulares, PYMES, grandes empresas…). Los datos bancarios son extraordinariamente ricos y variados pero el verdadero reto es cómo domesticar esa diversidad informacional para aportar valor para nuestros clientes. Esto implica tener el talento adecuado, las plataformas tecnológicas necesarias y un diseño organizacional que facilite las interacciones entre los diferentes actores necesarios para llevar a cabo las distintas iniciativas.\nSe estima que, sólo en EEUU, hay una demanda potencial de entre 4 y 5 millones de empleos de analistas de datos en el próximo año. ¿Qué nuevos perfiles está incorporando BBVA? ¿Es difícil encontrar y atraer este talento?\nEl tipo de perfil que se está demandando no es nada fácil de encontrar. Se trata de profesionales con un perfil muy multidisciplinar. Nos movemos en la intersección entre las matemáticas y el desarrollo de software todo ello mezclado con una buena dosis de conocimiento de cómo funciona la banca. La mayor parte de las incorporaciones vienen de ámbitos muy técnicos (ingeniería, matemáticas, física…) pero con una mentalidad pragmática que busca desarrollar soluciones concretas y accionables. Dedicamos muchos esfuerzos para atraer y retener este tipo de talento. Esto implica crear una cultura que fomente la curiosidad y la capacidad de experimentar; un entorno flexible, dinámico y codo con codo con pares con los que contrastar resultados. Es un interesante mix entre el ambiente empresarial y el académico.\n¿Puede ponerme un ejemplo cotidiano de cómo un cliente puede beneficiarse de un buen análisis de big data?\nLo cierto es que no nos gusta hablar de “Big Data” ya que sentimos que es una palabra relativamente vacía de contenido. Nos vemos más como una unidad que usa inteligentemente los datos para aportar valor al negocio. Como decía antes, el volumen no es lo problemático sino la generación de valor en un entorno con tanta diversidad. Los clientes se benefician de muchas maneras: algunas son evidentes (Commerce 360 es una herramienta para comercios que les ayuda en su día a día) y otras no tanto: estudiamos por ejemplo la forma en la que los clientes navegan por bbva.es para mejorar y simplificar los procesos de tal forma que la experiencia sea lo más satisfactoria posible. En tales casos el cliente no es necesariamente consciente, pero “bajo el capó” muchos procesos inteligentes basados en datos le ayudan.\n\n"
    },
    {
      "title": "7 Desafíos para la Economía Global Digital",
      "url": "https://www.bbvadata.com/es/7-challenges-for-the-global-digital-economy/",
      "date": "11 julio, 2017",
      "content": "\nLa economía global digital continuará expandiéndose y diversificándose en 2017. Las nuevas tecnologías, los clientes con nuevos hábitos y las amenazas globales abrirán oportunidades y amenazas a las empresas de diferentes sectores. Este artículo resume algunas de ellas.\nNuevas tecnologías\nEn los próximos años, las tecnologías Blockchain se consolidarán y se aplicarán a diferentes e innovadores usos, aumentando la transparencia y la descentralización de la información. Los nuevos modelos desafiarán cómo las organizaciones almacenan y administran las transacciones de datos y permiten a las empresas basadas en internet desarrollar nuevos productos y servicios financieros. La expansión del Internet de las cosas creará miles de millones de fuentes de datos capaces de medir y combinar datos físicos y digitales para crear y expandir productos y servicios, como la autenticación biométrica. La computación cuántica abrirá nuevas oportunidades para una economía basada en tiempo real y los dispositivos móviles tendrán poder computacional.\nNuevos competidores\nDespués de una temprana y atomizada fase de fintechs entrando en el mercado financiero, nuevos grandes competidores formarán bancos digitales y desafiarán a las industrias convencionales. Los fintechs más pequeños se especializarán probablemente en sectores específicos y concederán el espacio a las grandes corporaciones tales como Apple, Google y Facebook que se centrarán en pagos en línea y servicios financieros generales a sus clientes.\nNuevas regulaciones\nEspecialmente en Europa, las nuevas regulaciones, como la segunda versión de la Directiva sobre servicios de pago (PSD2) y la Directiva sobre protección de datos generales, tienen por objeto transformar la industria financiera y estimular la competencia en el sector financiero y proporcionar más seguridad contra el fraude. En un mercado más competitivo, diversificado y abierto, las empresas tendrán dificultades para ofrecer servicios a precios más bajos.\nNuevos consumidores\nLos Millennials y las siguientes generaciones de nativos digitales enfrentarán los desafíos digitales actuales de manera diferente. La robotización de la economía y nuevas medidas como el ingreso universal se convierten en realidad. Los ciudadanos pasan menos tiempo trabajando y aumentan su capacidad de consumo. El debate sobre la privacidad entrará en una nueva fase y los usuarios individuales contarán con nuevos recursos para explotar sus datos personales, como los armarios de datos personales. Por un lado, los individuos tendrán menos miedo de intercambiar sus datos, y los recopiladores de datos tendrán que pagar más por los datos personales que puedan recopilar.\nNuevos modelos de negocio\nUn mundo sin dinero en efectivo se convertirá en una realidad pronto. Todas las transacciones digitales ayudarán a las empresas a obtener una imagen completa de su mercado y entender más claramente las oportunidades del mismo. Un mundo completamente digital hará que los servicios financieros sean más transparentes y accesibles, y creará oportunidades para una explosión multimodal como la realidad aumentada, permitirá a los clientes analizar el valor de un edificio o calcular el coste de una hipoteca para un coche específico. Los mercados de datos permitirán a los nuevos negocios adquirir datos que no producen y generan nuevos productos y servicios. El crowdfunding dejará espacio para crowdlending.\nNuevas amenazas globales\nLas nuevas amenazas globales continuarán expandiéndose y transformando la economía. La economía compartida encontrará un mejor marco legal y seguirá avanzando. El hackeo masivo obligará a los países y las empresas a invertir fuertemente en la seguridad y los sistemas políticos sufrirán una crisis recurrente derivada de la persistente crisis de seguridad.\n \n\n"
    },
    {
      "title": "Aprendizaje por Refuerzo para la Optimización de la Estrategia de Precios",
      "url": "https://www.bbvadata.com/es/reinforcement-learning-for-pricing-strategy-optimization/",
      "date": "4 julio, 2017",
      "content": "\nDarle la vuelta a una tortilla es un movimiento complejo que los humanos necesitan aprender a través de la experimentación y la repetición. Entonces, ¿cómo podría un robot aprender a dar la vuelta a una tortilla? Imaginemos que hay un brazo robotizado capaz de realizar los mismos movimientos que un brazo humano. La mano sostiene una sartén, y la masa está cocida y tiene la forma de tortilla perfecta.\nImaginemos también que somos capaces de escribir el código necesario para hacer que el brazo robotizado mueva la mano de una manera que podría hacer que la tortilla voltee en el aire y caiga de nuevo sobre la sartén. La consecuencia más probable de nuestro primer intento posiblemente sería un fracaso.\nUn humano se detendría y pensaría lo que salió mal. ¿Fue el movimiento demasiado fuerte? ¿Muy débil? ¿Debería haber esperado a que la tortilla caiga más a la derecha o a la izquierda? Cuando pensamos en las posibles consecuencias de nuestros movimientos y vemos el resultado, aprendemos. Y también lo hacen las máquinas.\n\n \nUn equipo dirigido por Petar Kormushev utilizó las tecnologías de machine learning para entrenar a un robot de Barret Wam para dar vuelta a las tortillas a través del aprendizaje por refuerzo. Después de 50 intentos, el robot fue capaz de darle la vuelta a la tortilla. Utilizando una mezcla compleja de técnicas basadas en Markov Decision Process (Procesos de Decisión de Markov), el equipo diseñó un sistema capaz de entender lo que funcionó y lo que salió mal.\nEn BBVA Data & Analytics, estamos explorando este mismo modelo para optimizar las estrategias de precios, que pueden ser útiles para el aumento de ingresos y la retención de clientes. En las líneas siguientes, explicamos los pilares básicos de esta metodología.\nProceso de Decisión de Markov\nEl aprendizaje por refuerzo consiste en aprender a decidir en una situación dada qué acción es la mejor para lograr un objetivo. Un agente de hardware o software está conectado a su entorno a través de la percepción y la acción. En cada instante el agente recibe desde el entorno a través de los sensores el estado  en el que se encuentra. Entonces el agente decide ejecutar una acción , que genera como una salida. Esta salida cambia el estado del entorno a , que se transmite al agente con una señal de refuerzo .\nEsta señal informa al agente de la utilidad de ejecutar la acción  desde el estado  para lograr una meta específica.\nEl comportamiento o política  del agente debe ser tal que elija acciones que aumenten la suma de todas las señales de aprendizaje recibidas con el tiempo. Formalmente, el modelo consiste en un espacio de estados, , en el que se puede encontrar el agente, y un espacio de acciones, , que el agente puede ejecutar. También el modelo incluye varias funciones que son desconocidas en principio: una función , que realiza las transiciones de estado, y una función , que calcula el refuerzo que el agente recibe en cada momento. Con estas funciones se completa la descripción de los Procesos de Decisión de Markov (MDP).\nEl objetivo del agente es encontrar una política  que, para cada estado , decida qué acción  debe tomarse, de modo que se maximice alguna medida de refuerzo a largo plazo. Este objetivo se denomina criterio de optimalidad y puede ser diferente dependiendo del problema a resolver.\nResolver el problema de optimización de la estrategia de precios (PSO) como un problema de aprendizaje por refuerzo (RL) requiere el modelado de PSO como un proceso de decisión de Markov.\nCriterios de optimización\nLa optimización de la estrategia de precios para las carteras de renovación requiere el manejo de dos funciones principales:\n\n\n\nGanancia\n\n\n\nDonde  representa la distribución de Bernoulli para la probabilidad  de aceptación de un ratio de renovación dado.\n\n\nRetención\n\n\n\nDonde  es la probabilidad de aceptación de un ratio de renovación dado .\nMaximizaremos la función de ganancia que contiene implícitamente la retención.\nEspacio de estados\nLos estados son las condiciones posibles del objeto antes y después de que se ejecute una acción. Una manera sencilla de explicar esto sería diferenciar las condiciones iniciales y las condiciones óptimas o peores esperadas. En nuestro ejemplo, un estado es una tupla de características globales y de clientes.\nEspacio de acciones\nLas acciones se refieren a la manera en que interactuamos conscientemente con el objeto y se pueden manifestar de diferentes maneras: puede referirse a la presión o ablandamiento que aplicamos sobre una superficie, o el movimiento del brazo, en el caso del robot y la tortilla.\nEn PSO, una acción es el ratio de renovación para el cliente actual. Asumimos que tenemos un espacio finito y discreto de acciones. Los ratios de renovación para cada cliente están limitados con las restricciones globales aplicadas a cada cliente (dependiendo de sus variables). Es decir, cada cliente tiene sus restricciones para los ratios de renovación. Dividimos el rango de posibles ratios de renovación de cada cliente de manera que cada cliente tenga 10 acciones posibles (incrementos/decrementos) incluyendo el valor de 1 (cuando el precio es el mismo que en la última renovación).\nFunción de transición\nLa función de transición realiza la transición de un estado dado a otro diferente después de ejecutar una acción específica. En PSO usamos el modelo de probabilidad (Regresión Logística) como un simulador que nos devuelve la respuesta en forma de probabilidad de aceptación de un ratio de renovación dado (acción) por cada cliente. Actualizamos las variables globales dependiendo del valor final de la clase (renovación o no).\n\nFunción de refuerzo\nEl refuerzo es una medida de cómo de buena es la acción aplicada al estado. Si una acción conduce a una política óptima, entonces el refuerzo reconoce la acción óptima, y es cuando el aprendizaje ocurre. Como nuestro criterio de optimalidad es maximizar la función de ganancia, el refuerzo en PSO es la diferencia entre el valor de la función de ganancia en el estado anterior y el actual.\nSe puede decir que tenemos un conocimiento completo del modelo de un MDP finito si conocemos el espacio de estados y el espacio de acciones, las probabilidades que definen su dinámica y la función de refuerzo.\nFunciones de valor\nMuchos algoritmos de aprendizaje por refuerzo intentan aprender la política  con aproximación de funciones de valor. Estas funciones estiman, dada una política, lo bueno que es para un agente estar en un estado particular, o incluso lo bueno que es ejecutar una determinada acción de un estado particular. El valor de un estado bajo una política , que se denota con , es el refuerzo que se espera obtener si empezamos a guiarnos por la política  desde el estado  hasta el infinito:\n\nDonde  denota el valor esperado dado que el agente sigue la política . Esta función se denomina como función de valor-estado.\nDe la misma manera, , puede definirse como el valor de ejecutar una acción  desde un estado  que sigue una política , es decir, el refuerzo que se espera obtener si comenzamos a guiarnos por la política  después de ejecutar la acción  desde el estado .\n\nEsta función se denomina función de valor-acción. En ambas ecuaciones el parámetro  se ha introducido como un factor de descuento para acciones futuras, siguiendo un criterio de optimalidad de horizonte infinito descontado en el momento definido:\n\nSiempre hay una o más políticas que son mejores o iguales que otras políticas, que se definen como políticas óptimas. Una política óptima se denomina como . Las políticas óptimas comparten una única función óptima de valor-estado.\n\nTambién existe una función de valor-acción óptima , que también es única y que maximiza el valor de cualquier par de estado-acción para cualquier política.\n\nLas políticas óptimas pueden derivarse fácilmente de estas funciones. Hay una gran cantidad de métodos que se pueden aplicar para aprender las funciones de valor, y por lo tanto, las políticas óptimas para un MDP finito dado:\n\nProgramación dinámica\n\nIteración de valor\nIteración de política\n\n\nMétodos libres de modelo (on-policy y off-policy métodos)\n\n Aprendizaje por diferencia temporal (Q-Learning)\n Monte Carlo con arranque exploratorio\n\n\n Métodos basados en el modelo\n\n Método de equivalencia de certeza\nDyna-Q\nPrioritized Sweeping\nQueue-Dyna\n Programación Dinámica en Tiempo Real\n\n\n\n \nEn el problema de la optimización de la estrategia de precios utilizamos Q-learning para aprender las funciones de valor y las políticas óptimas. La fase de implementación consistió en explorar el dominio ejecutando diferentes estrategias, normalización y agrupación de estados, construyendo Q-table inicializada a cero con un número de estados igual al número de clusters y simulando todas las tuplas de experiencias siguiendo el Q-learning. Para cada tupla de experiencia, los estados deben ser normalizados y discretizados siguiendo los factores de normalización y los clusters obtenidos anteriormente.\nEn el trabajo descrito ha colaborado con nosotros un experto en aprendizaje por refuerzo de la Universidad Carlos III de Madrid Fernando Fernández Rebollo.\n¿Has utilizado Aprendizaje Reforzado para otros usos? Comparte tus ejemplos con nosotros en el Twitter de BBVA Data & Analytics.\n\n"
    },
    {
      "title": "Creando Valor a partir de Datos Financieros Geolocalizados",
      "url": "https://www.bbvadata.com/es/creating-value-from-geolocated-financial-data/",
      "date": "5 junio, 2017",
      "content": "\nRecientemente, tuvo lugar Carto Locations 2017, una conferencia inmersiva de 2 días sobre los últimos desarrollos de la industria, la innovación y las ideas en el mundo de la inteligencia de la localización. Junto con otros profesionales y expertos de la industria, Elena Alfaro dio una visión general de nuestras actividades en los campos de la analítica, la visualización de datos, el análisis geoespacial y la ciencia de datos. Explicó en particular cómo estamos creando valor de grandes cantidades de datos financieros geolocalizados.\nLos registros de tarjetas bancarias que analizamos indican la hora y el lugar exactos de las transacciones. Otra dimensión son los detalles demográficos y el perfil de los clientes. Prácticamente, tenemos la capacidad de entender los comportamientos de los clientes e incluso predecir algunos de ellos. Además, somos capaces de recoger las relaciones de los comercios o personas alrededor de cada cliente, como una red social.\n \n\nEl análisis de estas diferentes dimensiones de los mismos conjuntos de datos ofrece oportunidades para apoyar a los clientes BBVA a conocer mejor su vida financiera y compararla con otros comportamientos similares y en un contexto geográfico similar. Es por eso que creamos Commerce360, una herramienta contextual de Inteligencia de Negocios para minoristas de cualquier tamaño que trata de ayudarlos a tomar decisiones accionables (vea Introducing Commerce360).\n\nOtro ámbito de actividad es Urban Analytics donde nuestra analítica apoya a los gerentes urbanos, operadores turísticos, etc. para tomar decisiones informadas. Por ejemplo, hemos trabajado en Madrid, con las autoridades de la ciudad de Barcelona, y otras ciudades de España, y también con el Ministerio de Turismo de México (Sectur).\n \n\nPuedes obtener una visión general de los conjuntos de datos con los que trabajamos a través de esta herramienta interactiva que permite explorar el comportamiento de los turistas en España durante los meses de verano. En la misma línea de esfuerzos, contribuimos regularmente a iniciativas de desarrollo sostenible como parte de nuestra agenda de “Datos para el bienestar social”. Por ejemplo, en nuestro estudio sobre el impacto económico del Huracán Odile, descubrimos que los datos financieros agregados pueden dar una idea de cómo las crisis afectan a poblaciones vulnerables (Ver Medición de la Resiliencia Económica de las Personas a Desastres Naturales).\n \n\n \nFinalmente, abrimos el acceso a nuestros datos financieros a empresas, pequeños comercios y desarrolladores. Cualquier persona interesada en la construcción de servicios de valor añadido basados en APIs financieras a través de BBVA API Market.\n \n\nPor ejemplo, BBVA PayStats ofrece datos estadísticos anonimizados de millones de transacciones realizadas con tarjetas BBVA, creando un mapa virtual compuesto por hábitos de consumo, datos demográficos y orígenes, actualizado semanalmente.\nPuedes ver aquí su charla completa en el evento Carto Locations.\nMuchas gracias a nuestros amigos de CARTO por la invitación.\n\n"
    },
    {
      "title": "¿Cómo mejorar la atención al cliente usando analítica de texto?",
      "url": "https://www.bbvadata.com/es/how-to-improve-customer-service-using-text-analytics/",
      "date": "17 mayo, 2017",
      "content": "\n¿Qué pasa cuando enviamos un comentario a través de una página web, nuestro teléfono móvil o correo electrónico? A diario, BBVA recibe miles de comentarios de sus clientes que son procesados por diversas unidades y, cada vez más, se emplean técnicas de análisis de texto para mejorar la respuesta a esta amplia demanda. En este artículo explicamos tres técnicas que estamos desarrollando en BBVA para procesar comentarios recibidos a través de distintos canales.\n \nPredicción de polaridad del sentimiento\nLa predicción de polaridad del sentimiento consiste en analizar el componente subjetivo de un comentario para predecir con mayor exactitud el resultado positivo o negativo de una valoración. Esta técnica, basada en la propuesta de Bo Pang and Lillian Lee “A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts”, permitiría a BBVA clasificar los comentarios de los agentes cuando llaman a sus clientes para ofrecer nuevos productos.\nPara probar la utilidad de este método se analizaron 152.000 comentarios de las 250 películas mejor valoradas en la base de datos IMDb. Partiendo de la premisa de que una película con una valoración de menos de 6 estrellas era una valoración negativa y una de más de 6 era positiva, se pudo demostrar que analizando sólo oraciones subjetivas se tiene una mejor predicción.\nEn ese contexto, esta técnica podría ayudar a clasificar a los clientes que han respondido de manera positiva o negativa para evitar volver a contactar a clientes que no quieren ser contactados.\n \nClasificación automática de comentarios\nEl Modelado Temático (Topic Modeling) es una técnica de análitica de texto capaz de asignar etiquetas temáticas a un conjunto de documentos. Esta técnica complementa  los resultados de las técnicas más tradicionales de clasificación y clustering, porque permite asignar varias etiquetas a un mismo documento sin supervisión al identificar los temas comunes que se tratan en una colección.\nEsta técnica se probó para analizar más de 370.000 comentarios sobre la satisfacción de los clientes y reducirlos, de manera automática a veinte grandes temas como por ejemplo “Número de cajas”, “Transferencias” “Atención personal” “Cajeros automáticos”. Gracias a esta técnica pueden identificarse más fácilmente  los grandes temas subyacentes en los comentarios de los clientes. Los temas identificados permitirían crear y refinar una taxonomía de los comentarios basada en datos que posteriormente se puede usar para aligerar tanto los procesos de anotación manual o clasificación automática de comentarios.\n \nPriorización de temas interés\nEl departamento de Customer Solutions de BBVA ha implantado una metodología global basada en la recomendación del cliente llamada Índice de Recomendación Neta (IReNe), que permite permite determinar el nivel de satisfacción y tomar decisiones para mejorar los servicios y productos.\nAproximadamente se reciben 80.000 respuestas al mes en un formato no estructurado, y resulta complicado identificar con precisión las razones del descontento de los clientes. Mediante una técnica de codificación automática se analizaron un millón de respuestas, y se identificaron los temas (tópicos) que explicaban la actitud de los clientes. Los temas más frecuentes  de descontento, son  el tiempo de espera, la resolución de dudas, la infraestructura de la sucursal entre otras.\nCon la codificación del texto libre se determinaron aquellos temas en los cuales actuar de manera prioritaria utilizando un indicador para determinar el potencial impacto de cada problema. Gracias a este cálculo, la oficina de México, pudo introducir mejoras en sus procesos que le permitieron mejorar 10 puntos la tasa de satisfacción de los clientes.\n¿Conoces otros ejemplos de analítica de datos que puedan ser útiles para clasificar los comentarios de los clientes?\n\n"
    },
    {
      "title": "Análisis de Influencia sobre el Comercio de las Medidas de Calmado de Tráfico",
      "url": "https://www.bbvadata.com/es/urban-analytics-to-measure-the-economic-impact-of-traffic-calming/",
      "date": "19 abril, 2017",
      "content": "\nLas ciudades son hardware físico que sirve de soporte a múltiples dinámicas superpuestas, el resultado de esta superposición es un sistema extremadamente complejo. Cualquier cambio en una de sus condiciones produce efectos tanto positivos como negativos en las demás capas. Tradicionalmente los gestores urbanos han tratado de medir esos efectos -principalmente a través de las encuestas- para obtener información sobre si las modificaciones introducidas en las condiciones de contorno producían los resultados deseados.\nLa digitalización ha traído, entre otras, dos innovaciones importantes para los gobiernos de las ciudades: por un lado la oportunidad de abrir nuevos canales participativos que den voz efectiva a la ciudadanía, y por otro la posibilidad de reunir cantidades masivas de evidencias para medir los efectos de la toma de decisiones.\nEn BBVA Data & Analytics creemos que este mundo cada vez más complejo necesita enfoques sistémicos para medir las consecuencias de cualquier evento, sea intencionado o imprevisto. Estamos particularmente implicados en reorientar los debates y llevarlos desde el ámbito de las opiniones subjetivas hacia las evidencias objetivas. Nuestra investigación en este campo nos ha llevado al desarrollo de métodos y técnicas que proporcionan información sobre la actividad comercial de una región a partir de la capacidad descriptiva de los datos financieros (fundamentalmente, transacciones con tarjetas bancarias). Hemos llevado a cabo múltiples estudios para medir el impacto económico de circunstancias excepcionales, como desastres naturales o grandes eventos.\nEn este artículo, describimos cómo medimos y caracterizamos los efectos de las decisiones de gestión del espacio urbano, sobre un caso real y reciente, en base a la huella digital de la actividad comercial. Explicamos cómo medimos la actividad comercial esperable en un área y la comparamos con la realmente registrada. Creemos que las autoridades locales pueden utilizar este tipo de observaciones para optimizar los efectos positivos y mitigar los negativos en el proceso de toma de decisiones, lo que redundará en una toma de decisiones con efectos previstos y deseados.\n\nEsta es la Gran Vía (literalmente “gran camino”) la icónica calle comercial, también conocida por su vida nocturna, situada en el centro de madrid, españa.\n\n\n \nEn Madrid, el eje de Gran Vía sufre problemas de congestión del tráfico y los coches ocupan un espacio que los peatones están reclamando. Al igual que en otras ciudades de todo el mundo, el gobierno de la ciudad está evaluando soluciones sostenibles que mitiguen la densidad del tráfico automovilístico y al mismo tiempo promuevan la actividad económica y el uso del espacio por los ciudadanos.\nLa pasada temporada navideña, el Ayuntamiento de Madrid adoptó medidas para calmar el tráfico en la Gran Vía -una de las calles más concurridas de la ciudad- en beneficio de los peatones. Durante algunas semanas, hasta principios de enero, el acceso se limitó a un carril en cada dirección y se impuso un límite de velocidad de 30 km/h. La medida tenía como objetivo fomentar la circulación de los peatones y mejorar indirectamente la experiencia de compra en el periodo de máxima afluencia del año. También fue la prueba de una estrategia más global para reducir los efectos negativos de la congestión de los coches en el centro. Como era de esperar, las estrategias de calmar el tráfico provocaron acalorados debates en los medios sobre sus implicaciones económicas.\n\nComo parte de nuestras iniciativas de análisis urbano, analizamos aproximadamente 7.000 transacciones diarias de tarjetas bancarias para extraer datos sobre lo patrones de gasto en el área de Gran Vía, no solo en el periodo de estudio, sino en una serie temporal de 3 años de profundidad.\n\nA lo largo de la Gran Vía, en diciembre de 2016, había 285 TPVs, su desglose en categorías comerciales es el siguiente: 26% bares y restaurantes, 19% tiendas de moda, 18% hoteles, 37% otras categorías.\nLo que se esperaba: El modelo de predicción\nEn primer lugar, inferimos lo que habría sucedido durante el periodo de Navidad sin restricciones de tráfico. Para ello utilizamos modelos de Series Bayesianas Estructurales, un procedimiento análogo al que previamente habíamos utilizado para medir el impacto económico del huracán Odile, que azotó la península de Baja California Sur en México, en 2014. En ese caso de estudio, el modelo predijo el nivel de gasto esperado si el huracán no hubiera alcanzado el área, por comparación con las series temporales de regiones no afectadas por el huracán, y pudimos así medir el tiempo necesario para que la serie temporal teórica y la registrada convergieran de nuevo, momento en el que consideramos recuperada la actividad normal.\nAplicando esta metodología  al estudio de los efectos de la estrategia de calmado de tráfico en Gran Vía, pudimos comparar el comportamiento comercial del área antes de las restricciones, con el comportamiento comercial de áreas similares (en términos de correlación de series temporales) en las que no se adoptó ninguna medida de esta naturaleza.\nNuestro enfoque se basó en los siguientes pasos:\n\nLa serie temporal de gastos se calcula para el área de Gran Vía, y para todos los códigos postales en España, desde enero de 2014 hasta diciembre de 2016, con resolución diaria (unos mil puntos de nivel de consumo para cada uno de los 11.000 códigos postales del país).\n\n\nA partir de un umbral de correlación mínimo, se seleccionan N series temporales correspondientes a N códigos postales para formar el modelo que define la serie temporal que mejor se adapta al patrón de actividad comercial de Gran Vía.\n\n\nAsí se identificaron los códigos postales con una alta correlación en los últimos tres años en sus patrones temporales de consumo con respecto al área de Gran Vía. En concreto se seleccionaron 200 códigos postales con una correlación mínima 0,85\n\n\nEl modelo resultante define el gasto total esperado en el área de Gran Vía en condiciones “normales”, por analogía con lo ocurrido en los N códigos postales con un comportamiento similar.\n\nA continuación contrastamos el gasto esperado en la Gran Vía (según el modelo de predicción) con el gasto realmente registrado durante el periodo navideño de 2016, midiendo la diferencia entre las dos series temporales.\nMidiendo el impacto: Comparando el modelo de predicción con la realidad\nHubo restricciones de tráfico en tres periodos. El primer periodo, del 2 al 11 de diciembre, registró un pequeño incremento de +1% del gasto. El segundo, entre el 16 y el 18 de diciembre, obtuvo una variación de +2% del gasto. En contraste, el último periodo, del 23 de diciembre al 8 de enero, registró una disminución de -13% en el gasto.\nLas pasadas navidades, observamos una disminución total de gasto de -8% en comparación con la actividad esperada en gran vía, con comportamientos dispares por categoría de actividad.\nLos gastos previstos y reales en Gran Vía durante el período del estudio\nAdemás de llevar a cabo un análisis conjunto, se hace necesario caracterizar cualquier el impacto económico por sectores de actividad. Por ello analizamos la evolución de tres categorías comerciales principales de la zona: moda, restaurantes y hoteles. Los resultados ayudan a destacar el impacto con mayor detalle:\n\nObservamos una disminución global del -17% en el gasto de moda -la actividad dominante por ingresos en este eje- durante los tres periodos.\n\n\nSin embargo, los restaurantes -la actividad dominante por número de empresas- aumentaron sus ventas en un +5% durante los dos primeros periodos.\n\n\nLa categoría hotelera también obtuvo variaciones positivas durante los tres periodos, con un incremento de +11% en gastos.\n\nComo breve resumen, al comparar la actividad real de la Gran Vía durante la temporada de Navidad con la actividad esperada del modelo de predicción, se obtuvieron las siguientes medidas de impacto económico:\n\nAunque en términos absolutos la actividad comercial registrada en los periodos navideños de 2015 y 2016 alcanzó niveles similares, la facturación de la última Navidad fue inferior a la esperada según el modelo estadístico utilizado. De hecho, el mismo modelo aplicado a nivel de toda la ciudad no detecta la disminución registrada en el área de Gran Vía.\nLa capacidad descriptiva de estas nuevas fuentes de información, junto con las capacidades analíticas de la ciencia de datos aplicadas al ámbito de los análisis urbanos, proporcionan este tipo de información sobre el pulso comercial de una región. La fuente de datos con la que trabajamos -huellas económicas digitales anonimizadas- nos permite describir tanto efectos de onda corta (eventos que, durante horas o días específicos, tienen efectos temporales en el ritmo normal de un área), como efectos de onda larga (cambios lentos en el perfil y comportamiento de los ciudadanos y turistas o tendencias que transforman el tejido comercial de un barrio, cambiando su carácter). Creemos que es nuestra obligación abrir y compartir este conocimiento con las autoridades locales que deben tomar complejas decisiones y con la sociedad, para que pueda adaptarse a las circunstancias cambiantes.\n\n"
    },
    {
      "title": "Our Digital Transformation Under the Scrutiny of MIT CISR",
      "url": "https://www.bbvadata.com/es/our-digital-transformation-under-the-scrutiny-of-mit-cisr/",
      "date": "17 abril, 2017",
      "content": "\nRecently, we have had the pleasure of having Barbara Wixom at our Madrid office. Barbara is a Principal Research Scientist at MIT Sloan Center for Information Systems Research (CISR) investigating data monetization. She has been studying as one of the pioneers in the field and for our role in transforming BBVA.\nIn her talk, she highlighted our efforts to create an evidence-based culture (e.g. creating a shared language, inspiring colleague to employ a data-driven spirit) and how our data engines are creating competitive advantages. Also she described how monetizing data requires entering into the “information business” and its business models (e.g Commerce360, Paystats) that only come when adapting the operational backbone of the company.\nDigital transformation is no easy business. Barbara’s CISR colleagues Peter Weill and Stephanie Woerner recently published “Is Your Company a Digital Leader or a Digital Laggard” that focused on BBVA to understand what differentiates a digital business leader from those trailing behind in the global digital revolution. \nRead about this research briefing and the BBVA transformation it describes in an article on the BBVA website.\n\n"
    },
    {
      "title": "See the Future to Innovate in the Present",
      "url": "https://www.bbvadata.com/es/see-the-future-to-innovate-in-the-present/",
      "date": "21 marzo, 2017",
      "content": "\n4YFN stands for “4 Years From Now” and it is the sister event to the Mobile World Congress  that takes place in Barcelona every year. Unlike its older brother, targeted to the audience of large technological corporations, 4YFN promotes startups and entrepreneurs who try to figure out the new business models called to disrupt the status quo.\nIt is remarkable that even the name of the event is oriented towards the future and stimulates forward thinking: How will the world be like in 4 years from today? Answers to that question is key for organizations that want to avoid introducing to the market an obsolete product. Considering the future when designing the present is fundamental.\nThis is how Siri was conceived. Its originators imagined how an “intelligent assistant” would be even before the technologies needed to make it possible were available. Peter Diamandis, co-founder of Singularity University, tells this short story:\n \n\n \nToday, large technology companies speculate publicly about the future to test the design before writing a single line of code. For example, Amazon applies a type of fictional design called Working Backards. Their Drone Delivery service or Just Go Technology are a clear example of a hypothetical near future that allows them to learn today about the future:\n \n \n\n \n\n \nThese videos highlight a possible future of a service in a very present experience. In their narrative, the line between reality and fiction is very thin, which allows to test hypotheses: How would the market react? What concerns will uncover? If we really want this future, how can we make it become real? Those are just some of the doubts that Amazon now can answer.\n \nAt BBVA Data & Analytics we use these Design Fiction techniques to establish a tangible vision of the futures where we want to operate (see Experience Design in the Machine Learning Era). We aim to understand the implications of AI and Machine Learning technologies, the language used to describe them, the rituals, the magic moments, the moments of truth or the potential frustrations. It helps the different stakeholders of a project to engage with essential questions to understand what the desired experience means and why the team should build it.\n \nDrafts of 6 pieces generated through design fiction\nDesign Fictions act as a totem for discussion and evaluation of changes that could bend the visions of the desirable and planning of what is necessary.\nThese approaches are not exclusive to large companies. In fact, startups, which are of a smaller size and have less legacy, have more agility to speculate about future challenges and opportunities. 4YFN presented a unique opportunity to connect with data analytics startups from all over the world. Many of them apply machine learning and related technologies to perform location analytics, text analytics, code analytics, all kind of bots or user profiling among other applications. For instance:\n \nSentiance is working towards what they call the internet of you. Their solution analyses data coming from sensors (mobile phones, wearables, home thermostats, etc.) to understand the user’s context to adapt it to him/her.\n \n\n \nA different approach to the same vision comes from the UK with Pixoneye: instead of using sensor data they base their user understanding  from the analysis of the pictures stored on the camera roll.\nA french startup Moore has crunched out an algorithm that promises making the limitations on data storage space something from the past. Its machine learning algorithm learns how users use their data to predict which files they are most likely to use and seamlessly tiers inactive data to the cloud, even without Internet connection.\nThe team of Estimote are known for democratizing the use of beacon technologies in retail stores. On their mission of creating an OS for the physical world,  at 4YFN they presented the Estimote Mirror that enables a seamless interaction with objects and screens:\n \n\n \nThese are just a few examples out of the more than 500 startups that attended 4YFN this year.\nFor large companies, being close to startups has been proven to be of vital importance:\nWhile large companies cover the present, startups allow to see the future.\nWorking closely with startups is part of a company’s open innovation strategy and can be done in many ways: through entrepreneurial competitions such as the BBVA Open Talent, through corporate accelerators like Telefonica’s Wayra or through corporate investment funds such as Google Ventures just to name a few . At 4YFN we found the concept of Corporate Clienting that BMW Startup Garage proposes: the best way to work with startups is through being their agile client so they streamline the process of buying startup products without having to buy company’s equity.\n \nEntrepreneurs getting access to state-of-the-art BMW prototyping tools thanks to the Startup Garage program\n \nBut not only open innovation nurtures big companies’ innovation. Cultivating internal innovation programs has proven to be equally or more cost-effective than open innovations strategies. In this sense, companies like Samsung with its internal innovation program C-Lab, or our analytics innovation initiatives that employ design fiction to create visions of potential futures.\n \nOne of our analytics innovation sessions led by Fabien Girardin\n\n"
    },
    {
      "title": "Part of BBVA’s Technological Progress and Transformation",
      "url": "https://www.bbvadata.com/es/part-of-bbvas-technological-progress-and-transformation/",
      "date": "18 marzo, 2017",
      "content": "\nWe are proud to be part of BBVA’s technological progress and transformation in a complex environment. Built on 160 years of experience, BBVA is now present in 35 countries with more than 134,000 employees. At BBVA Data & Analytics, we contribute in anticipating the needs of each of the 70 millions customers, in improving their experience and increasing their number.\n\n\n"
    },
    {
      "title": "Data Science at BBVA",
      "url": "https://www.bbvadata.com/es/data-science-at-bbva/",
      "date": "15 marzo, 2017",
      "content": "\nWhat are the fields in which Data Analytics and Big Data technologies can be applied in a heavily regulated environment such as banking? What problems and drawbacks arise when working with large amounts of data in a financial institution, and how to deal with them?\nIn the following video recorded at the VII RITSI national congress, one of our Data Scientist María Hernández answers these questions and describes some success stories to transform Big Data into personalized experiences for BBVA customers. \nShe goes through our process that begins with the first concept tests and design of models, to give way to the development of pilot projects and products in restricted environment, and that finally culminates with the production of new tools. María particularly points out some challenges Data Scientists need to solve at every stage, from ideation to implementation, with some tips on how to handle them in this industry.\n \nHere is the full talk. Enjoy!\n\n\n"
    },
    {
      "title": "Data Science Influenced by Talented and Successful Women",
      "url": "https://www.bbvadata.com/es/data-science-influenced-by-talented-and-successful-women/",
      "date": "13 marzo, 2017",
      "content": "\nToday, we are more than 40 highly-qualified data scientists, technologists, domain experts, strategy practitioners, and visual design thinkers. Even if this is still not enough, 33% of us are women. Besides inspiring and educating data scientists we support women in the field. This is also the purpose of the Women in Data Science (WiDS), a series of event organized by Stanford University. In the words of Carme Artigas, Chairwoman of Synergic Partners, in her capacity as WiDS ambassador:\n“Data Science is changing business models. It poses new ethical challenges and conflicts and us women need to raise our voice to influence the new world that is right around the corner. Now’s the time to make an impact on the fourth industrial revolution”.\nIn a recent WiDS Conference in Madrid we could observe the growing presence of women in all fields, and Data Science is not an exception. Our CEO, Elena Alfaro, who participated in the event, confirmed this fact with our efforts to promote gender equality.\nDuring that event, Amparo Alonso, coordinator of LIDIA, a research and Artificial Intelligence group, share her optimism about the new opportunities that these technologies open. She believes that society as a whole will need time to adapt to the changes that Artificial Intelligence is going to bring. It will destroy many jobs, while opening new professional fields that do not exist today. She particularly highlighted that:\n“Algorithms enhance our capabilities as a civilization”\nFor Angela Shen-Hsieh shared her point of view as current Head of Product Innovation at Telefónica R&D, and former IBM employee, where she worked in the development of its Watson supercomputer’s ability to interact through conversations. She mentioned that:\n“Artificial Intelligence is just a step in the automation process”\nGas Natural Fenosa is also focusing on data analytics to offer a better service to its customers. “We want to offer energy advice to our customers to help them curb consumption, enjoy better rates, or contract the type of service they really need,” said Rosa Sanz, Head of People and Resources at the power utility.\n \n\n \nAlong with the 75 WiDS meetings held across the world, we revendicate the role of women in shaping technological evolutions and we want women to become a pivotal part of a more diverse fourth industrial revolution.\n\n"
    },
    {
      "title": "We Join UN Global Pulse in “Data for Climate Action” Challenge",
      "url": "https://www.bbvadata.com/es/we-join-un-global-pulse-in-data-for-climate-action-challenge/",
      "date": "10 marzo, 2017",
      "content": "\nAt BBVA Data & Analytics, we strongly believe in the moral obligation of data-driven organizations to share data and contribute to society. Hence, we regularly contribute to sustainable development initiatives as part of our Data for Social Good agenda. For instance, in our economic impact study of Hurricane Odile, we discovered that aggregated financial data can yield insights into how crisis affect vulnerable populations (See Measuring People’s Economic Resilience to Natural Disasters). This type of work would not be possible without the efforts of our friends at UN Global Pulse and their innovation initiatives on Big Data.\nToday we are happy to enter into a new collaboration with UN Global Pulse with a contribution to their “Data for Climate Action” challenge, an initiative that enables unprecedented access to data and tools from leading companies to fight climate change (See the press release). It represents another concrete way to put data to work for climate action, and we look forward to contributing once again, with our aggregated financial data and our excellence in data science.\nPractically, we are going to provide, via our Open API platform, statistical data on transactions made with BBVA debit and credit cards in Spain during the last three years through. In this way, BBVA is opening its core and bank functionalities in order to boost innovation for climate action and in the whole financial industry.\nAbout the Challenge\nData for Climate Action will target three areas relevant to the Sustainable Development Goal on climate action (SDG 13): climate mitigation, climate adaptation, and the linkages between climate change and the broader 2030 Agenda.\nThe challenge aims to generate original research papers and tools that demonstrate how data-driven innovation can inform on-the-ground solutions and transform efforts to fight climate change. It builds upon the model of data science competitions pioneered by organizations like Kaggle, and company-specific initiatives to share big data for the public good, such as the “Data for Development” challenges hosted by Orange.\nData scientists, researchers, and innovators from around the world are encouraged to apply and submit their proposals by April, 10th, 2017. Visit the official website at DataForClimateAction.org for additional details on the procedure.\nThe selected participants will have four months to conduct their research. A diverse panel of experts in climate change and data science will evaluate final submissions based on their methodology, relevance, and potential impact. Winners will be announced in November of 2017.\n\n"
    },
    {
      "title": "Big Data to Enhance the Tourism in Mexico",
      "url": "https://www.bbvadata.com/es/big-data-to-enhance-the-tourism-in-mexico/",
      "date": "27 febrero, 2017",
      "content": "\nLast November 29th we presented the results of the project “Big Data and Tourism” carried out together with BBVA Bancomer and the Secretariat of Tourism in Mexico (SECTUR).\nDue to its cultural heritage and to the natural diversity of the country, among other strengths, tourism in Mexico is a rising sector, a solid source of employment in the country, and a relevant growth pillar that contributed in 8,7% to the GDP in 2015.\nLeveraging on the descriptive capacity of financial data, the aim of our partnership project was to create new insights based on the spending patterns of national and international visitors. The driving features of the collaborations have been the following:\nAbout the data: We employed card payments data, a high resolution data source throughout time and space that, besides, reflects real facts, not just intentions or recalls. Its statistical value has been demonstrated in other projects through the replication of macroeconomic indexes. It has been a firm intent for innovation from SECTUR and INEGI to rely on big data sources in the frame of this pioneering project.\nAbout the methodology: Whereas foreign tourism flows are relatively easy to track and measure, given that the entry points to the country are limited, measuring national inner tourism flows is a much harder task. Dealing with anonymized data, we have developed and implemented a new model to assess the usual environment for everyone of Bancomer cardholders, so every time a transaction is made out of a card tenant usual environment, it is labeled as a transaction linked to touristic activity, and is aggregated with other transactions to be part of the statistical results. This has allowed us to characterize three different kind of profiles: residents, national visitors, and foreign tourists.\nAbout the infrastructure and tools: Once anonymized, data are stored in a cloud infrastructure, where processing and analysis are carried out. This is a much more economical and flexible solution than on premise infrastructures. Regarding the tools employed, for the data analysis mainly R software has been applied, and to visualize results, Tableau and CARTO have been used.\nAbout the team: Data Science is a new discipline that combines advanced skills from STEM education (science, technology, engineering and mathematics). BBVA has concentrated this talent on its analytic centre of excellence -BBVA Data&Analytics- but, thanks to internal training programmes and to the activity of its Community of Practice on Data Science, this knowledge is spreading throughout the organization as a vector of the digital transformation. In the development of this project, an interdisciplinary team in Mexico and Spain has been involved, being Juan de Dios Romero, Heribert Valero, Beatriz Alonso, and Juan Murillo the members on BBVA Data&Analytics side.\nAbout the results: the memorandum of understanding signed between SECTUR and Bancomer in October 2015, comprised the development of 12 analysis throughout 2016. The presentation made to media and stakeholders last November 30th focused on first two deliverables: “Magical Towns” and “Quintana Roo”.\nOur results reveal important aspects such:\n\nWhere does a greater concentration of tourism arise? Thanks to the usual environment assignation to each of every 20 million Bancomer customers, we have been able to measure the relative importance of national visitors, international tourists, and local inhabitants, showing spatial distribution of these three segments, and its evolution throughout time, being able to measure seasonality for every destination in very accurate way.\n\n\nWhich are the activities or commercial categories that present the highest expenditure?  we have provided average ticket, a metric that varies among different destinations and profiles, for each collective (national and international tourists), and also their time variations and trends.\n\n\nWhich is the country of origin of the foreign tourist, or what state does the national tourist come from? This is useful to make a ranking between the global tourism issuing markets.\n\nAs main conclusions in the Quintana Roo area, we can highlight that 35% of the expenditure made by foreign tourists was carried in Playa del Carmen, while the national tourist spending takes special relevancy in Cancún and Playa del Carmen, with 25% and 27% of expenditure respectively. Likewise, the highest concentration of tourists was registered in the periods of July/August, and November/December, and the categories with more relevance were “entertainment” for the foreign visitors and “travels” for national tourist. Around 50% of the expenses incurred by national tourists were made by Federal District, State of Mexico and Quintana Roo inhabitants, in that order. In terms of foreign tourism, the major percentage of spending was generated by Americans tourists, with Argentina ranking second.\nIn general, among the destinations belonging to the “Magical Towns” program, there is a greater activity of the national tourists compared with the international ones. In this type of destinations, the activity of the national tourist is regional and with heterogeneous spending behaviors that depend on both geographical location and financial inclusion.\nThis type of knowledge allows the entities responsible for tourism in each country, like SECTUR in Mexico, to improve their offer, to target their promotional activity,  and thus increase the number of visitors and their experience.\nA more detailed description of our approach is available (in Spanish) in this publication of “Magical Towns” Analysis and in this other publication of “Quintana Roo Analysis”.\n\nWe also invite you to look and share the visualization of the study.\n\n\n                    var divElement = document.getElementById('viz1488183987544');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1204px';vizElement.style.height='795px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                \n\n\n"
    },
    {
      "title": "Transforming Big Data into Personalized Experiences",
      "url": "https://www.bbvadata.com/es/transforming-big-data-into-personalized-experiences/",
      "date": "16 febrero, 2017",
      "content": "\nCompanies like Google, Amazon, Netflix have grown exponentially transforming user-generated data into personalized experiences. Today, many organizations like BBVA are using a similar approach with a better understanding of their customers and their expectations. In a recent CON.VERGE event, Marco Bressan, President of BBVA Data & Analytics explained how BBVA embarked into that journey. In an informal chat with Guibert Englebienne CTO & Co-Founder at Globant he covered aspects of transforming a bank into a data-driven business including:\n\nHow BBVA is leveraging value from data\nWhere data can really support customer experiences\nChallenges that people underestimate and overestimate when exploiting data\nHis own experience with changing culture about analytics\nSharing the contributions of BBVA Data & Analytics to build a data driven business\n\n \nHere is the full interview:\n\n\n"
    },
    {
      "title": "Transforming Open Data into Economic and Societal Value",
      "url": "https://www.bbvadata.com/es/transforming-open-data-into-economic-and-societal-value/",
      "date": "7 febrero, 2017",
      "content": "\nSince its launch in 2015, European Data Portal is offering a central access point to an increasing amount of re-usable Open Data. Many studies have been published on the financial benefits of this kind of data. However, very little is known about how the companies transform this data into economic and societal value.\n“We were the respondent with the biggest dedicated group of professionals working with Open Data, meanwhile most of the organisations that took part in the survey employ relatively small teams (85% of the organisations indicating 0-5 employees)”\nLast year, we have had the pleasure to contribute along with 200 other data-driven organisations from more than 21 countries on a survey led by European Data Portal on that topic. Here are some conclusions of the results published in “Re-Using Open Data”.\n \nThe fertile soils for data-driven organisations\nThe estimated direct market size of Open Data between 2016 and 2020 is 325 billions €, added as percentage of GDP of close to 36.9%. During this period, about 100,000 Open Data related jobs will be created, with an average increase of 7.3%. Leveraging the type of information will be an effective way of saving costs for the public administrations (about 1.7 billions € will be saved throughout Europe). The survey highlights that Spain and the United Kingdom are the most active countries contributing to the European Data Portal, therefore offering a fertile soil for data-driven organizations.\n \n\nSource: European Data Portal, 2016, Open Data Maturity in Europe\n \nBusiness models that leverage Open Data\nThere are many business models built on the exploitation of Open Data. The survey uses BBVA Data & Analytics as an example among many other companies that leverage Open Data to enhance internal processes. What makes us slightly more unique is that we additionally:\n\nhelp a global bank like BBVA offer new services through the analysis of customer behaviors in combination with available Open Data.\nshare aggregated and anonymised data through an Open Data Platform.\nperform research with Open Data to create societal value (see Our Contributions to Sustainable Development with Data and Analytics).\n\n \nThe Open Data capacity of organisations\nThe extraction of value from Open Data needs resources and talent in data science and data engineering. Employees are often the most important resource for organisations. We take that fact very seriously as the survey highlights: we were the respondent with the biggest dedicated group of professionals working with Open Data, meanwhile most of the organisations that took part in the survey employ relatively small teams (85% of the organisations indicating 0-5 employees).\n \nThanks to the European Data Portal for offering us to participate to that survey. Discover the results in the publication “Re-Using Open Data”.\n\n"
    },
    {
      "title": "The Powerful Fusion of Intuition with Data",
      "url": "https://www.bbvadata.com/es/the-powerful-fusion-of-intuition-with-data/",
      "date": "1 febrero, 2017",
      "content": "\nLast Summer, we launched Commerce360, our service that offers small and medium businesses powerful metrics with contextual business intelligence on millions of consumers throughout Spain. It was one of the first tangible example of how BBVA has transformed its data-driven bank vision into a reality. With Commerce360 even the smallest entrepreneur has contextual business intelligence at their fingertips that exceeds their customer base in a way not even large enterprises hoped for a few years ago. It helps answer fundamental questions for decision-making and marketing actions such as:\n\nWhere do my customers come from? How is my business doing compared to my competitors? What are the profitable hours for my business? etc.\n\nLast week we took a moment to step back and reviewed the learnings on how data can change retail business intelligence. In an event hosted by our friends of the Innovation Center of BBVA in Madrid, we asked our Commerce360 Ambassadors to share the experience and knowledge in the use of metrics and indicators to measure the performance of their business and take decisions. Here is how our CEO Elena Alfaro describes the initiative (in Spanish):\n\n\n“Today 70% of the decisions to eat in a restaurant originated with a digital interactions” — Diego Coquillat, CEO of 10Restaurantes.es\n\nSeveral speakers debated how retailers organize their business around metrics and how Commerce360 supports that practice. For example, Diego Coquillat, CEO of 10Restaurantes.es, affirmed that today 70% of the decisions to eat in a restaurant originated with a digital interactions. Moreover, 50% of the restaurant reservations are now made online. He argued that Commerce360 has a huge potential that goes beyond real-time data and that is in the prediction, the possibility of anticipating actions to improve the overall customer experience. That is a major opportunity for the hospitality industry. Indeed, Diego mentioned that, within the five key pillars of the customer experience in that industry, four of them are becoming increasingly digital: The entry door, the tip, the prestige, the personalization (all of them digital) and persons.\n \n\n \nData and metrics are valuable to better understand customers, they also offer a fantastic tool to secure investments. For business finance expert José María Casero, most of the project to open a retail business do not receive financial support because entrepreneurs do not know how to describe well their needs. He argued that this obstacle can be overcome with the use of data — like the ones that Commerce360 provide — because the right metrics allow professionals seeking investments to explain, substantiate and justify their project.\nFor additional examples on the practical use of Commerce360, here are some testimonials (in Spanish) captured during the event.\n\n \nYou can listen the podcast (in Spanish) of the event in the following link \n\n"
    },
    {
      "title": "The Data-Driven Bank: From Vision to Reality",
      "url": "https://www.bbvadata.com/es/the-data-driven-bank-from-vision-to-reality/",
      "date": "24 enero, 2017",
      "content": "\nLast week took place Brainstorm@BBVA, an internal company event created to provide all employees with a first-hand update on how of the bank’s most strategic projects are progressing and what kind of work is being conducted.\nOn this occasion, Marco Bressan, Head of Data & Open Innovation at BBVA and President of BBVA Data & Analytics acted as master of ceremony, to explain the benefits and implication of transforming a bank into a data-driven organization.\n“Data-driven organizations, make their decisions, design their internal processes and, above all, gauge the experience of its customers based on data. Actually, machines make these decisions and learn to adopt them through data.” — Marco Bressan.\nThe event started with a brief video from 2014 in which BBVA President Francisco González Rodríguez shares his vision of a data-driven bank:\n\n \n“That vision has become a reality” claimed Marco Bressan. A reality for BBVA but also for many other companies and sectors. Data-driven organizations understand perfectly what the customer expects from them and tailor their experiences to these expectations; it means, they use the data within each product. This companies understand the trade-off between trust and data as a key factor in the relationship with customers. They use the data in all the decisions they make, and most of them are done automatically. Thanks to the data, these companies and their employees learn and adapt quickly.\n \n\n \nOur CEO, Elena Alfaro took the stage to give tangible uses of data and analytics. She described our effort to support different business units in the development of new products or services emphasizing that mastering machine Learning technologies became key. Being able to offer a personalized service to each customer is only possible if a machine learns of the behavior of each one.\n \n\n \nOne result presented was Commerce360, a web platform that turns anonymous and aggregated data from purchases of BBVA credit or debit cards at PoS terminals into business intelligence for retailers in Spain and Mexico.\nAlejandro Valladares, head of Business Intelligence at BBVA Spain, commented how the customers needs have changed with the new era and they demand new capabilities:\n\nFaster, real-time responses to cover their needs.\n\n\nFlawless customer experience, based on a thorough knowledge of each one of their interactions, of the internal data.\n\n\nNew external data, whether public or transferred by the customer, complementing the internal vision.\n\n \n\n \nFinally, Juanjo Divasón, CEO of Madiva Soluciones, a company that joined BBVA two years ago, shared the story behind the development of one of BBVA’s most recent data-based products: BBVA Valora, an app that allows anyone to discover the real market price to buy or rent a house.\nThese examples confirmed that the vision of the bank that leverages big data and machine learning for improving processes and experience, is now a reality. Marco Bressan concluded the event encouraging BBVA’s employees to continue contributing to that vision.\n\n"
    },
    {
      "title": "Our Contributions to Sustainable Development with Data and Analytics",
      "url": "https://www.bbvadata.com/es/our-contributions-to-sustainable-development-with-data-and-analytics/",
      "date": "19 enero, 2017",
      "content": "\nThis week, our Head of Urban Data Services & Applications, Juan Murillo Arias traveled to Cape Town to attend the first meeting of the UN World Data Forum, an initiative that seeks to foster cooperation with a wide array of professional groups, such as information technology, geospatial information managers, data scientists, and users, as well as civil society stakeholders. The meeting provided an excellent opportunity to catch up with Juan and learn more about how data analytics can help support sustainable development. Here is an approximate transcript of his contribution to the forum in the Big Data Innovations for sustainable development session.\nYou have been a member of BBVA Data & Analytics since day one. Could you give us a brief introduction and an overview of BBVA’s experience with big data analytics:\nJuan: BBVA Data & Analytics is part of BBVA Group, a global corporation that provides financial services to 67 million customers, mainly in Spain, Turkey, the U.S. and Latin America. We are a center of excellence in financial data analysis that spun off from the BBVA Innovation Center three years ago. Our mission is to extract value from big data through the development of innovative solutions for BBVA, its customers and for third parties. Our teams are based in Madrid, Barcelona and Mexico City and boast more than 40 data scientists and engineers, many of them holding Ph.D.’s in Artificial Intelligence and a deep knowledge of finance and banking.\n\n“There is great value in anonymized and aggregated data sets, and our customer base represents a relevant sample of the population across several geographies.”\n\nAt BBVA we use data-driven applications for many different purposes: to deliver better products and services to our customers, by gaining a deeper understanding of their needs and preferences (customized solutions); to detect & prevent fraud; to enhance our risk management practices; for client segmentation and marketing purposes, and to improve financial inclusion by developing digital contracts thanks to which people in rural areas can get access to banking services without visiting a bank branch.\nHow did BBVA realize that putting its data-related insights to work to achieve the UN Sustainable Development Goals (SDG) was an opportunity? \nJuan: As any private corporation with millions of customers, each interaction through our digital channels generates behavioral data. Most of the players in the big data industry have become aware that it provides them with a very powerful lever to improve their services and adapt them to the needs of each customer. But the value we can get by transforming data into customized information goes beyond improving customer experiences. There is also tremendous value in anonymized and aggregated data sets, and our customer base represents a relevant sample of the population across several geographies. By analyzing this high-resolution behavioral data, we have been able to produce detailed representations of both flows of people and money that are useful to better understand the dynamics within a specific region. For instance, we can measure tourist flows based on their digital footprint in economic activities. Similarly, in a collaboration project with United Nations Global Pulse, we have been able to map the economic impact of Hurricane Odile in Baja California Sur, Mexico, by comparing socio-economic behaviors before, during and after the natural disaster.\n\nMany of the human activities related to the 17 Sustainable Development Goals involve a transaction that must be processed (e.g. access to clean energy or responsible consumption). That is why we are very happy to share this new knowledge with any stakeholder capable of transforming them into initiatives that can contribute to, eventually, spread prosperity, speed up recovery after a negative event, or measure progress and trends in the achievement of the SDGs over a period of time. Our intent is to contribute to make data-driven decisions evident – and not the exception – for all: governments, NGOs, citizens, and corporations.\nWhat does this represent from a business standpoint? Is it corporate responsibility? A new business model? A hybrid? \nJuan: We do not believe very much in a model purely based on corporate social responsibility. However, depending on the focus of the project and other returns, we have exceptionally maintained some partnerships with no economic counterpart. We believe in a hybrid model in which we provide our expertise in aggregated data to third parties and we allocate part of the profits to social good initiative. That allows us to keep the whole activity sustainable.\n\n\n“Our intent is to contribute to make data-driven decisions evident, and not the exception, for all: governments, NGOs, citizens, and corporations.”\n\nHow were you able to sell this idea internally?\nJuan: First of all, we worked very closely with different areas within BBVA to make this kind of projects possible, we collaborated with our legal compliance areas and key stakeholders to make sure we were applying the most rigorous protocols in terms of security and privacy protection.\nSecond, we identified the three key benefits that we felt our involvement in socially responsible projects could yield from a data standpoint, namely:\n\nDriving innovation. When we employ a certain type of data for the first time for a new purpose that opens new perspectives and helps us learn.\nAttracting talent. Invitation to join our team, contributing to socially responsible projects and charitable initiatives and getting involved with the networks that are emerging around them is a source of motivation.\nSetting us apart from other financial institutions, helping us communicate the results of our digital transformation process.\n\nMany people feel that big data is empowering a few people in detriment of larger groups. Has working for the common good changed your relationship with customers?\nJuan: Much like any technological innovation has done in the past, big data has opened the door to a new world of opportunities and threats. Frankly, I don’t think big data is empowering a few at the expense of larger parts of society. On the contrary: cloud technology and data science skills are allowing many startups to disrupt the traditional market share of big players in many fields -from leisure and entertainment, to e-commerce– offering advantages for end-customers, who can now enjoy more affordable and customized services; this can be seen as a democratization enabled by big data solutions.\nRegarding our customers, we follow two guidelines:\n\nWith our responsible data use policy, we comply with the most recent legal standards, and communicate our big data initiatives in a transparent manner.\nWe do our best to bring all those opportunities associated with big data applications to our customers, safeguarding their interests at all times. And we do this through a new range of services: Some of them are aimed at improving their financial skills and helping them to better manage their finances, others try to provide access to lending without increasing risks for the bank, or to provide business intelligence data-based solutions to SMEs. Other projects are aimed at society as a whole. Indeed, we strongly believe that any progress or success achieved in data analytics can bring benefits for society as a whole.\n\nWhat does a successful public-private partnership that seeks to harness data for the benefit of society look like?\nJuan: An ideal public-private partnership is the one that strikes the right balance between all parties’ contributions:\nA public partner ideally knows the problem that needs to be addressed, and the kind of actions that should be supported by the conclusions and results, in order to arrive at a useful data-driven solution that improves the life of those who are vulnerable.\nA private partner generally brings the methodology, often the analytic skills and, sometimes, a private data source (though there are many rich public data sources as well) to the equation. Analysts generally work under the guidance of the public stakeholder, and co-create the insights that the public counterpart can turn into solutions, thanks to their better understanding of the issue, the territory, and its local features.\n\n"
    },
    {
      "title": "Introducing a High Definition Retail Trade Index",
      "url": "https://www.bbvadata.com/es/introducing-a-high-definition-retail-trade-index/",
      "date": "13 enero, 2017",
      "content": "\nBBVA Research is a reference in macroeconomic analysis and economic research in regions where BBVA is present (e.g. Spain, USA, Turkey, South America) and at a geopolitical level. We encourage you to subscribe to their reports and follow them @bbvaresearch. As part of their observations of consumption in Spain, they exploit National Statistics Institute (INE) figures, and rely on several  conjunctural indicators such as the Retail Trade Index. Traditionally, that indicator is based on the monthly survey of 12.500 merchants and takes a full month to get published with figures aggregated at a Autonomous Community level.\nIn response, together with BBVA Research, we engaged in a joint project to drastically improve the geographical and temporal granularity of the analysis of consumption in the domestic market. For the first time, we joined our complementary abilities and aligned our objectives to leverage Big Data generated by consumers, governments and companies with a macroeconomic perspective.\n \n\n \nOur approach converged into three disciplines with great synergies and data as a common material to manipulate:\n\nData Science to create knowledge from large volume of the electronic transactions at a high spatial and temporal resolution.\nTraditional statistics with sampling techniques to remove biases by heterogeneous geographical representation of the BBVA customers.\nEconometrics to develop high frequency structural decomposition  models for time series.\n\n \n“Our results reveal that we are able to anticipate INE’s monthly Retail Trade Index with a more detailed geographical pulse”\n \nThe solution we developed looks into the aggregated information of anonymized transactions carried out with debit or credit card by BBVA customers in Spanish point of sale terminals (PoS). That way, the spatial scope of the analysis covers the activity of more than one million merchants providing a unique descriptive capacity at a municipal level, or even at the neighborhood level. Additionally, using this information source and taking into account the date and time of the purchase we get a timely pulse of sales in the retail sector that allows us to anticipate the publication of the official index. The results show that, when comparing the monthly growth rates of the indexes published by the INE and those constructed from our data, we can prove the existence of a stable and significant  short-term relationship with a correlation that exceeds 95% at both national and regional levels.\n \n\nThis first outcome of our collaboration with BBVA Research indicates that Big Data analytics have a high potential to enrich the study on economic agents decision-making. Currently our approach is being applied in Spain and Mexico, as we are considering other applications in other regions.\nA more detailed description of our approach is available (in Spanish) in BBVA Research’s most recent publication on Consumption in Spain for the second semester of 2016 in the section dedicated to Big Data y consumo: el índice BBVA de comercio al por menor.\n\n"
    },
    {
      "title": "Bye Bye 2016! Hello 2017!",
      "url": "https://www.bbvadata.com/es/bye-bye-2016-hello-2017/",
      "date": "29 diciembre, 2016",
      "content": "\nOur CEO Elena Alfaro has just closed the review of our yearly objectives with a round of applause. This successful 2016 is almost over. We leave this year with countless hours of hard work along with great moments, some of them quantified in the  #funfacts below. I take the opportunity, in the name of the whole BBVA Data & Analytics team, to wish you all the best for 2017!\n\n\n"
    },
    {
      "title": "Pricing Strategy Optimization Considering Customer Sensitivity",
      "url": "https://www.bbvadata.com/es/pricing-strategy-optimization-considering-customer-sensitivity/",
      "date": "28 diciembre, 2016",
      "content": "\nSetting a price for a given customer and product (i.e pricing) is a fundamental problem in the banking sector and is closely related to many financial products, such as insurance or credit scoring. While pricing problems can be solved with standard regression models like Generalized Lineal Models (GLM), a pricing strategy optimization deals with two main components: the pricing model and the customer’s degree of acceptance of a given price. The objectives of the strategy are to:\n\nIncrease retention (probability of acceptance of a given price)\nIncrease revenue. \n\nIn the recent CFE-CMStatistics 2016 conference, we presented a simple framework to calculate the Pareto frontier of several pricing strategies through Random Optimization (RO) driven by a probabilistic model, maximum decay points (MDP) and business constraints. This methodology yields expected retentions and revenues and allows for the testing of new prices. Practically, our contribution:\n\nProposes new prices that dynamically adapt to market necessities.\nUses models that include business constraints and help limit the search space to optimize and proposes fair prices.\nUses parallel processing to simulate multiple different scenarios and to select the best.\n\nWe provide a detailed description of our methodology in Pricing strategy optimization considering customer sensitivity with Monte Carlo simulations, a publication still currently under review. Additionally, the slides of my presentation on pricing optimization is available below:\n\nAs byproduct of that research work we also developed the Holden R library that we hope to make public soon. It was named after pricing theorist Reed Holden.\n\n"
    },
    {
      "title": "Inspiring Data and Analytics Talks of 2016",
      "url": "https://www.bbvadata.com/es/inspiring-data-and-analytics-talks-of-2016/",
      "date": "23 diciembre, 2016",
      "content": "\nWithin BBVA, we govern an active data science online community with regular exchanges of references being from academia, start-ups, GAFA, banking, discussing the latests research publications, model applications, toolkit release, data engineering deployments, trends in visual analytics and more. This is part of our culture to cross-pollinate practices and approaches within our different teams and countries. This year we saw many inspiring talks that we think should be spread widely. Here are a few that we hope you enjoy as much as we did:\nIntroduction to Gaussian Processes by Neil Lawrence\nThis talk gives a brilliant overview of regression and classification model (e.g. GAM, SVM, including some neural networks) that determine Gaussian Processes. It is mathematically robust with the capacity among other things to provide bayesian confidence intervals. Additionally, it gives a bayesian focus on machine learning with a capacity to process large amounts of data (we encourage you to read Gaussian Processes for Machine Learning).\n\nVisual Analysis of Hidden State Dynamics in Recurrent Neural Networks by Hendrik Strobelt\nData visualizations are increasingly being used to break algorithmic black boxes. This line of work comes from the needs of data scientists and society in general to understand what models do and to be able to explain their implications. Currently we only know that they work. This presentation of LSTMVis shows a visual analysis tool for recurrent neural networks with a focus on understanding their hidden state dynamics.\n\nDesign as a Search Problem by Mike Bostock\nIn this fascinating talk Mike Bostock establishes design as a “search problem” and discusses some strategies for addressing this problem that can help make better and more efficient design data visualizations.\n\nThe Cost of Missing Something by Tricia Wang\nWhy do so many companies make such bad decisions? Tricia Wang demystifies the pitfalls of big data, with stories from Nokia to Netflix to the Oracles of Ancient Greece, and shares how integrating “thick data” can help companies avoid bad decisions and thrive in the unknown. Among other things, Tricia runs the resourceful Ethnography Matters.\n\nGoodCityLife.org by Daniele Quercia\nIn his Databeers talk, Daniele Quercia presents a human approach of wayfinding using data analytics that suggests sensorial and emotional routes to go from A to B in a city. The aesthetic layer trained as part of a game and that exploits Google Street View imagery. Find out more about Smelly Maps, Happy Maps etc in Good City Life.\n\nWe hope you enjoyed these talks and hope to share more in 2017!\n\n"
    },
    {
      "title": "Measuring People’s Economic Resilience to Natural Disasters",
      "url": "https://www.bbvadata.com/es/measuring-peoples-economic-resilience-to-natural-disasters/",
      "date": "20 diciembre, 2016",
      "content": "\nWe have been publishing research papers demonstrating that bank card transaction data is a resource that can provide insights into economic activity in a more timely fashion and with greater granularity than traditional methods. For instance:\n\nScaling of city attractiveness for foreign visitors through big data of human economical and social media activity. https://arxiv.org/abs/1504.06003\nMoney on the move: Big data of bank card transactions as the new proxy for human mobility patterns and regional delineation. the case of residents and foreign visitors in spain. http://ieeexplore.ieee.org/document/6906771\nMining Urban Performance: Scale-Independent Classification of Cities Based on Individual Economic Transactions. https://arxiv.org/abs/1405.4301\nPredicting regional economic indices using big data of individual bank card transactions. https://arxiv.org/abs/1506.00036\nEvaluating uncertainty scores for deep regression networks in financial short time series forecasting. https://www.bbvadata.com/there-is-no-such-thing-as-a-certain-prediction/\n\nThis year, as part of our Data for Social Good initiative, we collaborated with United Nations Global Pulse to create new approaches of big data analytics for humanitarian action. Our results showed a replicable and evidence-based approach to understanding economic vulnerability:\nMartinez, E. A., Rubio, M. H., Maestre-Martínez, R., Arias, J. M., Patane, D., Zerbe, A., Kirkpa- trick, R., and Luengo-Oroz, M. A. Measuring economic resilience to natural disasters with big economic transaction data. CoRR abs/1609.09340 (2016)   https://arxiv.org/abs/1609.09340\n \nHere is an approximate transcript of my presentation of that research work at the Bloomberg Data For Good Exchange conference in New York.\nHello! My name is Maria, I am a data scientist, member of BBVA Data & Analytics, an excellence center in financial data analytics. We all know that floods and earthquakes, hurricanes or fast-moving epidemics all represent a risk to economic sustainability of regions. I will present you the results of an analysis of aggregated bank card transactions to measure the economic impact of Hurricane Odile that hit the Baja California Sur peninsula in Mexico in 2014. That work was conducted in collaboration with United Nations Global Pulse and BBVA Bancomer.\n \n\n \nIf we look at Baja California Sur in 2014, we can observe that BBVA Bancomer customers generally created around 15.000 card transactions every day. This is the pulse of the peninsula. Its rhythm looks normal until Hurricane Odile hits the area. Odile was a category 3 hurricane, with wind speed reaching 200 km per hour. It proved one of the most destructive hurricane in the recent years. With that observation we asked ourselves:\n\nCan we measure the economical impact of a natural event?\nCan we measure how people change habits in order to prepare for a disaster?\nDifferent groups act or recover in different ways? Which are the most vulnerable groups?\n\nHere is how we answered these questions:\nBBVA Bancomer is the main financial institution in Baja California Sur with 25.000 card payments per day, including Point of Sales and ATM cash withdrawals. This means that the studied population represented 30% of all bank account holders (i.e. approximately 100.000 active customers) with annonymized and aggregated transactions, grouped by gender and income level.\n \n\n \nThe effect of the hurricane on card payments is big, dropping near zero (blue line). By looking at other unaffected regions in Mexico (gray lines), we built a model that estimate the expected activity in Baja California Sur by comparing with the ones that had the most similar behavior in the past.\nUsing that estimation, we can measure how different the real and the expected series are. The difference represents the activity that did not occur because of the hurricane. Practically, 30% fewer card transactions and 12% less cash withdrawn were made in the month after the hurricane.\n \n\n \nAs it is a hurricane, the population was aware of the moment of the landfall and took the necessary actions. By looking at how consumption patterns changed the day before the hurricane struck, we can see that women spent almost twice as much as men compared to the expected consumption of both groups.\nIn terms of payment categories, groceries and fuel transactions increased by 20% and other less vital activities, such as restaurants, decreased by almost 40%. This is a pretty obvious result that endorses that our model is able to capture the people’s pattern and strengths our findings. Additionally, we were able to measure that people take in general 22 days to resume their normal payment activity.\n \n\n \nHowever, different areas recovered at different rhythms. Here you can see that southernmost regions were more deeply affected. For example, San José del Cabo needed 40 days to recover whereas Mulegé in the north is barely affected with only 2 days to get back to normal activity. Notice also that La Paz, very close to Todos los Santos, is not as affected as expected potentially because it is a fairly large city and, therefore, more resilient. In terms of income, individuals with a higher income took longer than others to recover. You can find these and many other results in the interactive data story of where you can explore more about the story of this hurricane and our study.\n \nI also invite you to look and share the video of the study.\n\n \nIn this research, we have demonstrated that we can understand the impact of a hurricane at different stages of its evolution. Based on transactional data, we can estimate the preparation the days before, the local economic impact and the recovery after a shock. We believe that these types of insights can help authorities improve community resilience, which benefits the vulnerable and is also good for business continuity.\nThis is the first study of its class and further research is needed to understand how different disasters affect the population and whether they share a similar pattern.  We also think that building tools that help to monitor this data in real time will drastically reduce the uncertainty of the decisions made on the ground.\n \nThank you and thanks to United Nations Global Pulse and BBVA Bancomer for their collaboration that made this work possible.\n\n"
    },
    {
      "title": "Towards a Data-Driven Economy in Europe",
      "url": "https://www.bbvadata.com/es/towards-a-data-driven-economy-in-europe/",
      "date": "13 diciembre, 2016",
      "content": "\nThe implications of the so-called Fourth Industrial Revolution (i.e. the fusion of technologies that is blurring the lines between the physical, digital, and biological spheres) across policymaking are hard to fully measure. Recently I was invited by the think tank Friends of Europe to explore, in collaboration with other stakeholders, the steps that must be taken to enhance transatlantic cooperation, and also to help Europe shape how its digital skills, labor market and regulatory framework should evolve. For instance, Andrus Ansip, European Commission Vice-President for the Digital Single Market, went through some of the achievements Europe accomplished, like the elimination of roaming within the EU, and the Free Data Flow Initiative. However he pointed out that the biggest obstacle for a true digital single market are the Member States and their domestic policies.\nMy contribution discussed the role of data in the digital economy, and on how Europe could take a step further to allow the data-driven economy flourish. Here a summary of the 10 aspects I think need particular focus to deal with data and analytics innovation and data protection in a coordinated way:\n\nCitizens data literacy, as today we are living a dual situation where people give away their data without control, but at the same time complain about data usages that might be good for society, like investigating with health data.\nLinked to that, there is a need for more transparency from companies and governments in how they use data. This should be promoted and seen as a business advantage, and not just something to comply with legally.\nData security research coordination, so we get to global standards that warrantee protection against cybercrime. For example using blockchain technologies for distributed identity and data management.\nData flow and data storage. There is a good effort here with the free data flow initiative, but still we experience many stoppers and delays when we want to use cloud storage and cloud tools.\nAlgorithm protection and interpretability. Investments in algorithm development are high, and the companies should be able to decide if they want to protect them or open them up. Interpretability of algorithms is also mandatory in the General Data Protection Regulation (GDPR), but here further discussions should be allowed, so the industries can work on explaining complex models that work better that those that are simpler and therefore easier to understand for a human.\nData anonymisation and the usage of anonymised & aggregated data. There are many opportunities in the usage of this type of non-personal data, both for companies and for the society as a whole, but still lack of clarity on the cases where this is allowed.\nLink to previous aspect, open data policies, not only for the public sector, and promoting that this actually happens. Today a company that opens its data has many more risks than rewards, so this is why almost no one is doing it.\nThe right to data portability in all sectors, as a way to promote fair competition and innovation in data services. This data portability will be mandatory in 2018 for the financial sector according to the PSD2 regulation, but not for the rest of the sectors, although GDPR introduces it as an important aspect data innovation.\nThe harmonization of data legislations in all the state members, so a digital businesses can operate in all of them with no frontiers. Today a digital company has to comply with 25 different regulations if it wants to operate in the EU. This is affordable for the digital giants, but prohibitive for a European startup.\nHow AI will impact labor markets, and how can we help them adapt as fast as we can.\n\nThis is a non-exhaustive list of regulatory, cultural and educational factors that in my opinion have to be considered at the same time by the EU, the Member States and the companies operating in Europe. I look forward to continue the discussion in similar events that will take place during spring 2017.\n \n\n"
    },
    {
      "title": "Experience Design in the Machine Learning Era",
      "url": "https://www.bbvadata.com/es/experience-design-in-the-machine-learning-era/",
      "date": "8 diciembre, 2016",
      "content": "\nTraditionally the experience of a digital service follows pre-defined user journeys with clear states and actions. Until recently, it has been the designer’s job to create these linear workflows and transform them into understandable and unobtrusive experiences. This is the story of how that practice is about to change.\nOver the last 6 months, I have been working in a rather unique position at BBVA Data & Analytics, a center of excellence in financial data analysis. My job is to make the design of user experiences reach a new frontier with the emergence of machine learning techniques. My responsibility — among other things — is to bring a holistic experience design to teams of data scientists and make it an essential part of the lifecycle of algorithmic solutions (e.g. predictive models, recommender systems). In parallel, I perform creative and strategic reviews of experiences that design teams produce (e.g. online banking, online shopping, smart decision making) to steer their evolution into a future of “artificial intelligence”. Practically, I boost the partnerships between teams of designers and data scientists to envision desirable and feasible experiences powered by data and algorithms.\n\nTogether we are defining a different kind of experience design with systems that learn from human behaviors. I believe this is a new practice because:\n\nIt creates new types of user experiences.\nIt redefines the relation between humans and machines.\nIt requests a tight partnership between designers and data scientists.\n\nLet me describe each of these implications.\n1. The new types of user experiences\nNowadays, the design of many digital services does not only rely on data manipulation and information design but also on systems that learn from their users. If you would open the hood of these systems, you would see that behavioral data (e.g. human interactions, transactions with systems) is fed as context to algorithms that generates knowledge. An interface communicates that knowledge to enrich an experience. Ideally, that experience seeks explicit user actions or implicit sensor events to create a feedback loop that will feed the algorithm with learning material.\n\nLet me give you a practical example. Do you know how Spotify Discovery Weekly works?\nDiscovery Weekly is Spotify’s automated music recommendations “data engine” that brings two hours of custom-made music recommendations, tailored specifically to each Spotify user every Monday.\nSpotify Discovery Weekly explained. Schema strongly inspired by The magic that makes Spotify’s Discover Weekly playlists so damn good.\nThe Discover Weekly’s recommender system leverages the millions playlists that Spotify users create. It gives extra weight to the company’s own experts playlists and those with more followers. The algorithm attempts to augment a person’s listening habits with those with similar tastes. It does it in three main tasks:\n\nOn one side, Spotify creates a profile of each user’s individualized taste in music, grouped into clusters of artists and micro-genres.\nOn the other side, Spotify uses the billion of playlists to build a model of all the music they know about based on all the songs people group into playlists.\nEvery week it connects the knowledge of music built with each user personal taste profile. Basically, if a favorite song tends to appear on playlists along with a third song not heard before, it will suggest that new song.\n\nA typical Discover Weekly playlist recommends 30 songs, a big enough set to discover music that matches with a personal taste among other false positives. That experience provokes the curation of thousands of new playlists that are fed back into the algorithm a week after to generate new recommendations.\nThese feedback loop mechanisms typically offer ways to personalize, optimize or automate existing services. They also create opportunities to design new experiences based on recommendations, predictions or contextualization. At BBVA Data & Analytics I came up with a first non-comprehensive list:\n1.1. Design for discovery\nWe have seen that recommender systems help discover the known unknown or even the unknown unknowns. For instance, Spotify helps discover music through a personalized experience defined on the match between an individual listening behavior and the listening behavior of hundreds of thousands of other individuals. That type of experience has at least three major design challenges.\nFirst, recommenders systems have a tendency to create a “filter bubble” that limits suggestions (e.g. products, restaurants, news items, people to connect with) to a world that is strictly linked to a profile built on past behaviors. In response, data scientists must sometimes tweak their algorithms to be less accurate and add a dose of randomness to the suggestions.\nSecond, it is also good design practice to let an open door for users to reshape aspects of their profile that influence the discovery. I would call that feature “profile detox”. Amazon for example allows users to remove items that might negatively influence the recommendations. Imagine the customers purchase gifts for others and those gifts are not necessarily material for future personalized recommendations.\nFinally, organizations that rely on subjective recommendation like Spotify now enlist humans to give more subjectivity and diversity to the suggested music. This approach of using humans to clean datasets or mitigate the limitations of machine learning algorithm is commonly called “Human Computation” or “Interactive Machine Learning”.\n1.2. Design for decision making\nData and algorithms also provide means to personalize decision making. For instance at BBVA Data & Analytics we developed advanced techniques to advise BBVA customers on their finance.\n\nFor example, we consider the temporal evolution of account balances to segment savings behaviors. With that technique we are able to personalize investment opportunities according to each customer’s capacity to save money.\nThis type of algorithms that leads to decision-making needs to learn to be more precise, simply because they often rely on datasets that only give a perspective of reality. In the case of financial advisory, a customer could operate multiple accounts with other banks preventing a clear view on on saving behaviors. It proved a good design practice to let users tell implicitly or explicitly about poor information. It is the data scientist’s responsibility to express the types of feedback that enrich their models and the designer’s job to find ways to make it part of the experience.\n1.3. Design for uncertainty\nTraditionally the design of computer programs follows a binary logic with an explicit finite set of concrete and predictable states translated into a workflow. Machine learning algorithms change this with their inherent fuzzy logic. They are designed to look for patterns within a set of sample behaviors to probabilistically approximate the rules of these behaviors (see Machine Learning for Designers for a more detailed introduction to the topic). This approach comes with a certain degree imprecision and unpredictable behaviors. They often return some information on the precision of the information given.\n \nThe beautiful seams of the Kayak Price Trend algorithm with a confidence level for the purchase advise\nFor example the booking platform Kayak predicts the evolution of prices according to the analysis of historical prices changes. Its “farecasting” algorithm is designed to return confidence on whether it is a favorable moment to purchase a ticket (see  The Machine Learning Behind Farecast). A data scientist is naturally inclined to measure how accurately the algorithm predicts a value: “We predict this fare will be x”. That ‘prediction’ is in fact an information based on historical trends. Yet predicting is not the same as informing and a designer must consider how well such a prediction could support a user action: “Buy! this fare is likely to increase”. The ‘likely’ with an overview of the price trend is an example of a “beautiful seam” in the user experience, a notion coined by Mark Weiser at the time of the Xerox Palo Alto Research Center and further developed by Chalmers and MacColl as seamful design:\n“Seamful design involves deliberately revealing seams to users, and taking advantage of features usually considered as negative or problematic”.\nSeamful design is about exploiting failures and limitations to improve the experience. It is about improving the system allowing users to tell about poor recommendations. DJ Patil describes subtle techniques in Data Jujitsu.\n\nOther types of machine learning algorithms communicate the seams with scores of precision and recall.\n\nThe precision score communicates the ability to provide a result that exactly matches what’s desired.\nThe recall score communicates the ability to provide a large set of possible good recommendations.\n\nThe ideal for an algorithm is to deliver high precision and recall scores. Unfortunately, precision and recall often work against each other. There is often a need to take design decisions with the trade-off between precision versus recall. For instance, in Spotify Discovery Weekly, a design decision had to be taken to define the size of playlists according to the performance of the recommender system. A large playlist highlights the confidence of Spotify to deliver a rather large inventory of 30 songs, a wide-enough set to increase the opportunities for users to stumble on perfect recommendations.\n1.4. Design for engagement\nToday, what we read online is based on our own behaviors and the behaviors of other users. Algorithms typically score the relevance of social and news content. The aim of these algorithms is to promote content for higher engagement or send notifications to create habits. Obviously these actions taken on our behalf are not necessarily for our own interest.\nIn the attention economy, both designers and data scientists should learn from the anxieties, obsessions, phobias, stress and other mental burdens of the connected humans. Source: The Global Village and its Discomforts. Photo courtesy of Nicolas Nova.\nArguably, we entered into the attention economy, and major online services are fighting to hook people, grap their attention for as long as possible. Their business is to keep users active as long and frequently as possible on their platforms. This leads to the development of sticky, needy experiences that often play with emotions like Fear of Missing Out (FoMO) or other obsessions to dope the user engagement.\nThe actors of the attention economy use also techniques that promote addiction such as Variable Schedule Rewards. It is the exact same mechanisms as the ones used in slot machines. The resulting experience promotes the service’s interest (the casino) hooking people endlessly searching for the next reward. Our mobile phones have become those slot machines of notifications, alerts, messages, retweets, likes, that some of us check on an average 150 times per day if not more. Today designer can use data and algorithms to exploit cognitive vulnerabilities of people in their everyday lives. That new power raises the need for new design principles in the age of machine learning (see  The ethics of good design: A principle for the connected age).\nYet the experience designed with machine learning algorithms do not need to be the one of a casino.\n1.5. Design for time well spent\nThere are opportunities to design a radically different experience than engagement. Indeed, an organization like a bank has the advantage of being a business that runs on data and does not need customers to spend the maximum amount of time with their services. Tristan Harris’ Time Well Spent movement is particularly inspiring in that sense. He promotes the type of experience that use data to be super-relevant or be silent. The type of technology to protect the user focus and to be respectful of people’s time. The Twitter “While you were away…” is a compelling example of that practice. Other services are good at suggesting moments to engage with them. Instead of measuring user retention, that type of experience focuses on how relevant the interactions are.\n\n1.6. Design for peace of mind\nData scientist are good in detecting normal behavior and abnormal situations. At BBVA Data & Analytics we are working to promote a peace of mind to BBVA customers with mechanisms that gives a general awareness when things are fine and that trigger more detailed information on abnormal situations. More generally, we believe current generation of machine learning brings new powers to society, but also increases the responsibility of their creators. Algorithmic bias exists and may be inherent to the data sources. In consequence, there is a particular need to make algorithms more legible for people and auditable by regulators to understand their implications. Practically, this means knowledge that the an algorithm produces should safeguard the interest of their users and the results of the evaluation and the criteria used should be explained.\nAdditional related terrains of experience design are:\n\nDesign for fairness\nDesign for conversation\nDesign for automation\n\nAnd probably many more…\n2. The new relation between humans and machines\nIn the previous section we have seen that the experiences powered by machine learning are not linear or based on static business and design rules. They evolves according to human behaviors with constantly updating models fed by streams of data. Each product or service becomes almost like a living, breathing thing. Or as people at Google would say: “It’s a different kind of engineering”. I would argue that it is also a different kind of design. For instance, Amazon explains Echo’s braininess as a thing that “continually learns and adds more functionality over time”. This description highlights the need to design the experience for systems to learn from human behavior.\nThe design for machines to learn. Image inspired by Mike Kuniavsky: The UX of Predictive Behavior for the IoT.\nConsequently, beyond considering the first contact and the onboarding experience, that type of product or service requires considerations on their use after 1 hour, 1 day, 1 year, etc. If you look at the promotional video of the Edyn garden sensor you will notice the evolution of the experience from creating new habits for taking care of a garden to communicating the unknown unknowns about plants, to convey peace of mind on the key metrics, and to guarantee time well spent with some level of watering automation.\n\nThat type of data product requires a responsible design that considers moments when things start to disappoint, embarrass, annoy or stop working or being useful. The design of the “offboarding experience” could become almost as important as the “onboarding experience”. For instance, allegedly a third of the Fitbit users stop wearing the device within 6 months. What happens to these millions of abandoned connected objects? What happens to the data and intelligence on the individual they produced? What are the opportunities to use them in different experiences?\nProducts characterized by an experience that evolves according to behavioral data that constantly feed algorithms (e.g. Fitbit) are living products that inevitably also have a tendency to die. Source: The Life and Death of Data Products.\nThere are new ways to imagine the relation after a digital break-up with a product. Digital services work on an increasingly vast ecosystem of things and channels but user data have a tendency to be more centralized. Think about the notion of portable reputation that allows people to use a service based on the relation measured with another service.\nLooking a bit further into the near future, the recent breakthrough in Natural Language Processing, Knowledge Representation, Voice Recognition and Nature Language Production could create more subtle and stronger relations with machines. In a few iterations, Amazon Echo might start to be much more nurturing. A potential evolution that anthropologist Genevieve Bell foresees a shift from human-computer interactions to human-computer relationships in The next wave of AI is rooted in human culture and history:\n“So the frame there is not about recommendations, which is where much of AI is now, but is actually about nurture and care. If those become the buzzwords, then you sit in this very interesting moment of being able to pivot from talking about human-computer interactions to human-computer relationships.” — Genevieve Bell\nIn this section we have seen that algorithms are getting closer to our everyday lives and that data provide a context for an evolving relationship. The implications of that evolution require most intense collaboration between design and data science.\n3. The partnership between designers and data scientists\nMy experience so far envisioning experiences with data and algorithms shows that it is a different practice from current human-centered design. At BBVA Data & Analytics, the role of data scientists has been elevated from reactive model and A/B test developers to proactive partners who think about the implications of their work. Our singular data science teams breaks into sub-teams that partner more directly with engineers, designers, and product managers.\n3.1. When design meets science\nAt the moment of shaping an experience, we exploit thick data, the qualitative information that provides insights on people’s lives (see  Why Big Data Needs Thick Data), big data from the aggregated behavioral data of millions of people and the small data that each individual generates.\nClassically, designers focus on defining the experience of the service, feature or product. They nest the concept within the larger ecosystem that relates to it. Data scientists develop the algorithms that will support that experience and measure it with A/B testing.\nThe first few weeks in my role at BBVA Data & Analytics, I found designers and data scientists often stuck in deadlocked exchanges that typically sounded like this:\nDesigner: Hello! What can you, your data and algorithms tell me?\nData scientist: Well… What do you want to know?\nThe main issue was the lack of shared understanding of each other’s practice and objectives. For instance, designers transform a context into a form of experience. Data scientists transform a context with data and models into knowledge. Designers often adopt a path that adapts to a changing context and new appreciations. Data scientists employ processes similar to humber-center design but are more mechanical and less organic. They strictly follow the scientific methods with its cyclical processes of constant refinement.\nA properly formulated research question helps define the hypothesis and the types of models to develop in the prototyping phase. The models are the algorithms that get evaluated before they are deployed to production into what we call at BBVA Data & Analytics a “data engine”. Whenever the experience supported by the “data engine” does not perform as expected, the problem needs to be reformulated to continue the cyclical process of constant refinement.\nThe data science method and its cyclical processes of constant evaluation and refinement\n3.2. The touchpoints\nThe scientific method is similar to any design approach that forms and makes new appreciations as new iterations are necessary. Yet, it is not an open-ended process. It has a clear start and end but no definite timeline. Data scientist Neal Lathia argues that “cross-disciplinary work is hard, until you’re speaking the same language”. Additionally, I believe designers and data scientists must immerse themselves in the other’s practice to build a common rhythm. So far, I codified several important touchpoints for designers and data scientists to produce a meaningful user experience powered by algorithms. They must:\n\nCo-create a tangible vision of the experience and solution with priorities, goals and scope\nAssess any assumption with insights from quantitative exploration, desk research and field research.\nArticulate the key questions from the vision and the research. Is the team asking the right questions and are the answers algorithms could give actionable?\nUnderstand all the limitations of the data model that gives answers.\nSpecify the success metrics for a desirable experience and define them before the release of a test. The validation phase acts as stopping point and it must be defined as part of the objectives of the project (e.g. improve the recall of the recommendations by 5%, detect 85% of customer who are about to default).\nEvaluate the impact of the data engine on the user experience. As stated by Neal Lathia, it is particularly hard for data scientists to work “offline” on an algorithm and measure improvements that will correlate with improvements in the actual user experience.\n\nThis intertwined collaboration illustrates a new type of design that I am trying to articulate. In a recent article Harry West CEO at frog suggested the term ‘design of system behavior’:\n“Human-centered design has expanded from the design of objects (industrial design) to the design of experiences (adding interaction design, visual design, and the design of spaces) and the next step will be the design of system behavior: the design of the algorithms that determine the behavior of automated or intelligent systems” — Harry West\n3.3. A vision-driven partnership\nSo far I have argued that “living experiences” emerge at the crossroad of data science and design. An indispensable first step is for designers and data scientists is to establish a tangible vision and its outcomes (e.g. experience, solution, priorities, goals, scope and awareness of feasibility). Airbnb Director of Product Jonathan Golden calls that a vision-driven product management approach:\n“Your company vision is what you want the world to look like in five-plus years — outcomes are the team mandates that will help you get there.” — Jonathan Golden\nHowever, that conceptualization phase requires that visions live not just as flat perfect things for board room PowerPoint. Therefore, one of my approaches is to engage the design/science partnership to produce Design Fictions. It has similarities with Amazon’s Working Backward’ process as described by Werner Vogels:\n“You start with your customer and work your way backwards until you get to the minimum set of technology requirements to satisfy what you try to achieve. The goal is to drive simplicity through a continuous, explicit customer focus.” — Werner Vogels\nThinking by doing with Design Fiction creates potential futures of a technology to clarify the present. Schema inspired by the Futures Cones and Matt Jones: Jumping to the End — Practical Design Fiction.\nDesign Fiction aims at making tangible the evolution of technologies, the language used to describe them, the rituals, the magic moments, the frustrations, and why not the “offboarding experience”. It helps the different stakeholders of a project to engage with essential questions to understand what the desired experience means and why the team should build it. What are the implications of purchasing that next generation Garden Sensor? What can you do with it? What aren’t you allowed to do? What won’t you do anymore? How does a human interact with that technology the first time, and then routinely after a month, one year or more? Creative and tangible answers to these questions can come to life before a project even starts with the creation of fictional customer reviews, user manual, press release, ads. That material is a way to bring the future to present or as we say at the Near Future Laboratory:\n“The Design Fictions act as a totem for discussion and evaluation of changes that could bend visions of the desirable and planning of what is necessary.”\nAt BBVA Data & Analytics, this means that I gather data scientists and designers with the objective of creating a tangible vision of their research agenda. First, we first map the ongoing lines of investigations.\n\nThen we project their evolution into 2 or 3 iterations wondering: What would the potential resulting technology look like? Where could it be used? Who would use it and for what type of experience? Each participant uses the template of a fictional ad to tell stories with practical answers to these questions. Together we group them into future concepts.\n\nWe collect all the material and promote the most promising concepts. After that, we share these results internally in series of paper and video advertisements that describe the main features, attributes, characteristics of the experience from our point of view (the feasible) and the user’s point of view (the desirable).\n\nThis type of fictional material allows both designers and data scientists to feel and get a practical understanding of the technology and its experience. The results help build credibility, enlist support, counter skepticism, create momentum and share a common vision. Finally, the feedback of people with different perspectives allows to anticipate opportunities and challenges.\n4. The design characteristics\nWith the advance of machine learning and “artificial intelligence” (AI), it became the responsibility of both designers and data scientists to understand how to shape experiences that improve lives. Or as Greg Borenstein argues in Power to the People: How One Unknown Group of Researchers Holds the Key to Using AI to Solve Real Human Problems:\n“What’s needed for AI’s wide adoption is an understanding of how to build interfaces that put the power of these systems in the hands of their human users.” — Greg Borenstein\nThat type of design of system behavior represents a future in the tight partnership between design and data science. So far in that journey of creating meaningful experiences in the machine learning era, I can articulate the following characteristics:\n\nFeedback: Data is the lifeblood of the user experience with systems that learn. Guarantee that systems are properly fed with carefully crafted feedback loop mechanisms.\nRelationship: The combination of data and learning algorithms can trigger an evolution of multiple experiences. Plan the relationship between humans and the machine that learns for instance creating habits aligned with people’s interest, finding the known unknown, discovering the unknown unknowns, communicating a certain peace of mind, or valuing time well spent. Additionally, prepare the offboarding experience for moments in the relationship when things start to disappoint, embarrass, annoy or stop working or being useful.\nSeamfulness: Consider bringing both the power and imperfections of algorithms to the surface as part of the experience. For example, predicting is not the same as informing and a designer must consider how well the level of uncertainty in a prediction could support a user action.\n\nThis is an extended transcript of a talk I gave at the Design Wednesdays event at the BBVA Innovation Center in Madrid on September 21, 2016. Many thanks to the BBVA Design team for their invitation and the quality of the organization!\nThanks to Leonardo Baldassini, Neal Lathia and Nicolas Nova for their feedback on this text.\n\n"
    },
    {
      "title": "There is no Such Thing as a Certain Prediction",
      "url": "https://www.bbvadata.com/es/there-is-no-such-thing-as-a-certain-prediction/",
      "date": "8 diciembre, 2016",
      "content": "\nOne question that naturally springs up when imagining what Artificial Intelligence (AI) can bring to the banking industry, and one that we get asked fairly often, is: Can you predict people’s expenses? As it is often the case, such a simple question is in fact only apparently simple. The prediction of personal financial transactions may range from estimating the amount of your next electricity bill (a simple problem, in most cases), all the way to guessing the time and amount of your next ATM withdrawal (a seemingly impossible task, in most cases).\nWith all its inherent difficulties, anticipating the behaviour of personal accounts is a challenge that holds a special place in any modern platform of intelligent banking services. For this reason, we have been investigating the best way to tackle this problem. This week we are presenting some results in the Workshop on Machine Learning for Spatiotemporal Forecasting as part of the NIPS 2016 conference. Our contribution is entitled Evaluating uncertainty scores for deep regression networks in financial short time series forecasting. Here is a walk-trough:\nKeep it short\nTo use a jargon common to both statistics and Machine Learning, we were facing a time-series regression problem, something that, normally, any trained statistician would not even blink at. However, a detail made our problem setting perceivably harder than usual: Its coarseness. In the analysis of personal financial transactions, it often makes sense to aggregate data at a monthly timescale, since many important financial events only happen with a monthly cadence (think payroll), or a multiple thereof (think utility bills), or would introduce excessive noise in the data if considered at a finer level (think grocery shopping). Furthermore, we wanted to be able to predict expenses for as many clients as possible, including those having relatively short financial histories. This forced us to work on just a year-worth of data with monthly aggregations. In other words, for each client and transaction category, we only had 12 historical values. We had to guess the 13th.\nWhile this class of problems has been widely studied, especially for long series, some of the best-suited statistical methods for these prediction problems, such as Holt-Winters or ARIMA, tend to produce poor results on such short time series. This led us to look at our problem from a different perspective, that of Machine Learning.\nIf it flies like a duck…\nInstead of trying to predict the 13th value in our series solely by looking at each of them individually, as classical time series methods would normally do, we adopted a common underlying principle in many Machine Learning algorithms:\nIf it looks like a duck, swims like a duck and quacks like a duck, then it probably is a duck.\nThis disarmingly informal statement (sometimes referred to as the duck test) is a humorous phrasing of a common methodology of machine learners: If a target time series is sufficiently similar to training  time series whose 13th value is known, then the behaviour observed in the training series should help us guess how the target series will evolve.\nWe therefore started experimenting with classic approaches directly leveraging this principle, such as Nearest Neighbour regression and Random Forest regression. Worryingly, under most of our error metrics these supposedly cleverer methods were severely losing to much naiver solutions such as predicting by taking an arithmetic mean of the series, or simply by using the value 0 for all predictions (an effect of the sparsity of many financial time series). It was clear that in those short 12-values sequences, there was more structure than these methods were able to capture.\nIn order to squeeze as much information as possible from our short series, in a collaboration with the University of Barcelona we developed a deep neural network which employed Long Short-Term Memory (LSTM) units in order to exploit their power to remember past information, a property that makes them useful tools to learn sequential structures.\nThe margin of improvement was evident.The next figure compares error and success metrics of different prediction methods, including our LSTM network.\n\nAn odd duck\nWhile greatly improved, the error-metrics values were still high. Higher than you’d want for any client-facing application ready to be released into the world.\nAn often underestimated implication of the duck-test approach is that if a target series doesn’t really have any other series that looks akin, many algorithms will still stubbornly output a prediction – only, it will be unreliable. It goes without saying that an uncertainty score (or confidence value) accompanying a prediction is a highly desirable feature for an algorithm, but one that deep networks, including ours, usually lack (a matter of very active research).\nIn order to estimate the uncertainty of our deep-network predictions, we turned our attention to a few quantities we could compute that were reminiscent of the concept of uncertainty. For example, while computing the distance to the nearest time series was not the most accurate prediction method, the distance to the most closely-looking series can be considered a proxy of prediction uncertainty (duck test!), as can the reconstruction error of a marginalised denoising autoencoder or the distance between the representation of a series that can be found in the last layer of our LSTM network (an idea that proved greatly successful in the field of Computer Vision). To further add to our list of uncertainty-estimation methods, we also tried a supervised approach by training a Random Forest to learn prediction errors directly.\nFinally, we applied the duck-test principle to prediction models, rather than to the data: If a target series gets similar predictions from models that have been partially corrupted by noise, this means that the original model is sufficiently confident of its knowledge of the series’s pattern. We implemented this approach by applying dropout noise on the trained network, hence bootstrapping 95% confidence intervals on our predictions.\nFor the benchmark of confidence calculation methods we used Mean Absolute Relative Error vs coverage (fraction of non-rejected samples) curves. Intuitively, this means that methods producing better prediction-uncertainty estimates keep the MARE low for larger proportions of the test set. Note that this allows us to filter out the series which the algorithm is not confident about, leaving us with a much cleaner dataset to use in production environments.\nWe compared the different confidence calculation methods we implemented with simple rejection baselines, such as estimating uncertainty only by looking at the absolute value of predictions or by a completely random value. Perhaps unsurprisingly, given its supervised nature, random forest regression achieved the best confidence estimation. Among unsupervised methods, the best estimator we evaluated is the distance to the n-th neighbour in the space of embeddings produced by the last dense layer of the regression network. We found it interesting that the density in the network embedding space is a relatively good proxy of the uncertainty.\n\nCiprian, M., Baldassini, L., Peinado, L., Correas, T., Maestre, R., Rodriguez Serrano, J.A., Pujol, O., Vitrià, J. Evaluating uncertainty scores for deep regression networks in financial short time series forecasting. Poster at the Workshop on Machine Learning for Spatiotemporal Forecasting, in Neural Information Processing Systems, 2016.\n\n"
    },
    {
      "title": "Big Data is the Answer, But What is the Question?",
      "url": "https://www.bbvadata.com/es/big-data-is-the-answer-but-what-is-the-question/",
      "date": "1 diciembre, 2016",
      "content": "\nThis year, the knowledge we derive from financial data directly reached the hands of customers. Our work helps transform a bank into a data-driven business. We are always thrilled to share our experience with data providers, data technology providers, researchers and other actors in the data economy. In a recent Big Data Value Association Summit in Valencia the organizers asked us to describe our success story. They particularly wanted to learn from the biggest challenges that are limiting applications of data in the retail banking industry. Here is an a summary of my point of view:\nThe challenge is not about technology, but mainly about culture change. Let me explain. Our capacity to derive knowledge from financial data (e.g. customer knowledge, customer advisory, fraud detection, contextual business knowledge) emerged from connecting the right datasets with the right talents to ask the right questions. In an industry that has suffered from marketing myopia, retail banks need to adopt a digital culture to understand what is feasible with big data technologies and what is desirable to match the new needs of customers. Today, banks have been convinced that big data is an answer to their future success, but they struggle to articulate the critical questions. Beyond the proper use of technologies and techniques to creating value from big data, our approach has been to provoke a culture change. A couple of keys to our success have been to:\n\nProve that data is one of the bank’s main asset, but the real value is in the talent and domain knowledge of the data science team.\nDevelop a community of practice that actively educates new talents to shape and strengthen a data-driven organization.\nCommunicate the data science practice dismantling the myths on the data-driven bank, sharing practical results and engaging the organization with visions.\n\nThese efforts — among others — concretely helped us align objectives, coordinate efforts and drive BBVA in the successful application of data and analytics.\nThanks to the Big Data Value Association Summit organizers and our friends at Instituto Tecnológico de Informática in Valencia for their invitation!\n \nAlso, you can see below the post-produced video from BDVA Summit where all the interviews are included:\n\n\n"
    },
    {
      "title": "Mapping Talent at Big Data Spain",
      "url": "https://www.bbvadata.com/es/mapping-talent-at-big-data-spain/",
      "date": "29 noviembre, 2016",
      "content": "\nWe are strong believers that in the analytics business, more than any other business, the value of the company is dependent on the talent of the employees. At BBVA Data & Analytics that is our competitive advantage. We are a team of people passionate about the latest technology, open source and innovative problem solving. We mix data scientists that love mathematics and algorithms, designers who communicate the world with insightful visualizations, and of course business people who thrive by bringing solutions to the hand of customers.\nWhen we attend leading events we expect the meet and exchange with similar profiles. But since we have analytical and visual minds we always want to discover more about the definition of talent in the analytics business. At the recent Big Data Spain (BDS16), we used an engaging approach in the form of a giant fingerprint that captured the skills in data science from a small sample of the more than 1200 attendees.\n\nWith a radial display of the information, participants could fill out multiple options for different questions, all together recreating the visual image of a fingerprint. Questions covered the typical technical skills or tools of a data scientist (e.g. Which programming language do you use? What is your favorite IDE?) to more curious and fun inquiries an the approach of work and their practice (e.g. Are you more “Machine Learning” or “Statistics”? Are you always on time or sometimes late?).\nWe brought an empty fingerprint to the event and the attention and willingness to participate surpassed our expectations.\n\nWith a sample of 87 attendees, here are some insights and anecdotes the fingerprint revealed\n\nAttendees define themselves mainly as practical and creative\n15% of them hold a PhD and just an 8% define themselves as self-taught\nTheir approach to life is more “Machine-Learning” than “Statistics”\nPython is the most loved language, followed by R\nWhen it comes to communicate, people prefer Tableau, and only a 16% use d3js\n… and, the majority are on time to meetings\n\nThanks to the contributors to the fingerprint and to the organizers of Big Data Spain to make a great event since 2012. It proved great place to meet with the community, exchange ideas with talented people and showcase our most recent work in using data: for social good, for urban analytics, for retail businesses or for customer advisory. See you next year!\n    \n\n"
    },
    {
      "title": "Scaling Simple Solutions to Complex Data Science Problems",
      "url": "https://www.bbvadata.com/es/scaling-to-lots-of-problems/",
      "date": "28 noviembre, 2016",
      "content": "\nAdmittedly, I always find it difficult to narrowly define the term data scientist in a few words. For instance, the expertise of our data science team is the opposite of narrow with competencies in machine learning, software development, data visualization, design, statistics, economics or social science. What ties us all together is our capacity to solve problems. Additionally, we must find efficient solutions within constraints of time and resources, sometimes keeping the philosophy that “a done something is better than a perfect nothing”.\nIn a previous life as corporate researcher in computer vision, I was dedicating my time to conducting state-of-the-art research and embedding their outcomes into industrial products. During that period, I had to prioritize machine learning methods with a good trade-off between practicality and quality. A good “design pattern” I found was to use methods that can be dubbed as data proximity algorithms. Machine learning professor and author of “The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World” Pedro Domingos calls them analogy-based algorithms and exemplifies them as follows:\n“Faced with a patient to diagnose, analogy-based algorithms find a patient in their files with the most similar symptoms and assume the same diagnosis. This may seem naive, but analogisers have a mathematical proof that it can learn anything given enough data.”\nWith sufficient data — and nowadays we are surrounded by a lot — data scientists are witnessing how these methods can provide solutions that are generic, fast, and conceptually simple to implement. They tend to have a short time-to-validation (ideal for the fans of the “fail fast” philosophy) – while obtaining performances near or above the state-of-the-art in some applications. Those methods are particularly compelling for data scientists who must quickly find solutions to a diverse set of problems.\nIn a recent Databeers event in Barcelona I argued for the emerging necessity for data scientists to consider methods that scale to lots of problems. My talk is 6-minutes short and rather inspirational than technical – but it does contain some technical references for the specialized audience. Enjoy!\n\nThanks a lot to the organizers for the invitation and to the public for their interest.\n\n"
    },
    {
      "title": "Transforming a Bank into a Data-Driven Business",
      "url": "https://www.bbvadata.com/es/transforming-a-bank-into-a-data-driven-business/",
      "date": "8 noviembre, 2016",
      "content": "\nAt BBVA Data & Analytics, we are strong believers in Amara’s law that says\n“We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run”\nOne of our main missions is to participate to the transformation of a global bank like BBVA into a data-driven business. This fall, when walking in the streets of Madrid, Barcelona, Bilbao and many other places in Spain, we started to notice that what was once a future vision has become a reality. Like many other major meaningful technological breakthrough, the presence of data-driven services are rapidly becoming so mundane that if you do not stop and reflect for a second you might underestimate their effect. In a recent video interview our chairman and BBVA’s Chief Data Scientist Marco Bressan explains the reality and dismantles the myths of a data-driven bank.\n\nUsing the examples of the recently launched BBVA Valora and our own Commerce360, Marco describes how the value of data is now directly transformed into value for customers. Valora, produced by our friends at Madiva gives anybody detailed information to negotiate the purchase of a house and a mortgage. With Commerce360 even the smallest entrepreneur has contextual business intelligence at their fingertips that exceeds their customer base in a way not even large enterprises hoped for a few years ago.\nSimilarly, within BBVA, our data scientists have been creating new approaches and tools to produce knowledge from financial data that colleagues are now introducing into their own practice. For instance, the capacity to fully decode customer journeys is now in the hands of customer intelligence teams.\nFinally, Marco argues for a moral commitment of data-driven businesses to share information, contribute with knowledge to society’s development and give answers to situations of crisis in the World. Our recent collaboration with the United Nation Global Pulse to measure the economic impact of Hurricane Odile in Mexico is a clear example of using data and data science to help authorities improve community resilience, which benefits the vulnerable and is also good for business continuity.\n\nToday, our efforts testify that a data-driven bank is a more efficient and competitive business that brings added value to its clients, customers, employees and society. As described by Marco, this evolution is slowly but surely translating into a more personalized experience that adapts to the needs of clients without invading their private sphere.\n \n\n"
    },
    {
      "title": "What Does Wind Look Like?",
      "url": "https://www.bbvadata.com/es/what-does-wind-look-like/",
      "date": "26 septiembre, 2016",
      "content": "\nIn our recent collaboration with United Nations Global Pulse to measure the economic impact of natural disasters we analyzed purchase behaviors when Hurricane Odile struck Baja California Sur in September 2014. As part of the study we visualized the strength of the winds and showed how the hurricane initially formed until finally making landfall.\nIn this article I describe how we produced this visualization by gathering wind data and animate it on the web.\nData sources\nA great resource for wind data is NOAA, which offers plenty of data services, including GFS. GFS is a weather forecast model produced by the National Centers for Environmental Prediction, with dozens of atmospheric and land-soil variables available.\nThis forecast model covers the entire globe, forming a grid of points (at different scales) which is used by the operational forecasters to predict weather out to 16 days in the future, at different hourly intervals. We extracted historical data on three variables: the date range (September 2014) and the wind vector components (horizontal and vertical) of the desired geographic extension to cover (Mexico). Aside from that, time granularity of the weather forecasts was important, so we chose the maximum avaialable, 3-hourly intervals. With these variables, we were good to go.\nData wrangling\nData extraction\nGFS offers historical data and weather forecast data in a specific file format called GRIB, which is a binary file format widely adopted in meteorology to store historical data and forecast weather data. In order to process these datasets we had to install wgrib2, a command-line tool used to query and extract data from grib files.\nWe extracted the following data:\n\n U-component (horizontal, UGRD) of wind vector (10m above ground)\n V-component (vertical, VGRD) of wind vector (10m above ground)\nTemperature (TMP) (100m above ground)\n\nWe used the following variables to query the datasets:\n\n Dates range between 10 September 2014 and 18 September 2014 (hurricane Odile made landfall on 15th September 2014)\n\nBounding box of covered region (in longitude/latitude coordinates): *\n\nBottom: 14.5\nLeft: -118.5 *\nTop: 32.5\nRight: -86.5\n\n\n3-hourly interval forecasts (maximum time granularity available)\n0.5 grid scale (measures taken every 0.5 degrees in longitude/latitude space)\n\n\n\nHere you can find the script we used to extract the required data in csv format.\nThis is what the extracted dataset looked like:\n\r\n\r\n$ head /tmp/result.csv\r\n\"t0\",\"t1\",\"variable\",\"filter\",\"lon\",\"lat\",\"value\"\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-118.5,14.5,-3.48\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-118,14.5,-3.55\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-117.5,14.5,-4.96\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-117,14.5,-6.28\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-116.5,14.5,-6.82\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-116,14.5,-6.23\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-115.5,14.5,-6.88\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-115,14.5,-7.53\r\n\"2014-09-10 00:00:00\",\"2014-09-10 03:00:00\",\"UGRD\",\"10 m above ground\",-114.5,14.5,-6.69\r\n\r\n\nThe extracted fields are fairly self-explanatory:\n\nt0: Timestamp of when the forecast was made\nt1: Timestamp of the forecast\nvariable: name of measure extracted\nfilter: height of measure\nlon: longitude of measure\nlat: latitude of measure\nvalue: value of measure\n\nData processing\nAt this stage we had a grid of points with wind and temperature measures available every 3 hours for the specified dates range and bounding box. Since hurricanes evolve at a very high pace, after some tests we noticed this time granularity was not enough to reproduce the hurricane trajectory properly. On the other hand, the grid was not dense enough either (measures were only available every 0.5 degrees in longitude/latitude coordinates) so we end up interpolating values for both space and time, so that we could rely on more points over time to draw the wind.\nWe used mostly pandas to carry out this grid interpolation. This script does pretty much all the necessary work and generates a new json file (data.json) containing grid interpolated measures of wind data (horizontal/vertical components, wind speed) in an hourly basis, which was meant to be consumed later on by the visualization.\nData visualization\nThe input for the data visualization was a list of grids, every one corresponding to the wind vectors for an specific hour in a day. Every grid could actually be seen as a vector field.\nVector fields are often used to model speed and direction of moving fluids, or the strength and direction of some force, for example. In our case, our whole purpose was to model the wind speed and direction.\nIn order to visualize how the wind moves over time we used the JavaScript canvas API, simulating a particle system driven by the wind vector field at some point in time t. Every some number of frames, the grid of wind vectors changes dynamically and so do the particles, achieving the effect of how the hurricane formed and moved over time.\nSince our grid of wind measures was finite, we had to interpolate particles positions using their closest grid points as well.\nBuilding the particle system\nParticles can be seen as a list of objects with the following properties:\n\n x: current x canvas position\ny: current y canvas position\nstartX: initial x canvas position\nstartY: initial y canvas position\n lastX: last x canvas position\nlastY: last y canvas position\nage: the particle age, used to reset particles’ positions after some time\n\nThe following piece of code initializes the canvas and the particle system:\n\r\n\r\nconst canvas = document.getElementById('canvas');\r\nconst ctx = canvas.getContext('2d');\r\nconst width = canvas.width;\r\nconst height = canvas.height;\r\n\r\n// Particles related\r\nconst N_PARTICLES = 2000;\r\nconst PARTICLE_MAX_AGE = 30;\r\nlet particles = _.map(_.range(N_PARTICLES), () => {\r\n  let x = Math.random() * width;\r\n  let y = Math.random() * height;\r\n  return {\r\n      x: x,\r\n      y: y,\r\n      startX: x,\r\n      startY: y,\r\n      lastX: x,\r\n      lastY: y,\r\n      age: 0\r\n  };\r\n});\r\n\r\n\nFast lookups\nIf we have a look at our dataset, we notice how data is sorted by date and longitude values increase first, and only then latitude values do so. We will take this into account to index our data longitude-wise, creating a multi-index data structure. This is necessary since we will be querying wind vectors values quite often and these operations need to be fast.\nThe following function returns the corresponding longitude and latitude indexes of our data structure given the longitude and latitude coordinates (in degrees):\n\r\n\r\nfunction lonLatToGridCoords(lon, lat, bounds, rows, cols, deltaX, deltaY) {\r\n  let x = Math.floor1)lon - bounds.left)/deltaX);\r\n  if (x == cols) x -= 1;\r\n  let y = Math.floor2)bounds.top - lat)/deltaY);\r\n  if (y == rows) y -= 1;\r\n  return [x,y];\r\n};\r\n\r\n\nThis is used when building the multi-index grid later on:\n\r\n\r\nfunction createGrid(gridData) {\r\n  const points = gridData.data.points;\r\n  const uv = gridData.data.uv;\r\n  const values = gridData.data.values;\r\n  if (!(points.length == uv.length && points.length == values.length"
    },
    {
      "title": "Data For Social Good",
      "url": "https://www.bbvadata.com/es/data-for-social-good/",
      "date": "15 septiembre, 2016",
      "content": "\nAlthough governments usually lag behind businesses in adopting new management techniques, many are starting to pay attention to the value of big data. Of course this may be because analytics has had a greater impact on the field of microeconomics and policy makers have traditionally relied more on macroeconomic analysis. We have been a leading company in applying big data to macroeconomic analysis, and we have published a number of papers on Mexican tourism, social mobility, and an awad-winning paper on new techniques for predicting macroeconomic indices. In these papers, we have demonstrated that bank card transaction data is a resource that can provide insight into economic activity in a more timely fashion and with greater granularity than traditional methods.\nThe Impact of a Natural Disaster\nTo apply this data source to yet another new area, we collaborated with United Nations Global Pulse, an innovation initiative of the United Nations Secretary-General, with the mission to accelerate adoption of big data for humanitarian action. A case study was designed to determine what descriptive statistics we could derive from bank card transactions that could measure the recovery time of an area hit by a natural disaster. For this, the team studied behavior in Baja California Sur (BCS) in Mexico when it was hit by hurricane Odile in September 2014. The damage attributed to hurricane Odile included 11 dead, 135 injured, thousands left homeless and considerable infrastructure damage totaling approximately US$1 billion. \nThe purpose of this analysis was to provide the raw input needed for more informed policy decisions for future events. Once we know what data is relevant, with this means of data collection we can gather the information in near real-time as natural disasters develop, improving our ability to respond rapidly. \nQuantifying Recovery\nA challenge to most economic studies is that they are not laboratory experiments that you can rerun, nor are they even clinical trials with a control group where you can study what would have happened to some individuals if there had not been a hurricane. Using Bayesian time-series models, analytics has come a long way in addressing this problem with synthetic controls. We used a R library “Causal Impact” to estimate what would have been the economic activity in BCS if the hurricane had not occurred and compared that to the actual activity to determine the amount of time needed for an affected region to return to a baseline. Google developed this library with the purpose of measuring the impact of ad campaigns: the number of clicks counted vs. the counterfactual number of clicks there would have been without a campaign. But since its publication, the library has found wide use in social statistics. With it, we built a normality model using both BCS’s past economic patterns and data from states with similar patterns. \nWe also subdivided the population to be studied into 3 income groupings, by gender, and by geography. Since geography in the raw data was subdivided into 341 postal codes with just 131 of those producing meaningful data, the geographic areas were grouped to be more manageable into 8 clusters using the analytics k-means algorithm.\n\n \nTotal expenditure on items such as food and gasoline in the days immediately before the hurricane struck increased by 50%.\nMuch of the behavior observed is what one would expect during an emergency, although now we can be much more precise about the magnitude of changes in behavior. The study was able to identify which areas were hit the hardest by the drop in economic activity and the relative time it took to recover. Some of the obvious effects that were quantified were that right before the hurricane spending shifted to buying food and gas, poorer income groups prepared less, electronic transactions plummeted during the hurricane due to widespread power outages, and in the aftermath the mix of transactions switched to cash withdrawals. Less obvious insights were that women engaged in twice the number of preparedness transactions (perhaps this one was only less obvious to male observers),  women’s spending took longer to return to the baseline, and lower income groups returned to normal patterns of spending faster than higher income groups. See our publication for more details.\n \n\n\nOverall, the average recovery time was observed to be around 2 weeks. When broken down further by location, recovery times varied from 2 days to more than 1 month (for the towns located on the south coast where the hurricane struck with its highest intensity).\nFuture Work\nWe believe policy makers should consider the implications of these observations. They will need to determine whether poorer citizens are not preparing because they do not have resources or they do not have access to the warnings about impending danger.\n \n“This type of real-time quantitative data on how people prepare for disaster could be used to inform proactive, targeted distribution of supplies or cash transfers to the most vulnerable, at risk populations”\nMiguel Luengo-Oroz, Chief Data Scientist at UN Global Pulse\n \nRevelations about a gender gap could indicate preparation measures should target women, as microcredit does, since they are engaging in stockpiling beforehand more than men. Real-time transaction monitoring in the aftermath could speed supply response and prevent price gouging.  And of course insurance can more accurately estimate economic losses with this data.\nAs electronic payments migrate from cards to mobile devices, such as BBVA’s Wallet app, they will only become more ubiquitous and the accuracy of this type of analysis will improve. Even if businesses lead the way in finding new uses for this data, it won’t end there. This was the first study of how to use transaction data to improve humanitarian aid, and further work will allow highly targeted responses that were previously infeasible.   \n“With this project, we have created a replicable and evidence-based approach to understanding vulnerability. New insights can help authorities improve community resilience, which benefits the vulnerable and is also good for business continuity. Displacement by floods and earthquakes, hurricanes or fast-moving epidemics all represent a risk to economic sustainability”\nElena Alfaro, CEO of BBVA Data & Analytics\n\n\n\nFind the whole story here\nPress release\nDownload the paper\n\n \n \n\n"
    },
    {
      "title": "Introducing Commerce360",
      "url": "https://www.bbvadata.com/es/introducing-commerce360/",
      "date": "21 julio, 2016",
      "content": "\nWith the rise of information-driven companies such as Google, Facebook and Amazon, it has become almost cliché to say that a company’s most valuable asset is its data, but it’s hard to deny that the more data a business has on its clients, the greater advantage it has over its competitors. This has put small businesses at an even greater disadvantage, since they lack the resources to gather and process terabytes of data, which has become routine for large enterprises. This is the reason why we have been developing a new service that offers small and medium businesses not only have extensive data on their own clients but also have contextual business intelligence on millions of consumers throughout Spain and soon in Mexico and other countries.\nBig data is not just the privilege of big business. With Commerce360 even the smallest entrepreneur has contextual business intelligence at their fingertips that exceeds their customer base in a way not even large enterprises hoped for a few years ago.\nWe launch Commerce360\nWe introduce you Commerce360, a web-based business intelligence service available to BBVA retail clients. Behind the service is a massive store of data collected from BBVA bankcard transactions and Point of Sale terminals that allows the service to offer a wide variety of summary statistics from this data after it has been anonymized and aggregated. In this way, businesses cannot trace a particular transaction or transactions to a specific individual or business, but they can analyze transactional data with several levels of granularity. Commonly, the type of insight a retail business look for is of the following:\n\nEstimate how much total revenue their business is generating versus the average in their business sector.\nCompare the average transaction amount of their business with the aggregate average for businesses in their sector and their neighborhood.\nExamine the cyclical patterns in their business sector and/or their geographic zone by month, week, day of the week, or time of day.\nAnalyze information about the home postal code of their clients and the clients of other businesses in their sector, broken down by country, province, or postal code.\nLearn about the demographics of their clients and the clients of other businesses in their sector and neighborhood. They can see the gender and age breakdown of their clients as compared to their competitors.\nLearn about repeat customers in their own business, and how many customers they’ve lost.\n\nA typical use case is that a retail business detects that other stores are receiving more clients from a specific neighborhood. Perhaps these other stores also have different average transaction amounts or different peak hours. A business could then plan an advertising campaign targeting this neighborhood, perhaps announcing changes in prices or hours of services, and then track any change in business from the targeted zone. Another use case is exemplified in this promotion video for Spain:\n\nThe Commerce360 dashboard is highly customizable. A user has 116 distinct modules they can download and arrange as they please on their desktop. By clicking options on the top-right of the module, the user can read more information on how the data was aggregated, see alerts, and see suggestions on how the information can be used to improve business, that is, the statistics are explained in everyday language so even those that don’t work with numbers on a daily basis can understand them. Here is a quick overview in Spanish:\n\nHow Commerce360 is Powered\nCommerce360 is deployed on Amazon on top of Redshift, S3, EMR, DynamoDB, and Elastic Load Balancing. Updates and calculations are done regularly from the BBVA mainframes to the Amazon environment. To do this, we use a data API that anonymizes, tokenizes, and aggregates the data in such a way that all the information stays confidential and cannot be traced to any individual.  \nThe data API has been opened up to the general public to foster an open and innovative ecosystem. For instance, BBVA ran two Innova Challenges: in 2013 and then in 2015. Contestants were challenged to find new uses for the data made available by our data API. \nThe technologies used to implement the business logic are typical for a web application. The logic running on the server side is mostly Java with some R programs for behind-the-scenes analytics. JavaScript, with its advanced D3 library for visualization, makes the dashboard graphics easy to implement on the client side. CartoDB is also used, since many of the modules will explore geographic information with heat maps and other easy-to-understand visualizations.\nOur Vision to Bridge the Digital Divide\nFeedback from the pilot program has been positive. BBVA clients using the service had a lower churn rate than non-users and when informed about the product, and we are noticing an encouraging rate of non-clients with interest in switching to BBVA. One of the fears society tends to have about technological advance is the digital divide: the power those that have data have over those that don’t. But today big data is not just the privilege of big business. With Commerce360 even the smallest entrepreneur has contextual business intelligence at their fingertips that exceeds their customer base in a way not even large enterprises hoped for a few years ago.\nIf you are interested, go to commerce360.es to register. We also invite you to check our 20-minutes webinar below (in Spanish) to get a feel of the service functionalities and experience.\n\n\n"
    },
    {
      "title": "The Competitive Necessity of Transaction Categorization",
      "url": "https://www.bbvadata.com/es/the-competitive-necessity-of-transaction-categorization/",
      "date": "18 julio, 2016",
      "content": "\n‘La revolución de las pequeñas cosas‘, or in English ‘the revolution of the small things’. That’s the slogan of BBVA’s ad campaign in Spain highlighting the digital transformation of the bank and the design of its services to promote an experience based ‘peace of mind’ and ‘time well spent’. We take an active part in that transformation.\n\nFor instance, in the past people spoke of the tedious task of balancing their checkbook with all the enthusiasm they showed about going to dentist to have a cavity drilled. But now, when a BBVA client logs into their bank account through the web or a smartphone, they can see where their money is going, already categorized and displayed in helpful graphics. These features have been given the moniker of Personal Finance Management (PFM) and automatic categorization of financial transactions has been the first step whose success determines the success of all the following features.\n46% of banking clients under 30 said the quality of a bank’s Personal Finance Management would influence what bank they choose.\nThe first generation of these PFMs suffered from single digit adoption rates, one of the reasons being that the client would have to manually classify their transactions, which made working with these PFMs as much fun as balancing a checkbook. Better applications have been around for smartphones for a while through third parties, such as Mint. But while they do a good job categorizing transactions and therefore financial planning, a user cannot execute transactions with these applications. For that, users have to go to the interface their bank provides. After a slow start, banks are waking up to the competitive necessity of providing a PFM with accurate categorization at its heart. According to a survey done by Novantas, 46% of banking clients under 30 said the quality of a bank’s PFM would influence what bank they choose. Of those who use PFMs, 78% use a third-party product, but 40% of these would prefer to use their bank’s software if the quality was acceptable, mostly for perceived security and the ability to carry out transactions. For example, many third-party PFMs require that you input your banking passwords.\nAs of today, our data engine classifies daily millions of transactions into 15 major categories with 72 subcategories.\nAt BBVA we also offer a PFM tool in our online banking website. The tool is powered by the categorization engine developed at BBVA Data & Analytics, which uses a set of big data technologies to process daily millions of transactions, classify them and enrich them with category labels. The categorization taxonomy includes 15 major categories with 72 subcategories. Therefore the hundreds of thousands of BBVA  Personal Finance Management users have several layers of granularity to examine where the money goes; first on a general scale, then clicking down to a more specific category and finally to the individual transactions. They can visualize the temporal evolution of expenses and compare trends between categories. Our goal is helping customers a more meaningful view of their personal finances. Check the result in this video: \n\nThe classification criteria were first created by subject-matter experts, encoded into a set of algorithms, and have been adjusted with customer feedback. Now, users suggest classification changes and new tags, which generates a much greater amount of feedback that is difficult for experts to quantify and interpret, at least for human experts. The situation is different for machine learning algorithms and the next generation transaction categorization techniques that we are currently developing. These algorithms will automatically learn the classification criteria from user feedback in order to yield even more personalized experiences. More on that soon.\n\n"
    },
    {
      "title": "Visualization Comes of Age",
      "url": "https://www.bbvadata.com/es/visualization-comes-of-age/",
      "date": "5 junio, 2016",
      "content": "\nA fair part of our activities to transform data into knowledge involves visualizing large amount of data and communicate the results in interfaces and stories. At BBVA we also advocate for best practices in data visualizations and train data scientists to improve their skills (see for instance our Introduction to Data Visualization and to Human Perception in Data Visualization). Here is a description of where our heart is with this practice.\nLet’s Start With A Little History\nIn 1854 in the Soho district of London a Cholera outbreak was raging that would eventually kill 616 people. John Snow knew something that went against the conventional wisdom of the time, so to convince others he used not only data in its raw format but also a convincing visualization of that data. At that time it was accepted that cholera was caused by bad air or miasma, but Dr. Snow suspected the infection spread in the water. Unfortunately for him he could not prove a causation since germ theory would not be proposed by Louis Pasteur for some years to come. But he could establish a correlation and clearly demonstrate it. Dr. Snow used a dot map, what would become to be known as a Voronoi diagram, to trace the source of the infection to a pump on Broad Street. He placed dots over a street map of the infected area and saw a clear pattern of clusters around the pump in question. He then carefully documented that exceptions around the cluster had other sources of water. The city did disable the pump, but still would not accept his theory. One critic, Reverend Henry Whitehead, tried to disprove Snow’s theory, but was persuaded by his maps, and helped establish the root cause of the illness. John Snow’s maps are now a rather famous example of how visualization contributed to the field of epidemiology.\nOriginal map by John Snow showing the clusters of cholera cases in the London epidemic of 1854, drawn and lithographed by Charles Cheffins.\nToday visualizations, both good and bad, are ubiquitous. New companies like Tableau, Spotfire, Pentaho, Qlik, Quadrigram and CartoDB are offering tools or services beyond traditional business intelligence. It would be impossible to say if we are in a bubble with regards to visualization companies, but it is certain that some sort of Big Visualization is a necessary consequence of Big Data.\nVisualization Becomes a Discipline\nThe practice of Visualization became much more systematized in 1983 when Edward Tufte published his landmark book The Visual Display of Quantitative Information where he expanded on the objectives of useful graphics. The onset of Big Data shifted more importance to two of those objectives. One was to make large data sets coherent. It’s doubtful in 1983 Professor Tufte could have imagined the size of the datasets we are working with, considering smartphones and the internet are leaving a digital footprint of our every move. But one consequence is that it is harder than ever to separate the signal from the noise.  The other objective with increased importance is to “reveal the data at several levels of detail”. In 1983, if you were alive, you were probably consuming visualization in the form of static charts in newspapers and magazines. Today it’s a whole new ballgame. On web sites or smartphones, graphics are expected be interactive if not animated. Tufte’s principle of “data at several levels” takes on a new meaning as users demand a drill-down capability to explore data at more detail with a click of the mouse or swipe on the phone. For this to happen the old HTML documents had to be augmented with newer technologies such as Data Driven Documents (D3.js) via JavaScript that can dynamically change the document as the data changes. The topic even sparks debate on what deserves to be called visualization. Some would argue that charts of the previous generation is not visualization and only the new more interactive tools deserve this name. They hold that if a chart is a word, then visualization is a phrase. \nRegardless of how we name it, some things remain constant. Professor Tufte warned against chartjunk, excessive and meaningless adornments that don’t convey information, that we now refer to as “Datatainment”, gratuitous animation on interactive websites.\nLeading expert Albert Cairo, Professor of Visual Journalism at University of Miami and author of “The Functional Art”, asserts that Visualization is one of the hardest aspects of Big Data to get right. He offers a more concise list of objectives for graphics than Professor Tufte, recommending we pay attention to 5 principles. A visualization should be:\n\nTruthful\nFunctional \nBeautiful\nInsightful\nEnlightening\n\nProfessor Cairo stresses that “Beautiful” does not mean flashy but elegant by conveying the maximum information with the minimum drawing. He also points out that he deliberately chose to use the word “Truthful” instead of the more common “Clarity”. He has inveighed against visualizations that are clear, but not truthful. Again this is not a new problem. The best selling book of all time on statistics is Darrell Huff’s 1954 book “How to Lie with Statistics” and its most cited chapters have been about lying with charts. His classic examples, taken from real publications, included truncating charts and changing scale to exaggerate changes, and using pictures scaled by height to show a change when reader’s eyes will really register the change in volume of the drawing, which will be a change greater than just the height. \nIn his blog, The Functional Art, Professor Cairo criticized liberal economist Robert Reich for a distortion that is common nowadays, that Mr. Huff failed to mention. Professor Cairo disparages the use of the double “y” axes, putting two variables on different scales, used by Reich to correlate union membership and declining middle-class incomes. Here is Dr. Reich’s work and Cairo’s correction. \nAlberto Cairo’s reinterpretation of Dr. Reich’s graph. Taken from Alberto Cairo’s blog\nFor more examples of the misleading double axes, see the quite enjoyable site “Spurious Correlations”. There you can see how well US spending on science and space correlates with suicides by hanging or strangulation, or how the per capita consumption of cheese correlates with deaths by becoming tangled in bedsheets. Such is the power of visualization to enlighten or mislead. \nCairo’s new book will be called The Truthful Art and will address bad practices. All this criticism may not get offenders to change their ways. Mr. Huff, whose examples also came from sources interested in political lobbying, cast doubt on these being honest mistakes when he noted “it is rather like being short changed: When all the mistakes are in the cashier’s favor, you can’t help wondering.” But while distorting information might not hurt your career if you are political pundit, this type of self deception can be fatal if you are businessman. Every businessman wants to believe all his investments will be profitable but if he acts accordingly without understanding the risks, he’ll be bankrupt.\nThe Business Value of Visualization\nWe have constructed a dataset from bank card transactions that contains the information a businessman would need to accurately assess risks and spot trends. It’s the discipline of visualization that helps us communicate that information. In 2011, before we were spun off from the BBVA Innovation Center, we took a clue from our friend John Snow and used maps to understand interesting data. But this time we weren’t looking for clusters of disease but the intensity of business activity. We used our data and other’s to display business activity changing through the day and to understand related demographics. Here is an examples of a map showing the intensity of commercial activity at a street level through the day in Barcelona.\nWe have built on this work for a new product we are currently launching, Commerce360. Commerce360’s dashboard provides easy to understand text with various visualizations with an interactive drill-down capability. Clients can display line charts to see trends, such as average transaction amount, activity by hour of day or day of week etc, in their own business and against other businesses in a given geographic area. Using CartoDB, Commerce360 can show maps of zipcodes of where customers live or where they are shopping with the intensity of commercial activity indicated by the intensity of the color on the map. And if the client wants, they can click to see this information in a bar chart to see the breakdown by exact percentages. Commerce360 can also reveal insightful information about a business’s customers and the average customer of the competitors in a given area. The business’s dashboard will show a bar chart of the demographics of customers by gender and age with information on how these numbers have changed over previous periods as well as what is the average for his competitors.\n\n \nThe dashboard visualizations allow our clients to quickly comprehend large amounts of data and spot trends that would be impossible my merely looking at spreadsheets or tables. With this information our clients can identify areas that need improvement in comparison to the competition, understand what factors are driving customer behavior and plan marketing programs to offer specials or launch new products.  \nVisualization is a quickly evolving discipline, which makes it difficult to say where it will end up in the next few years. Business people need to comprehend data in more dimensions than ever before, not just seeing snapshots but how customers move through space and time. To illustrate this we can view some of the more cutting edge visualizations we have done in collaboration with the MIT Senseable City Lab.\nEconomic Activity in Easter in Spain\nThe following visualization not only illustrates Spain’s spending habits the week before easter, but animates them. There are several types of animation going on at the same time that let the viewer witness commercial transactions evolve minute by minute. Different colors are chosen to represent the types of businesses. The viewer can see how the aggregate amounts rise and fall through the day on the charts on the left, and where the transactions are occurring on the map to the right. The viewer can even understand the intensity of the commercial activity in each region as the circles on map representing spending get larger as the aggregate amount of the transactions rises. A number of data points are quickly comprehensible: We see that larger and smaller cities start their activity on different schedules, we see that there are different peak hours of business depending on the type of business, and we see that since different regions celebrate the holidays on different days this in turn is reflected in their spending patterns.     \n\nSince tourism is such an important part of Spain’s economy we created a similar animated visualization with Vizzuality, a visualization consulting firm, for the July and August tourist season, with a drill down capability on 17 different business categories, by country of origin of the foreign tourists, and by region of Spain.\n \n\n\nIn the high technology businesses, the importance of communication is habitually underestimated, and at the same time there is a growing body of evidence that visualization is the most effective way to carry out that communication. Although estimates vary, there is a consensus that a huge amount of our brains’ processing time is dedicated to understanding visual imagery. Our brains can process visual information by an order of magnitude faster than we process text. In fact people are likely to feel engaged with visualizations and understand the information, whereas just pure text would disengage us due to information overload. A Wharton School of Business study demonstrated when groups are shown the same information purely with text and numbers or with visualization, that visualizations persuaded on average 67% of the audience as compared to 50% for the presentations without them. The blog WebDAM estimates that 84% of the communications on the web will be visual by 2018 and posts with good visualizations have a 650% higher engagement than text only. We might take those numbers with a grain of salt but the trend is clear. To engage your customers and understand massive amounts of business data, you have to take your visualizations up to a whole new level. \n\n"
    },
    {
      "title": "Recommender systems: Marketing gets personal",
      "url": "https://www.bbvadata.com/es/recommender-systems-marketing-gets-personal/",
      "date": "13 mayo, 2016",
      "content": "\nDid you ever get the feeling that Amazon understands your desires better than your spouse? Did you ever search for a vacation in Galicia on Google and then notice you see more ads trying to sell you sea food and raincoats? This is thanks to recommender systems brought to you by the phenomenon of Big Data.\nIn today’s world, businesses collect an amazing amount of data on their customs and use it to anticipate what they can sell them next. In days gone by an econometrician would have called this estimating your utility curve, utility being how much you value a certain product, but they did not have anywhere as much data to work with as today’s data scientists, nor the plethora of algorithms available to work with.\nThis type of personal experience is essential to today’s internet companies. Jeff Bezos of Amazon explains:\n\nIf I have 4.5 million customers on the web, I should have 4.5 million stores on the web.\n\nThis is what is called long-tail marketing: catering not just to the most popular, but also selling to many clients with very particular tastes. Yesterday’s companies reached their customers through mass marketing whereas today’s companies reach them through mass customization.\n\nWe realized that by looking at relatively few variables they could make more appropriate recommendations for pension plans\n\nSome recommendation systems don’t have to be as complex as Amazon’s to be personalized and effective. At BBVA D&A we realized that by looking at relatively few variables they could make more appropriate recommendations for pension plans to BBVA clients. The old way of doing mass mailings doesn’t make sense when a bank has the means to understand which clients have different abilities to save. BBVA decided to target and group clients that actually were in a good position to save, and recommend an amount that made sense to deposit in a tax-advantaged plan given recent trends in the client’s behavior.  \nWhat was important to analyze was not just a snapshot of the clients’ savings at the end of the year but the trend from month to month and how volatile the savings trend was. For volatility, we measured the ratio of the change in the minimum balance to the change in the medium balance. A higher ratio indicated that the client’s savings was more uncertain.\nA time period long enough to establish a trend but short of enough to be relevant to the client’s current situation had to be examined, therefore we chose to examine seven months’ worth of data. Along with a positive savings trend and less uncertainty, a few other conditions, such as still having an income and having an email account, were needed before a client was added to the trial group. Roughly 115,000 clients were chosen to be part of the trial that would be compared to the control group.\nPersonalization through clustering\nTo personalize the saving amount recommended, we grouped the clients by saving capacity, volatility and cyclical behavior. We clustered the clients using a Gaussian Mixture Model with the R package Mclust and used Hive (MapReduce) for the time series preprocessing. The algorithm categorized clients into nine clusters. The functions used from the Mclust package do highly refined clustering, thus are demanding with regards to computing power. Given that this project had 115,000 clients to classify, using Mclust was very feasible, but the algorithm would have been impractical on a project with millions of users and tens of millions of items, without throwing more resources at the problem.   \nSince many pension plans come with tax advantages, there is a specific time of year when you can catch a savers attention. The team had one month to carry out the analysis and make recommendations. The targeted advertisement was sent throughout all of Spain early in 2016. Initial analysis indicated 60% more contracts issued to the target group over the control group, although a review is still ongoing to determine how much of the variation can be attributed to the targeted offerings.\nTypes, advantages, disadvantages and a hybrid models\nWith so many business rushing to utilize recommender systems, they are evolving quickly and not everyone one is using the same taxonomy to describe them. But a rough consensus has begun to form. In the broadest categories, recommender systems can be classified into 2 groups:\n\n    Content-based systems, which evaluates the items a user has bought and tries to suggest similar items.\n    Collaborative filtering, which will recommend items that are rated highly by other users with similar tastes.\n\nWith a content-based system, a movie streaming service might noticed that you liked “Men in Black” directed by Barry Sonnefeld and that he also directed “Get Shorty” and will suggest that, or that you rented a film with Will Smith and will recommend another film with him.\nYou would start off building a matrix of the client and the items they have purchased or liked, and then use a profile of the items’ attributes. These attributes may be binary, such as whether the movie is or is not an action film, or scalar, such as whether it has 1 to 5 stars. There are many complex quantitative techniques that can be used to interpret this data, such as Bayesian classifiers, clustering, decision trees, neural networks, and text analytics (when recommending news articles).\nA disadvantage of this approach is that it can be limited in scope. You only can recommend what you have already seen, so you won’t help the user broaden their taste. In extreme cases this is called the “cold start” problem or “new user” problem. Since you are relying on historical data it takes time to get a system like this working.\nAn advantage is that the user-item matrix can contain relatively little data and still give reliable recommendations. So after you get over the initial hump of starting, it works well.\nAnother way of predicting user preference would be collaborative filtering, creating a vector of items bought by other users that have been rated negatively or positively and if one user’s vector is similar to others, the model recommends the items others have rated highly but the user has not bought. Recommendations can be direct, such as an Amazon reviews, or inferred, such as liking a news article or viewing this type of article often.\nThe advantage of this approach is you need very little understanding of the item being recommended. You might even introduce the user to something new.\nThe disadvantages are that you need a lot of data to make this accurate. Also, as your data grows you need massive amounts of computing power. Your calculations will grow on an order of magnitude related to the number of clients you have times the number of items. If you are like Spotify, a streaming music site, with millions of users and millions of items, this quickly becomes impractical.\nReal-time vs. Resources and best-of-breed solutions\nIn either method, a service that needs to work in real time can resort to various cluster techniques to speed up the search. Clustering is where you can reduce the users or items to similar segments thus approaching the issue as a classification problem. Cluster algorithms have become popular because they can deliver better online performance. The trade-off is the more refined the segments, the more computing power is needed, but the less refined the segments, the less accurate the recommendations.\nGiven the advantages and disadvantages of each method many companies will not choose purely one or the other, but will build a hybrid model that is the best-of-breed solution for their particular business. Amazon has built a hybrid they call “Item-to-Item Collaborative Filtering”. Rather than matching similar customers, they match client purchases with other rated items clustered together, then combine the list. The computing time is massively more than other methods to create a rated similar-item table. Therefore it is done off-line. In real time only the most highly correlated items are returned.\nWe developed Latrl a city guide that runs as an hybrid recommender system\nBuilding an hybrid model with BBVA data\nIn the future, most recommender systems that BBVA will implement, will need will handle millions of items rather than the 115,000 as in the PPI recommender, and as we have mentioned purely content-based and purely collaborative systems both fall short in essential requirements.\nCurrently we build recommender systems on the best practices of traditional methods but through the process of continuous improvement, we are researching Machine Learning techniques for future versions. \nSome of BBVA’s recommender systems depend on human-defined filters built with Java-SQL and are executed over an Amazon Redshift’s cloud data warehouse. The next generation of these applications could be quite different.\nIndeed we want to reach customers via BBVA’s mobile app Wallet, SMS or email. We have the raw data needed to identify these individuals through their purchase history using BBVA bank cards. The challenge is converting this raw data into usable and accurate information, and for that end a hybrid model is under development. Some content-based evaluation is needed, but collaborative filtering is decisive in our approach. The process is shaping up to have the following high-level steps:\n\nDevelop the user-item profile\nCreate feature vectors and similarity matrices of merchants   \nGenerate meaningful clusters of merchants\nApply the collaborative-filtering model to generate a list of interested customers for a businesses offering\n\nOur dataset is a history of a client’s purchases that gives us implicit feedback about the client’s preferences. Although the preferences are implicit, the way people spend money is usually more revealing about how they really feel than an explicit “like” button or giving something three stars. People tend to be more honest with themselves when money is involved than in any other activity.\nThe project started evaluating Python Pandas but soon turned to Spark as the appropriate tool for the job. PySpark is used for prototyping but Scala and Spark are used when it is necessary to improve performance for production.\nGraphs and Clusters from BBVA data\nUsing Spark machine learning library (MLlib), various algorithms have been experimented with for appropriateness to analyze the dataset. For content-based evaluation, the (Frequent Pattern) FP-growth algorithm is frequently used with market basket analysis, therefore it seemed a promising place to start for our clustering model. FP-growth works with transactions that are viewed as a set of items (or merchants in our case) and the algorithm looks for common patterns. It’s able to work on large datasets by limiting itself to using frequent sets without generating all candidates. Once the merchant sets are generated with Spark, we use python’s igraph library to produce a graph structure suitable for cluster analysis. The clusters are made up of related nodes on the graph, and these set of nodes are commonly referred to as a community. To find these communities, the igraph library provides various implementations that we have tried, such as edge-betweenness, walk-trap, spinglass, fastgreedy and multilevel. In an optimistic sign these algorithms are all producing similar clusters.\nHere is an example of how the cluster is visualized.\n\nLearning ALS-WR from Netflix\nOnce the clusters are formed, they constitute the input for collaborative filtering, for which the project is investigating various ways of doing matrix factorization. In recommender systems, matrix factorization is used to discover latent features influencing the interactions between two different kinds of entities, such as businesses and potential customers. The project considered various implementations, such as MLlib’s Singular Value Decomposition SVD++. The most promising algorithm we are currently investigating is alternating-least-squares with weighted-λ-regularization (ALS-WR). As mentioned earlier, one of the disadvantages of collaborative filtering is that one needs a lot of data, while many datasets are sparsely populated and it demands considerable computing power as the data grows. The authors of this algorithm in their paper “Large-Scale Parallel Collaborative Filtering for the Netflix Prize” say these problems are greatly ameliorated because “the performance of ALS-WR (in terms of root mean squared error (RMSE)) monotonically improves with both the number of features and the number of ALS iterations.”\nFeedback from the current system is positive but when the replacement project reaches the testing phase, we expect to see continuing improvement in performance. There’s an arms race in this industry, hence nothing is ever “good enough.”\nThe million dollar algorithm\nSo how much is this worth to long-tail retailers? Well over a million dollars for only a 10% better algorithm if you ask Netflix. From 2006 to 2009 Netflix ran a contest to see if any group could improve on their proprietary algorithm, “Cinematch”, by 10%. Netflix released a training set with 100 million ratings and a testing set with 3 million ratings. When building a recommender model you must build the model with one set of data and test with another, otherwise you might only be reverse engineering a model to fit one set of static data. Only against a new test set of data can you see if the model handles new situations. Netflix awarded progress prizes of $50.000 for each 1% improvement or the best progress in a year. At first progress came swiftly. A 1% improvement was submitted in less than 2 weeks, and by 2007 a team named BellKor had submitted an 8,43% improvement. But diminishing marginal returns set in and every incremental improvement took more effort. Finally on 26 June 2009 the 10% barrier was broken and all teams had 30 days to submit their best and last effort. At the deadline BellKor had submitted a 10,09% improvement, which was less than 0,01% improvement over the second-place team. Now Netflix reports that 75% of what its clients watch is from recommendations.\nToday there is a website dedicated to such data science contests, Kaggle.com, where companies and organizations post prizes for the best algorithms. But if you want to win you better know your profession and work hard. Like everything in internet commerce, the competition is fierce.\n\n"
    },
    {
      "title": "Big Data Comes Alive with Animation",
      "url": "https://www.bbvadata.com/es/big-data-comes-alive-with-animation/",
      "date": "10 abril, 2016",
      "content": "\nOur colleagues at Vizzuality commented that the interactive visualization they did with us was such a piece of art it could be displayed in Madrid’s Prado Art Museum. I disagree. Everyone knows that Madrid’s modern art museum is the Reina Sofia and BBVA’s and Vizzuality’s work is absolutely modern.\nIn 2015 we decided in collaboration with our friends at Vizzuality to demonstrate the power of big data, in particular BBVA’s dataset on bankcard transactions. The team had 5.4 million transactions from July through August of 2014 to work with. The amount of information about tourism gathered by traditional methods, such as surveys, pales in comparison to the information BBVA has available. Other modern techniques, such as information gathered from mobile phones produce more copious data, but they lack what people really care about: How money is spent.\nVisually the most striking aspect is the animated map where the user can watch spending rise and fall throughout the summer in an hour-by-hour time series. Vizzuality loaded the dataset into CartoDB, which allows the user to jump around in time on the animation and click on various filter options to get a more refined picture, such as just tourists from a particular country of origin or zeroing in on one province of Spain.\nWhile animation is a great way to intuitively understand data, the user can examine hard numbers on custom charts to answer more concrete questions. The user interface has four other tabs that allow the investigation of tourist trajectories, local statistics, industry-sector trends, and foreign versus domestic spending, all with drill-down capability that can give subtotals by a variety of criteria.\nTourist trajectories\nAmericans love Madrid\nFor example, if someone is working in a tourist-related industry, it would be interesting to know that Barcelona receives more revenue than any other region at 23% percent of the total spent by tourists. But the Balearic Islands and southern Mediterranean coast may want to pay more attention as they receive more as a percentage of their income from tourists than other provinces.\nAt 18%, the French make up the greatest share of foreign tourist spending in Spain, followed closely by the British. But if a business happens to be in Madrid it would want to target other clients, as Americans spend most in this particular region, and businesses in the Balearics should note that they would cater mostly to the British. That does not mean anyone should ignore Chinese tourists. In aggregate, they are not the most frequent customers, but per capita they are the biggest spenders.\nTourism weight\nSomeone from the fashion industry would be happy to know that their industry is the leader in receiving tourist business, capturing 29% of their spending, unless they happen to be in a province more popular with hungry Americans or boozy British where they spend most of their time in restaurants and bars, allocating 28% and 21% of their total expenditure in this sector. Not surprisingly these Anglo-Saxon types spend more money than other nationalities on Fridays and Saturdays.\nAnonymized and aggregated data\nA user can even drill down to the time of day and type of business for any particular nationality. Despite this dizzying array of detail, none of the information can be traced to any individual. BBVA anonymizes and then aggregates the data in way that ensures confidentiality but can still be used for insightful statistical analysis. Our Head of Urban Analytics Juan Murillo explains:\nBBVA aligns itself with the philosophy of Open Data, and BBVA is a leading financial institution in utilizing this type of data to improve management in the private and public sector.\nIn this spirit, we have worked with various institutions, such as INE, Segittur, SECTUR and Madrid’s city government, helping them gain insight into economic activity and knowing better how to manage infrastructure for the needs of the tourist industry. But the use of BBVA data is not limited to pretty pictures and tourism. We have published a series of technical papers opening new applications, including more timely macroeconomic analysis. Big data applications for card transactions continue to grow inside and outside of BBVA, even if every project is not as flashy as the animation we did with Vizzuality.\n\n"
    },
    {
      "title": "Data Helps Mexican Tourism Grow",
      "url": "https://www.bbvadata.com/es/data-helps-mexican-tourism-grow/",
      "date": "3 diciembre, 2015",
      "content": "\nBusinesses in the era of Big Data accumulate all the information they can from interactions with their own customers, but governments usually have wider concerns which require information beyond what one business can gather, and they need data that reveals patterns about the population in general. \nBancomer and BBVA Data & Analytics Have the Experience\nTourism is of vital importance to the Mexican economy. It is 8.7% of the GDP, generates 4 million jobs and grew by 19% in 2015 over the previous year. Secretaría de Turismo in Mexico (Sectur) needs to understand the spending patterns of tourists in order to plan infrastructure and marketing for that industry, but since tourist transactions are not with the state itself, the government has no means to gather the necessary data. That’s why it makes sense that Sectur would turn to  financial institutions for help and to BBVA Bancomer and BBVA Data & Analytics specifically.  \nBBVA Bancomer and BBVA Data & Analytics (D&A) are both companies in the BBVA group. BBVA D&A was created from the BBVA Innovation Center in February of 2014 so that BBVA D&A could provide services outside of the financial sector, such as helping the public sector promote tourism and consequently the type of services Sectur needed.\nBBVA D&A had been involved in the Smarter Cities initiative, and collaborated on studies with the Madrid City Council and with Segittur (part of Spain’s Ministry of Tourism). Using information culled from BBVA credit and debit cards and cards of other institutions used at BBVA POS devices, we were able to create a data set that is large enough and then normalize the data to prevent bias so that statically significant conclusions can be ascertained about the economy at large. In Spain, BBVA D&A created a data set aggregating 524 million transactions worth 24 billion euros for 48 million distinct clients (assuming 1 card per client), which allowed us to describe detailed spending patterns and produce visualizations of tourist activity in Spain, which is even a larger part of the Spanish economy than it is in Mexico, at 10.8% of Spanish GDP. Products leveraged by BBVA Data & Analytics in Spain included statistical reports, interactive visualization tools and APIs to access the anonymous and aggregated data to produce ad-hoc insights.\nBBVA Bancomer’s Data Produces the Results\nBBVA D&A’s ability to leverage that experience for Sectur was exactly what they were looking for. Bancomer welcomed an opportunity to strengthen the relationship. Sectur decided to start with a more limited scope and just examine the area of the Riviera Maya as a Proof Of Concept (POC), divided into 4 sub-geographies: Cancún, Isla Mujeres, Playa del Carmen and Cozumel. We used our experience to create a data set with Bancomer from 450 million transactions, 18 million individual cards and 115 thousands businesses. Of course it was not as simple as taking the raw data and immediately being able to interpret it. First the data has to be anonymized to guarantee client confidentiality (customer privacy comes before all else) and then aggregated to a meaningful granularity. Fortunately, BBVA D&A could efficiently adapt the processes used in Spain to meet Sectur’s needs.\nThe study provided a wide variety of descriptive statistics to Sectur. They could see how the tourists spent their money in numerous categories and subcategories of business types, the amounts of the transactions, geographic patterns of where transactions occurred and time series analysis for patterns by time of day, day of the week or time of year. They also learned a lot about the clients themselves. They could see where they lived and what age and gender cohorts they belonged to. Sectur validated the results by cross-referencing information with the data they traditionally collect and found the results were consistent with their data and added valuable information. They concluded the data would be useful for better demand segmentation analysis, would improve promotional campaigns, would reveal new opportunities close to main tourist sites, and would define areas where government could optimize business services.\nFuture Work Digs Deeper\nThere is plenty of room for Bancomer to add more value for Sectur. The POC did not define the country of origin for foreign visitors but only client origin-information for domestic clients, which was not a limitation for BBVA D&A’s work for Spain and will not be a limitation in future studies for Sectur, as this and more information will be included in 12 more reports Sectur has requested from Bancomer for 2016. Sectur already estimates that while US tourists are important, interest from Latin America is growing and that 30% of their foreign tourists come from Argentina and Colombia. They are anxious to have in-depth information in order to plan more efficient publicity campaigns abroad. The first two reports will be for Acapulco and Veracruz, and Sectur will prioritize the remaining ten studies. These reports are on the city level but could have been done on the national, regional or state level, whatever fits the client’s requirements. After those, of course, Sectur can look at the other value-add tools that BBVA D&A has provided to clients, such as dashboard visualizations and APIs.\nBancomer’s interest in the success of the project is not limited just to delivering more value to their public sector clients. With an eye on their own enlightened self-interest, Bancomer notes that they have extended credit to private sector tourism businesses to the tune of 15.5 billion pesos. This business has grown by 20% since last year and is expected to grow 15% to 20% in the coming years. Bancomer realizes when Sectur is successful, few others will be in a position to benefit from that success more than Bancomer.\nFor more information, read this article.  \n \n          \n \n\n"
    },
    {
      "title": "Reference Generation: A Method for Venue Recommendation",
      "url": "https://www.bbvadata.com/es/da-at-netsci-2015/",
      "date": "12 noviembre, 2015",
      "content": "\nBeing concise is often a virtue and sometimes it is also a necessity. It’s especially interesting to explain something as complex as data science in just 6 minutes. This was the idea behind the Pecha-Kucha talks at the NetSci2015 conference of the Network Science Society in Zaragoza, Spain. The Pecha-Kucha presentation style recommends 20 slides for 20 seconds each, forcing the speaker to give a concise, fast paced, and entertaining talk. That’s how it was when our data scientists Daniel Villatoro and Dario Patanè presented “Reference Generation: A Method for Venue Recommendation”.\nDani and Dario gave their talk at NetSci Backstage, the Satellite Symposium breakout sessions of the conference, where they raced through a high-level view of our then-pilot product, Latlr_. Latlr_ was a venue recommendation system that could contextualize user parameters such as location, time of request, or user’s past history. While there are other well-known systems in the market, such as Foursquare, GoWalla, or Google Places, we didn’t have to build a user history from scratch, thus avoiding what is known as the “cold start” or “new user” problem. We have extensive purchase histories in a dataset derived from BBVA bank card records. The data is anonymized so no specific person can be identified, but it still has enough detail to create consumer preference categories. From this data we built networks of preferences based on co-visitation, that is, if two venues are visited by the same person, and enough people also exhibit the same behavior, once a critical mass is reached preferences can be inferred for other similar clients. Business are the nodes and clients in common are the links in the network, with the number of common clients strengthening the link until D&A can create a graph something like this. \n\nWith preferences clustered, the next step is to find things that are interesting to the client personally, since a general recommendation, such as just listing the most popular venue, isn’t good enough for most people. For this we used what’s known as Tanimoto or Jaccard’s distance. Simply put, that’s the number of things one user has in common with another user or user cluster divided by all the things all the users in question like, including all the things the first user has in common with other users and all the things they don’t, which was quickly explained to the audience with this intuitive diagram:\n\nAt this point all the application needed was some context. Being a mobile app, it knew the user’s location, or the user could override that and enter a location where they intended to be later. Then the user would tell it what type of businesses they were looking for. They could even override their default preferences and tell the app they wanted a business similar to another in a different location and the app would look for similar preferences in their location and list the choices in the appropriate order. \nOf course recommendations to consumers is not the only application of this type of algorithm. It can also be used for Business Intelligence, such as businesses looking to understand local competition.\n\n"
    },
    {
      "title": "Predicting Regional Economic Indices",
      "url": "https://www.bbvadata.com/es/bbva-da-won-best-paper-award-in-data-science-conference/",
      "date": "2 septiembre, 2015",
      "content": "\nIt’s not unusual for people to complain about the timeliness of macroeconomic statistics. Governments are constantly revising GDP figures and unemployment rates well after the fact. It seems when it comes to economic statistics it’s tough to make predictions about the past. Other limitations are that the nature of the data that is used as input to official government economic indicators is limited on the geographic granularity and it is rather static when tracing changes over time. \nA better way of doing this analysis was explored in a paper that won Best Paper Award for the 2015 data science conference of the Academy of Science and Engineering.  The paper, “Predicting Regional Economic Indices Using Big Data Of Individual Bank Card Transaction”, by a group of researchers from MIT Senseable City Lab and co-authored by our Head of Urban Analytics Juan Murillo explored the hypothesis that common macroeconomic indices could be predicted by using data from credit card transactions collected by BBVA. If correlations could be found with generally accepted economic indicators, that would open the possibility of using this type of dataset for deeper economic analysis than we are currently getting. Possibilities could be using the dataset for analysis that is more timely, more granular, and able to account for variables that can’t be captured in today’s figures.\nWe have created a dataset derived from transactions with BBVA credit cards, debit cards, and all credit card transactions on BBVA point of sale terminals. This provides an embarrassment of riches with regards to economic data, which leads to the challenge of deciding what data maps well with predicting which economic indices. Six socioeconomic indicators were chosen because they are widely used, easy to measure and good indications on quality of life:\n\nGDP\nhousing price level\nunemployment rate\ncrime rate\npercentage of higher education\nlife expectancy\n\nPreparing the BBVA data so it was in a format that could be correlated to these economic indicators was an important first step. First, since the data is from BBVA and BBVA has different market penetration in different areas, the data had to be adjusted to account for this and avoid any bias. Secondly, we are dealing with data of many types, currency amounts, percentages or simple counts. The data was normalized so we could carry out meaningful comparisons.\nOur work and methodology\nWhen data is normalized in this manner logistic regression is a common technique to use within the method of supervised machine learning. The purpose here is to find which of our variables can predict our indicators. We started with 35 variables, which was still a rather unruly number to work with. Many variables might appear to correlate with an intended prediction, but in truth one variable may be important to the prediction, and other variables correlate to that one variable, thus possibly introducing more noise than clarity. The team used Principle Component Analysis, a widely accepted machine learning technique, to reduce our 35 variables to 16 components, and later in the experiment they were able to reduce this to 6 components. \nAfter all this preparation our team was ready to get to the fun part and see if our hypothesis worked. In supervised machine learning, data is separated into training and testing data sets. The training set is used to help the computers find the right algorithm and the testing set is used to validate that the algorithm works in new situations. The team choose 34 provinces of Spain’s 52 provinces as the training sets and the remaining 18 as the test sets in various combinations over several iterations.\nCity-wide analyses\nThe model worked very well, although not perfectly. Five out of our six indicators showed a high degree of correlation. Only predicting the crime rate did not fit into our model. This appeared to be due to some outliers the model just could not account for. However, GDP, housing, employment, education and life expectancy could all be predicted with our model. \nOk, so now we know that we can reach conclusions that we already knew, but in a more timely fashion. This opens up the question of what can we learn that we couldn’t know before? The BBVA data possess unique attributes that make insights possible, insights that are not achievable with current indicators. Official statistics can describe the situation on a provincial level, but with our approach, it’s possible to carry out the analysis on city-wide or even neighborhood level. BBVA data can also be tracked by time of day, day of week, time of year allowing cyclical time series analysis to be done on very low level, not available before.\nThe Economist magazine reported last year that Big Data’s effect has mostly improved the work of microeconomists and in turn raised the prestige of microeconomics in comparison to that of the traditionally more active macroeconomists, who have up till now tended to neglect advances in the Analytics field. We believe our work is an important step in bringing Big Data to the macro level.\n\n"
    }
  ]
}